{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "tags": [
     "Works"
    ]
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read File and create a new dataframe called 'df'\n",
    "df = pd.read_csv('dataset_1.csv')\n",
    "\n",
    "# Removing useless columns\n",
    "df = df.drop(columns = ['Unnamed: 0', 'link', 'source'])\n",
    "\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "tags": [
     "Works"
    ]
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from fractions import Fraction\n",
    "\n",
    "def convert_fractions(text):\n",
    "    \"\"\"Defining a method that convert fractions\"\"\"\n",
    "    # Match optional whole number + fraction, or just fraction\n",
    "    pattern = r'(?:(\\d+)\\s+)?(\\d+)/(\\d+)'\n",
    "\n",
    "    def replacer(match):\n",
    "        whole = int(match.group(1)) if match.group(1) else 0\n",
    "        numerator = int(match.group(2))\n",
    "        denominator = int(match.group(3))\n",
    "        decimal = whole + Fraction(numerator, denominator)\n",
    "        return \"{:.10g}\".format(float(decimal))  # Clean, no trailing zeros\n",
    "\n",
    "    return re.sub(pattern, replacer, text)"
   ],
   "id": "f6177b908ebf022c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "tags": [
     "Works"
    ]
   },
   "cell_type": "code",
   "source": [
    "from fractions import Fraction\n",
    "\n",
    "# Fixed conversions\n",
    "CONVERSIONS = {\n",
    "    \"oz\": 30,      # ounce → gram\n",
    "    \"lb\": 450,      # pounds → gram\n",
    "    \"pt\": 475,      # pint → milliliter\n",
    "    \"qt\": 950,      # quart → milliliter\n",
    "    \"inch\": 2.5        # inches → centimeter\n",
    "}\n",
    "\n",
    "def convert_units(text):\n",
    "    \"\"\"Defining a method that converts units by using the fixed conversions\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "\n",
    "    # Convert oz, lb, pt, qt (form: \"4 oz\", \"2.5 lb\", etc.)\n",
    "    for unit, factor in CONVERSIONS.items():\n",
    "        pattern = r'(\\d+(\\.\\d+)?)\\s*' + unit\n",
    "        text = re.sub(pattern, lambda m: f\"{round(float(m.group(1)) * factor, 1)} {unit_to_metric(unit)}\", text)\n",
    "\n",
    "    # Convert measure in inches in pans\n",
    "    text = re.sub(r'(\\d+)\\s*x\\s*(\\d+)[-\\s]*inch', lambda\n",
    "        m: f\"{int(m.group(1)) * CONVERSIONS['inch']:.0f} x {int(m.group(2)) * CONVERSIONS['inch']:.0f} cm\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def convert_fahrenheit_to_celsius(text):\n",
    "    # Match degrees like 275°, 275 °F, 275\\u00b0F (escaped), etc.\n",
    "    pattern = r'(\\d+)\\s*(?:°|\\\\u00b0)'\n",
    "\n",
    "    def replacer(match):\n",
    "        fahrenheit = int(match.group(1))\n",
    "        celsius = round((fahrenheit - 32) * 5 / 9)\n",
    "        return f\"{celsius}°C\"\n",
    "\n",
    "    return re.sub(pattern, replacer, text)\n",
    "\n",
    "def unit_to_metric(unit):\n",
    "    return {\n",
    "        \"oz\": \"g\",\n",
    "        \"lb\": \"g\",\n",
    "        \"pt\": \"ml\",\n",
    "        \"qt\": \"ml\",\n",
    "        \"inch\": \"cm\"\n",
    "    }[unit]\n",
    "\n",
    "# Applying conversions on the INGREDIENTS and DIRECTIONS columns\n",
    "df['INGREDIENTS'] = df['ingredients'].apply(convert_fractions)\n",
    "df['DIRECTIONS'] = df['directions'].apply(convert_fractions)\n",
    "\n",
    "df['INGREDIENTS'] = df['INGREDIENTS'].apply(convert_units)\n",
    "df['DIRECTIONS'] = df['DIRECTIONS'].apply(convert_units)\n",
    "\n",
    "df['DIRECTIONS'] = df['DIRECTIONS'].apply(convert_fahrenheit_to_celsius)\n",
    "\n",
    "# Renaming the title column and dropping the converted columns\n",
    "df = df.rename(columns={\"title\": \"TITLE\"})\n",
    "df = df.drop(['ingredients', 'directions'], axis=1)\n",
    "\n",
    "df[['TITLE', 'INGREDIENTS', 'DIRECTIONS']].head(200)"
   ],
   "id": "35a1e5c191694098",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "tags": [
     "Works"
    ]
   },
   "cell_type": "code",
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "# Patterns to detect time expressions\n",
    "TIME_PATTERN = re.compile(\n",
    "    r'(\\d+(?:\\.\\d+)?)\\s*(minute|minutes|min|mins|m|hour|hours|hr|hrs|h|day|days|d)\\b',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "RANGE_PATTERN = re.compile(\n",
    "    r'(\\d+)\\s*-\\s*(\\d+)\\s*(minute|minutes|min|mins|m|hour|hours|hr|hrs|h)\\b',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Time estimates for common preparation methods\n",
    "PREP_ESTIMATES = {\n",
    "    'bake': 45, 'boil': 20, 'fry': 15, 'grill': 25, 'chill': 120,\n",
    "    'simmer': 30, 'marinate': 60, 'microwave': 10, 'no-bake': 20,\n",
    "    'refrigerate': 180, 'freeze': 240\n",
    "}\n",
    "\n",
    "# Special cases for recipe types\n",
    "RECIPE_TYPE_ESTIMATES = {\n",
    "    'pasta': 10, 'salad': 15, 'cake': 45, 'pie': 60, 'stew': 120,\n",
    "    'casserole': 60, 'soup': 30, 'cookies': 30, 'bread': 90,\n",
    "    'fudge': 20, 'candy': 30\n",
    "}\n",
    "\n",
    "def convert_to_minutes(qty, unit):\n",
    "    \"\"\"Converts time quantity to minutes.\"\"\"\n",
    "    qty = float(qty)\n",
    "    unit = unit.lower()\n",
    "    if unit.startswith('m'): return qty\n",
    "    if unit.startswith('h'): return qty * 60\n",
    "    if unit.startswith('d'): return qty * 1440\n",
    "    return 0\n",
    "\n",
    "def estimate_by_recipe_type(title, ingredients):\n",
    "    \"\"\"Fallback estimation based on recipe title or ingredients.\"\"\"\n",
    "    title = str(title).lower()\n",
    "    ingredients = str(ingredients).lower()\n",
    "\n",
    "    # Check for specific recipe types\n",
    "    for keyword, est in RECIPE_TYPE_ESTIMATES.items():\n",
    "        if keyword in title:\n",
    "            return est\n",
    "\n",
    "    if any(word in ingredients for word in ['raw', 'fresh']):\n",
    "        return 15\n",
    "    if 'frozen' in ingredients:\n",
    "        return 20\n",
    "    if 'canned' in ingredients:\n",
    "        return 10\n",
    "\n",
    "    # Default fallback\n",
    "    return 30\n",
    "\n",
    "def clean_instruction_step(step):\n",
    "    \"\"\"Normalize range formats and identify special cases.\"\"\"\n",
    "    # Replace ranges (e.g., \"10-15 minutes\" → \"15 minutes\")\n",
    "    step = RANGE_PATTERN.sub(lambda m: f\"{m.group(2)} {m.group(3)}\", step)\n",
    "\n",
    "    # Handle special keywords\n",
    "    lowered = step.lower()\n",
    "    if 'overnight' in lowered:\n",
    "        return 480\n",
    "    if 'until set' in lowered or 'until firm' in lowered:\n",
    "        return 60\n",
    "\n",
    "    # Sum all time expressions\n",
    "    return sum(\n",
    "        convert_to_minutes(qty, unit)\n",
    "        for qty, unit in TIME_PATTERN.findall(step)\n",
    "    )\n",
    "\n",
    "def parse_instructions(instructions):\n",
    "    \"\"\"Parses instructions and extracts total estimated time in minutes.\"\"\"\n",
    "    total_time = 0\n",
    "\n",
    "    if not isinstance(instructions, list):\n",
    "        try:\n",
    "            instructions = ast.literal_eval(str(instructions))\n",
    "        except:\n",
    "            instructions = [instructions]\n",
    "\n",
    "    for step in instructions:\n",
    "        if isinstance(step, str):\n",
    "            total_time += clean_instruction_step(step)\n",
    "\n",
    "    return total_time\n",
    "\n",
    "def categorize_time(total_time):\n",
    "    \"\"\"Classify total time into labeled categories.\"\"\"\n",
    "    if total_time == 0:\n",
    "        return 'Not specified'\n",
    "    if total_time < 10:\n",
    "        return 'Very fast (0-10 mins)'\n",
    "    if total_time < 20:\n",
    "        return 'Fast (10-20 mins)'\n",
    "    if total_time < 40:\n",
    "        return 'Medium (20-40 mins)'\n",
    "    if total_time < 90:\n",
    "        return 'Slow (40-90 mins)'\n",
    "    return 'Very slow (90+ mins)'\n",
    "\n",
    "# Calculate total preparation time\n",
    "df['total_time'] = df['DIRECTIONS'].apply(parse_instructions)\n",
    "\n",
    "# Estimate time for unspecified recipes\n",
    "missing = df['total_time'] == 0\n",
    "df.loc[missing, 'total_time'] = df[missing].apply(\n",
    "    lambda row: estimate_by_recipe_type(row['TITLE'], row['INGREDIENTS']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Categorize preparation times\n",
    "df['PREPARATION_TIME'] = df['total_time'].apply(categorize_time)\n",
    "\n",
    "df[['TITLE', 'INGREDIENTS', 'DIRECTIONS', 'PREPARATION_TIME']].head(200)"
   ],
   "id": "4ca47d472c93a067",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "'''import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "\n",
    "# 2. Conversione unità → kg\n",
    "UNIT_CONVERSION = {\n",
    "    'cup': 0.24, 'c.': 0.24, 'c': 0.24,\n",
    "    'teaspoon': 0.005, 'tsp': 0.005, 'tsp.': 0.005,\n",
    "    'tablespoon': 0.015, 'tbsp': 0.015, 'tbsp.': 0.015,\n",
    "    'package': 0.2, 'pkg': 0.2, 'pkg.': 0.2,\n",
    "    'can': 0.4, 'carton': 1.0,\n",
    "    'g': 0.001, 'g.': 0.001, 'gram': 0.001,\n",
    "    'kg': 1.0, 'kilogram': 1.0,\n",
    "    'lb': 0.4536, 'pound': 0.4536,\n",
    "    'oz': 0.02835, 'ounce': 0.02835,\n",
    "    'large package': 0.5, 'large pkg.': 0.5, 'large pkg': 0.5\n",
    "}\n",
    "\n",
    "# 3. Parsing function (qty, unit, name, grams_in_paren)\n",
    "def parse_ingredient(ing_str):\n",
    "    # peso fra parentesi\n",
    "    m = re.search(r'\\(\\s*([0-9]+(?:\\.[0-9]+)?)\\s*g\\.?\\s*\\)', ing_str, re.IGNORECASE)\n",
    "    grams = float(m.group(1)) if m else None\n",
    "\n",
    "    # quantity e unit\n",
    "    patt = r'^\\s*([\\d\\/.\\s]+)?\\s*([a-zA-Z\\.]+)?'\n",
    "    m2 = re.match(patt, ing_str)\n",
    "    qty_raw = m2.group(1) if m2 else None\n",
    "    try:\n",
    "        qty = eval(qty_raw.replace(' ', '+')) if qty_raw else 1.0\n",
    "    except:\n",
    "        qty = 1.0\n",
    "    unit = (m2.group(2) or '').strip('.').lower() if m2 else ''\n",
    "\n",
    "    # name pulito\n",
    "    name = re.sub(r'\\(.*?\\)|optional', '', ing_str, flags=re.IGNORECASE)\n",
    "    # rimuovo quantità/unità\n",
    "    name = re.sub(r'^[\\d\\/.\\s]+\\s*[a-zA-Z\\.]*', '', name).strip().lower()\n",
    "    return qty, unit, name, grams\n",
    "\n",
    "# 4. Calcolo posizionale con fallback substring e token\n",
    "def calculate_recipe_cost_positional(ingredients_list, ner_list):\n",
    "    total = 0.0\n",
    "    missing = []\n",
    "\n",
    "    for ing_str, ner_item in zip(ingredients_list, ner_list):\n",
    "        key = ner_item.lower()\n",
    "        qty, unit, name, grams = parse_ingredient(ing_str)\n",
    "\n",
    "        # scorporo se passo i grams\n",
    "        if grams is not None:\n",
    "            kg = (grams / 1000) * qty\n",
    "        elif unit == '':  # a pezzo\n",
    "            # prendo prezzo direct key\n",
    "            price = price_dict.get(key)\n",
    "            # fallback substring/token\n",
    "            if price is None:\n",
    "                # substring match\n",
    "                for k, v in price_dict.items():\n",
    "                    if k in key or key in k:\n",
    "                        price = v\n",
    "                        break\n",
    "                # token match\n",
    "                if price is None:\n",
    "                    for token in key.split():\n",
    "                        if token in price_dict:\n",
    "                            price = price_dict[token]\n",
    "                            break\n",
    "            if price:\n",
    "                total += qty * price\n",
    "            else:\n",
    "                missing.append(key)\n",
    "            continue\n",
    "        else:\n",
    "            conv = UNIT_CONVERSION.get(unit, 0.0)\n",
    "            if conv == 0:\n",
    "                missing.append(key)\n",
    "                continue\n",
    "            kg = qty * conv\n",
    "\n",
    "        # prezzo per kg\n",
    "        price = price_dict.get(key)\n",
    "        if price is None:\n",
    "            for k, v in price_dict.items():\n",
    "                if k in key or key in k:\n",
    "                    price = v\n",
    "                    break\n",
    "        if price is None:\n",
    "            missing.append(key)\n",
    "            continue\n",
    "\n",
    "        total += kg * price\n",
    "\n",
    "    return round(total, 2), missing\n",
    "\n",
    "# 5. Applica al DataFrame\n",
    "    df['INGREDIENTS'] = df['INGREDIENTS'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['NER_clean']  = df['NER_clean'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "def compute_cost_row(row):\n",
    "     cost, _ = calculate_recipe_cost_positional(row['INGREDIENTS'], row['NER_clean'])\n",
    "     return cost\n",
    "\n",
    "def categorize_cost(total_cost):\n",
    "    if total_cost == 0:\n",
    "        return 'Not specified'\n",
    "    elif total_cost < 10:\n",
    "        return 'Very cheap'\n",
    "    elif 10 <= total_cost < 20:\n",
    "        return 'Cheap'\n",
    "    elif 20 <= total_cost < 45:\n",
    "        return 'Medium'\n",
    "    elif 45 <= total_cost < 90:\n",
    "        return 'Expensive'\n",
    "    else:\n",
    "        return 'Rich'\n",
    "\n",
    "df['total_cost'] = df.apply(compute_cost_row, axis=1)\n",
    "\n",
    "df['CATEGORY_COST'] = df['total_cost'].apply(categorize_cost)\n",
    "\n",
    "# Mostra risultati\n",
    "df[['INGREDIENTS', 'NER', 'CATEGORY_COST']].head(200)'''"
   ],
   "metadata": {
    "collapsed": false,
    "tags": [
     "potrebbe essere eliminata"
    ]
   },
   "id": "99c5e6f021ec1a8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "df['NER_clean'] = df['NER'].apply(ast.literal_eval)\n",
    "\n",
    "# List containing keywords\n",
    "NON_VEGAN_KEYWORDS = {\n",
    "    'milk', 'cheese', 'butter', 'cream', 'yogurt', 'gelatin', 'lard', 'honey',\n",
    "    'egg', 'eggs', 'fish', 'meat', 'chicken', 'beef', 'pork', 'gelatina',\n",
    "    'collagen', 'casein', 'whey', 'lactose', 'ghee', 'isinglass', 'carmine',\n",
    "    'shellac', 'albumen', 'pepsin', 'royal jelly', 'propolis', 'cocoa butter',\n",
    "    'bacon', 'sour cream', 'condensed milk', 'shredded cheese', 'cheddar',\n",
    "    'paraffin', 'marshmallows', 'buttermilk', 'ground beef', 'steak'\n",
    "}\n",
    "\n",
    "VEGAN_EXCEPTIONS = {\n",
    "    'milk': {'soy', 'almond', 'oat', 'rice', 'coconut', 'cashew', 'hazelnut'},\n",
    "    'cheese': {'vegan', 'nutritional yeast', 'cashew', 'tofu'},\n",
    "    'meat': {'soy', 'seitan', 'tofu', 'tempeh', 'jackfruit', 'plant-based'},\n",
    "    'butter': {'vegan', 'plant', 'peanut', 'almond', 'soy'},\n",
    "    'cream': {'coconut', 'soy', 'oat', 'vegan'},\n",
    "    'bacon': {'vegan', 'tempeh', 'coconut'}\n",
    "}\n",
    "\n",
    "VEGAN_MODIFIERS = {\n",
    "    'vegan', 'vegetable', 'plant-based', 'no milk', 'no eggs',\n",
    "    'dairy-free', 'without animal derivatives', 'cruelty-free', '100% vegetable'\n",
    "}\n",
    "\n",
    "# --- Utility\n",
    "def parse_ingredients(ingredients_input):\n",
    "    \"\"\"Always returning a parsed list of ingredients\"\"\"\n",
    "    if isinstance(ingredients_input, list):\n",
    "        return ingredients_input\n",
    "    if isinstance(ingredients_input, str) and ingredients_input.strip():\n",
    "        try:\n",
    "            return ast.literal_eval(ingredients_input)\n",
    "        except:\n",
    "            return [\n",
    "                x.strip().strip('\"').strip(\"'\")\n",
    "                for x in re.split(r',(?![^[]*\\])', ingredients_input.strip('[]'))\n",
    "                if x.strip()\n",
    "            ]\n",
    "    return []\n",
    "\n",
    "def contains_vegan_exception(ingredient, keyword):\n",
    "    return any(re.search(rf'\\b{re.escape(ex)}\\b', ingredient, re.IGNORECASE)\n",
    "               for ex in VEGAN_EXCEPTIONS.get(keyword, set()))\n",
    "\n",
    "# --- Classification\n",
    "def classify_vegan(ingredients_input):\n",
    "    \"\"\"Returning True or False based on the ingredients\"\"\"\n",
    "    ingredients = parse_ingredients(ingredients_input)\n",
    "    for ing in ingredients:\n",
    "        if not isinstance(ing, str):\n",
    "            continue\n",
    "        ing_lower = ing.lower()\n",
    "        if any(skip in ing_lower for skip in ('water', 'salt')):\n",
    "            continue\n",
    "        if any(re.search(rf'\\b{re.escape(mod)}\\b', ing_lower) for mod in VEGAN_MODIFIERS):\n",
    "            continue\n",
    "        for keyword in NON_VEGAN_KEYWORDS:\n",
    "            if re.search(rf'\\b{re.escape(keyword)}(?:s)?\\b', ing_lower):\n",
    "                if not contains_vegan_exception(ing_lower, keyword):\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "def get_non_vegan_ingredients(ingredients_input):\n",
    "    \"\"\"Returning list of ingredients without vegan exceptions\"\"\"\n",
    "    ingredients = parse_ingredients(ingredients_input)\n",
    "    non_vegan = []\n",
    "    for ing in ingredients:\n",
    "        if not isinstance(ing, str):\n",
    "            continue\n",
    "        ing_lower = ing.lower()\n",
    "        if any(skip in ing_lower for skip in ('water', 'salt')):\n",
    "            continue\n",
    "        for keyword in NON_VEGAN_KEYWORDS:\n",
    "            if re.search(rf'\\b{re.escape(keyword)}(?:s)?\\b', ing_lower):\n",
    "                if not contains_vegan_exception(ing_lower, keyword):\n",
    "                    non_vegan.append(ing)\n",
    "                break\n",
    "    return non_vegan\n",
    "\n",
    "# --- Apply\n",
    "df['VEGAN'] = df['NER_clean'].apply(classify_vegan)\n",
    "df['NON_VEGAN_INGREDIENTS'] = df['NER_clean'].apply(get_non_vegan_ingredients)\n",
    "\n",
    "# --- Final validation\n",
    "anti_patterns = re.compile(\n",
    "    r'\\b(not vegan|meat|steak|fish|cheese|egg|eggs|beef|chicken|pork|bacon|cream)\\b',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "df['VEGAN'] = df.apply(\n",
    "    lambda row: False if row['VEGAN'] and isinstance(row['TITLE'], str) and anti_patterns.search(row['TITLE']) else row['VEGAN'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# View results\n",
    "df[['TITLE', 'NER', 'VEGAN', 'NON_VEGAN_INGREDIENTS']].head(200)"
   ],
   "metadata": {
    "collapsed": false,
    "tags": [
     "da capire se serve ",
     "Works"
    ]
   },
   "id": "fd85d06e00f4b10c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "tags": [
     "Works",
     "da capire se serve "
    ]
   },
   "cell_type": "code",
   "source": [
    "# List of non vegetarian ingredients\n",
    "NON_VEGETARIAN_KEYWORDS = {\n",
    "    'meat', 'chicken', 'beef', 'pork', 'fish', 'anchovy', 'tuna', 'salmon',\n",
    "    'shellfish', 'shrimp', 'crab', 'lobster', 'bacon', 'gelatin', 'lard',\n",
    "    'collagen', 'isinglass', 'pepsin', 'ground beef', 'steak'\n",
    "}\n",
    "\n",
    "# Exceptions for vegetarian subs\n",
    "VEGETARIAN_EXCEPTIONS = {\n",
    "    'meat': {'soy', 'seitan', 'tofu', 'tempeh', 'jackfruit', 'plant-based'},\n",
    "    'bacon': {'vegan', 'tempeh', 'coconut'},\n",
    "    'gelatin': {'agar', 'pectin'},\n",
    "    'fish': {'banana blossom', 'tofu', 'plant-based'}\n",
    "}\n",
    "\n",
    "VEGETARIAN_MODIFIERS = {\n",
    "    'vegetarian', 'veggie', 'plant-based', 'meatless', 'no meat',\n",
    "    'without meat', 'cruelty-free'\n",
    "}\n",
    "\n",
    "# --- Utility\n",
    "def parse_ingredients(ingredients_input):\n",
    "    \"\"\"Always returning a parsed list of ingredients\"\"\"\n",
    "    if isinstance(ingredients_input, list):\n",
    "        return ingredients_input\n",
    "    if isinstance(ingredients_input, str) and ingredients_input.strip():\n",
    "        try:\n",
    "            return ast.literal_eval(ingredients_input)\n",
    "        except:\n",
    "            return [\n",
    "                x.strip().strip('\"').strip(\"'\")\n",
    "                for x in re.split(r',(?![^[]*\\])', ingredients_input.strip('[]'))\n",
    "                if x.strip()\n",
    "            ]\n",
    "    return []\n",
    "\n",
    "def contains_vegetarian_exception(ingredient, keyword):\n",
    "    exceptions = VEGETARIAN_EXCEPTIONS.get(keyword, set())\n",
    "    return any(re.search(rf'\\b{re.escape(ex)}\\b', ingredient, re.IGNORECASE) for ex in exceptions)\n",
    "\n",
    "# --- Classification\n",
    "def classify_vegetarian(ingredients_input):\n",
    "    \"\"\"Returning True or False based on the ingredients\"\"\"\n",
    "    ingredients = parse_ingredients(ingredients_input)\n",
    "    for ing in ingredients:\n",
    "        if not isinstance(ing, str):\n",
    "            continue\n",
    "        ing_lower = ing.lower()\n",
    "        if any(skip in ing_lower for skip in ('water', 'salt')):\n",
    "            continue\n",
    "        if any(re.search(rf'\\b{re.escape(mod)}\\b', ing_lower) for mod in VEGETARIAN_MODIFIERS):\n",
    "            continue\n",
    "        for keyword in NON_VEGETARIAN_KEYWORDS:\n",
    "            if re.search(rf'\\b{re.escape(keyword)}(?:s)?\\b', ing_lower):\n",
    "                if not contains_vegetarian_exception(ing_lower, keyword):\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "def get_non_vegetarian_ingredients(ingredients_input):\n",
    "    \"\"\"Returning list of ingredients without vegetarian exceptions\"\"\"\n",
    "    ingredients = parse_ingredients(ingredients_input)\n",
    "    non_vegetarian = []\n",
    "    for ing in ingredients:\n",
    "        if not isinstance(ing, str):\n",
    "            continue\n",
    "        ing_lower = ing.lower()\n",
    "        if any(skip in ing_lower for skip in ('water', 'salt')):\n",
    "            continue\n",
    "        for keyword in NON_VEGETARIAN_KEYWORDS:\n",
    "            if re.search(rf'\\b{re.escape(keyword)}(?:s)?\\b', ing_lower):\n",
    "                if not contains_vegetarian_exception(ing_lower, keyword):\n",
    "                    non_vegetarian.append(ing)\n",
    "                break\n",
    "    return non_vegetarian\n",
    "\n",
    "# Apply to dataframe\n",
    "df['VEGETARIAN'] = df['NER_clean'].apply(classify_vegetarian)\n",
    "df['NON_VEGETARIAN_INGREDIENTS'] = df['NER_clean'].apply(get_non_vegetarian_ingredients)\n",
    "\n",
    "anti_patterns = re.compile(\n",
    "    r'\\b(not vegetarian|not veg|meat|steak|fish|beef|chicken|pork|bacon)\\b',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "df['VEGETARIAN'] = df.apply(\n",
    "    lambda row: False if row['VEGETARIAN'] and isinstance(row['TITLE'], str) and anti_patterns.search(row['TITLE']) else row['VEGETARIAN'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Show results\n",
    "df[['TITLE', 'NER', 'VEGETARIAN', 'NON_VEGETARIAN_INGREDIENTS']].head(200)"
   ],
   "id": "8813043f994e6a4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Qui inizia la magia:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "345918a28f5a9fc0"
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "classifier = pipeline(\n",
    "    task=\"zero-shot-classification\",\n",
    "    model=\"MoritzLaurer/deberta-v3-base-zeroshot-v1.1-all-33\",\n",
    "    device=0,\n",
    "    torch_dtype=torch.float16,\n",
    "    model_kwargs={\"cache_dir\": \"./cache\"},\n",
    "    batch_size=32,  # Optimal for RTX 3060 Ti\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "# %% Data Loading\n",
    "df = pd.read_csv(\"dataset_1.csv\")\n",
    "df['NER'] = df['NER'].apply(eval)  # Convert string lists to actual lists\n",
    "\n",
    "# %% Food Categories & Prices (EUR/kg)\n",
    "CATEGORIES = [\n",
    "    \"dairy\", \"meat\", \"seafood\", \"grain\", \"vegetable\",\n",
    "    \"fruit\", \"spice/herb\", \"processed\", \"sweetener\",\n",
    "    \"condiment\", \"legume\", \"oil/fat\"\n",
    "]\n",
    "\n",
    "MEDIAN_PRICES = {\n",
    "    \"dairy\": 3.50,       # Milk, cheese\n",
    "    \"meat\": 7.50,        # Chicken, beef\n",
    "    \"seafood\": 12.00,    # Fish, shrimp\n",
    "    \"grain\": 2.20,       # Flour, rice\n",
    "    \"vegetable\": 1.80,   # Onions, garlic\n",
    "    \"fruit\": 2.50,       # Tomatoes, bananas\n",
    "    \"spice/herb\": 18.00, # Vanilla, cinnamon\n",
    "    \"processed\": 4.50,   # Pasta, canned goods\n",
    "    \"sweetener\": 2.20,   # Sugar\n",
    "    \"condiment\": 5.00,   # Mayo, dressings\n",
    "    \"legume\": 3.00,      # Beans, lentils\n",
    "    \"oil/fat\": 8.00      # Olive oil\n",
    "}\n",
    "\n",
    "# %% Ingredient Cleaning (Fixed)\n",
    "def clean_ingredient(ingredient: str) -> str:\n",
    "    \"\"\"Conservative cleaning preserving ingredient names\"\"\"\n",
    "    # Remove quantities (e.g., \"200g\", \"1/2 cup\")\n",
    "    cleaned = re.sub(r'\\b\\d+[\\d/\\.]*\\s*[a-z]*\\b', '', ingredient, flags=re.IGNORECASE)\n",
    "    # Remove special chars except spaces\n",
    "    cleaned = re.sub(r'[^\\w\\s]', '', cleaned).strip().lower()\n",
    "    return cleaned if cleaned else \"unknown\"\n",
    "\n",
    "# %% Batch Classification (GPU-optimized)\n",
    "classification_cache = {}\n",
    "\n",
    "def batch_classify(ingredients: list, batch_size: int = 16) -> dict:  # Reduced batch size\n",
    "    unique_ingredients = list(set(ingredients))\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        for batch in tqdm([unique_ingredients[i:i+batch_size]\n",
    "                           for i in range(0, len(unique_ingredients), batch_size)],\n",
    "                          desc=\"Classifying Ingredients\"):\n",
    "            # Process batch on GPU\n",
    "            results = classifier(batch, CATEGORIES, multi_label=False)\n",
    "\n",
    "            # Cache results\n",
    "            for ing, result in zip(batch, results):\n",
    "                classification_cache[ing] = result['labels'][0]\n",
    "\n",
    "            # Clear GPU cache\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return classification_cache\n",
    "\n",
    "# %% Process Entire Dataset\n",
    "# Get all unique ingredients\n",
    "all_ingredients = [clean_ingredient(ing)\n",
    "                   for recipe in df['NER']\n",
    "                   for ing in recipe]\n",
    "unique_ingredients = list(set(all_ingredients))\n",
    "\n",
    "# Batch classify with progress bar\n",
    "_ = batch_classify(unique_ingredients, batch_size=64)\n",
    "\n",
    "# Calculate recipe costs\n",
    "df['total_cost'] = df['NER'].apply(\n",
    "    lambda x: round(sum(\n",
    "        MEDIAN_PRICES.get(classification_cache[clean_ingredient(ing)], 3.00)\n",
    "        for ing in x\n",
    "    ), 2)\n",
    ")\n",
    "\n",
    "# Dynamic price categorization\n",
    "costs = df['total_cost'].values\n",
    "q33, q66 = np.percentile(costs, [33, 66])\n",
    "\n",
    "df['price_tag'] = df['total_cost'].apply(\n",
    "    lambda x: 'cheap' if x <= q33 else 'medium' if x <= q66 else 'expensive'\n",
    ")\n",
    "\n",
    "# %% Generate Ingredient-Category Dataset\n",
    "ingredient_df = pd.DataFrame(\n",
    "    [(ing, cat) for ing, cat in classification_cache.items()],\n",
    "    columns=['ingredient', 'category']\n",
    ")\n",
    "\n",
    "# Add dietary tags\n",
    "ingredient_df['vegan'] = ~ingredient_df['category'].isin(['dairy', 'meat', 'seafood'])\n",
    "ingredient_df['vegetarian'] = ~ingredient_df['category'].isin(['meat', 'seafood'])\n",
    "\n",
    "# Save outputs\n",
    "df.to_csv('recipes_with_prices.csv', index=False)\n",
    "ingredient_df.to_csv('ingredient_categories.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c57cbd4e0ba36f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Qui la magia finisce :)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eade76ff96b7ad22"
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pickle\n",
    "\n",
    "# === 1. Pre-elaborazione: selezione prime 10.000 righe e colonne necessarie ===\n",
    "df_subset = df[[\"INGREDIENTS\", \"DIRECTIONS\", \"PREPARATION_TIME\"]]\n",
    "\n",
    "# === 2. Unione ingredienti + passaggi in full_text ===\n",
    "df_subset[\"INGREDIENTS\"] = df_subset[\"INGREDIENTS\"].apply(eval)\n",
    "df_subset[\"DIRECTIONS\"] = df_subset[\"DIRECTIONS\"].apply(eval)\n",
    "df_subset[\"full_text\"] = df_subset[\"INGREDIENTS\"].apply(lambda x: \" \".join(x)) + \" \" + df_subset[\"DIRECTIONS\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# === 3. Codifica etichette PREPARATION_TIME ===\n",
    "label_encoder = LabelEncoder()\n",
    "df_subset[\"PREP_TIME_ENCODED\"] = label_encoder.fit_transform(df_subset[\"PREPARATION_TIME\"])\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Etichette PREPARATION_TIME:\", label_mapping)\n",
    "\n",
    "# === 4. Train/test split ===\n",
    "X = df_subset[\"full_text\"]\n",
    "y = df_subset[\"PREP_TIME_ENCODED\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# === 5. Tokenizzazione e padding ===\n",
    "tokenizer = Tokenizer(num_words=20000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "max_len = 500\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# === 6. Costruzione del modello ===\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=20000, output_dim=128, input_length=max_len),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(label_mapping), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "# === 7. Addestramento ===\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    validation_data=(X_val_pad, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# === 8. Salvataggio ===\n",
    "model.save(\"prep_time_classifier.h5\")\n",
    "\n",
    "with open(\"prep_time_label_mapping.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_mapping, f)\n",
    "\n",
    "print(\"Modello e label mapping salvati.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4190c03acb0a826",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Lista di ricette\n",
    "recipes = [\n",
    "    {\n",
    "        \"ingredients\": \"200g spaghetti, 2 cloves garlic, chili pepper, extra virgin olive oil, salt\",\n",
    "        \"instructions\": \"Cook the spaghetti. Meanwhile, sauté garlic and chili in the oil. Mix with the pasta.\",\n",
    "        \"expected_time\": 15\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Lettuce, 1 can of tuna, tomatoes, black olives, onion, olive oil, salt\",\n",
    "        \"instructions\": \"Wash and cut the vegetables. Add the drained tuna and olives. Dress with oil and salt.\",\n",
    "        \"expected_time\": 10\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"300g Carnaroli rice, 1 saffron sachet, vegetable broth, butter, onion, Parmesan cheese\",\n",
    "        \"instructions\": \"Sauté the onion, add rice and deglaze with wine. Gradually pour in the broth. Add saffron and stir in butter and cheese.\",\n",
    "        \"expected_time\": 30\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"3 eggs, salt, pepper, butter\",\n",
    "        \"instructions\": \"Beat the eggs with salt and pepper. Pour into a pan with melted butter and stir until set.\",\n",
    "        \"expected_time\": 5\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Lasagna sheets, meat sauce, béchamel, Parmesan cheese\",\n",
    "        \"instructions\": \"Layer pasta, sauce, béchamel and cheese. Bake at 180°C for 40 minutes.\",\n",
    "        \"expected_time\": 60\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Apples, bananas, oranges, strawberries, lemon juice, sugar\",\n",
    "        \"instructions\": \"Cut the fruit into pieces, mix with lemon juice and sugar, refrigerate.\",\n",
    "        \"expected_time\": 10\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Chicken breast, onion, curry powder, cream or coconut milk, oil, salt\",\n",
    "        \"instructions\": \"Brown the onion, add diced chicken, curry, and cream. Cook until the chicken is tender.\",\n",
    "        \"expected_time\": 25\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Ladyfingers, mascarpone, eggs, sugar, coffee, cocoa powder\",\n",
    "        \"instructions\": \"Make a cream with yolks, sugar and mascarpone. Layer ladyfingers soaked in coffee and the cream. Dust with cocoa and chill.\",\n",
    "        \"expected_time\": 30\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"2 eggs, 250ml milk, 125g flour, sugar, butter\",\n",
    "        \"instructions\": \"Mix flour, eggs and milk into a batter. Cook in pan with butter. Fill as desired.\",\n",
    "        \"expected_time\": 20\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Mixed dried legumes, onion, carrot, celery, vegetable broth, oil, salt\",\n",
    "        \"instructions\": \"Soak legumes for 12 hours. Sauté onion, carrot, celery. Add legumes and broth and cook for at least an hour.\",\n",
    "        \"expected_time\": 90\n",
    "    }\n",
    "]\n",
    "\n",
    "# Imposta il valore maxlen usato nel training\n",
    "maxlen = max_len  # ← Cambia questo valore se ne hai usato uno diverso\n",
    "\n",
    "# Prepara i testi concatenando ingredienti e istruzioni per ogni ricetta\n",
    "sample_texts = [r[\"ingredients\"] + \" \" + r[\"instructions\"] for r in recipes]\n",
    "\n",
    "# Tokenizza e pad\n",
    "sequences = tokenizer.texts_to_sequences(sample_texts)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Fai la predizione su tutte le ricette\n",
    "predictions = model.predict(padded_sequences)\n",
    "predicted_classes = predictions.argmax(axis=1)\n",
    "\n",
    "# Etichette di output\n",
    "label_map = {\n",
    "    0: 'Fast (10-20 mins)',\n",
    "    1: 'Medium (20-40 mins)',\n",
    "    2: 'Slow (40-90 mins)',\n",
    "    3: 'Very fast (0-10 mins)',\n",
    "    4: 'Very slow (90+ mins)'\n",
    "}\n",
    "\n",
    "# Mappa ogni predizione all'etichetta corrispondente\n",
    "labels = [label_map[pc] for pc in predicted_classes]\n",
    "\n",
    "# Stampa tutte le etichette predette, in ordine\n",
    "for i, pc in enumerate(predicted_classes):\n",
    "    pred_label = label_map[pc]\n",
    "    expected = recipes[i][\"expected_time\"]\n",
    "    print(f\"Recipe {i+1} predicted: {pred_label} | Expected time (mins): {expected}\")"
   ],
   "id": "367571519be110ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# === Parametri comuni ===\n",
    "MAX_LEN = 500\n",
    "NUM_WORDS = 20000\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# === Funzione per preprocessare un dataframe ===\n",
    "def preprocess(df, text_columns, label_column, n=10000):\n",
    "    df = df[text_columns + [label_column]].dropna().head(n).copy()\n",
    "    df[text_columns[0]] = df[text_columns[0]].apply(eval)\n",
    "    df[text_columns[1]] = df[text_columns[1]].apply(eval)\n",
    "    df[\"full_text\"] = df[text_columns[0]].apply(lambda x: \" \".join(x)) + \" \" + df[text_columns[1]].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"encoded_label\"] = label_encoder.fit_transform(df[label_column])\n",
    "\n",
    "    return df, label_encoder\n",
    "\n",
    "# === Funzione per addestrare un classificatore ===\n",
    "def train_text_classifier(df, label_encoder, model_name_prefix):\n",
    "    X = df[\"full_text\"]\n",
    "    y = df[\"encoded_label\"]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "    X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=NUM_WORDS, output_dim=128, input_length=MAX_LEN),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(len(label_encoder.classes_), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train_pad, y_train,\n",
    "        validation_data=(X_val_pad, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "    # Salvataggio\n",
    "    model.save(f\"{model_name_prefix}.h5\")\n",
    "    with open(f\"{model_name_prefix}_label_mapping.pkl\", \"wb\") as f:\n",
    "        pickle.dump(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))), f)\n",
    "\n",
    "    with open(f\"{model_name_prefix}_tokenizer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "    print(f\"Modello '{model_name_prefix}' salvato con successo.\")\n",
    "    return model\n",
    "\n",
    "# === PRIMO MODELLO: PREPARATION_TIME ===\n",
    "# Supponendo che tu abbia già caricato il dataframe df\n",
    "df_time = df[[\"INGREDIENTS\", \"DIRECTIONS\", \"PREPARATION_TIME\"]].copy()\n",
    "df_time, prep_time_encoder = preprocess(df_time, [\"INGREDIENTS\", \"DIRECTIONS\"], \"PREPARATION_TIME\")\n",
    "prep_time_model = train_text_classifier(df_time, prep_time_encoder, model_name_prefix=\"prep_time_classifier\")\n",
    "\n",
    "# === SECONDO MODELLO: DIFFICULTY ===\n",
    "df_difficulty = pd.read_csv(\"recipes_with_prices\")  # oppure usa il path già aperto\n",
    "df_difficulty, difficulty_encoder = preprocess(df_difficulty, [\"ingredients\", \"directions\"], \"difficulty\")\n",
    "difficulty_model = train_text_classifier(df_difficulty, difficulty_encoder, model_name_prefix=\"difficulty_classifier\")"
   ],
   "id": "2c63caf69e9c1eb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T12:10:48.068903Z",
     "start_time": "2025-05-21T12:10:47.614494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "MAX_LEN = 500  # come nel training\n",
    "\n",
    "# --- Carica modelli e tokenizer ---\n",
    "prep_time_model = load_model(\"prep_time_classifier.h5\")\n",
    "difficulty_model = load_model(\"difficulty_classifier.h5\")\n",
    "\n",
    "with open(\"prep_time_classifier_tokenizer.pkl\", \"rb\") as f:\n",
    "    prep_time_tokenizer = pickle.load(f)\n",
    "with open(\"difficulty_classifier_tokenizer.pkl\", \"rb\") as f:\n",
    "    difficulty_tokenizer = pickle.load(f)\n",
    "\n",
    "with open(\"prep_time_classifier_label_mapping.pkl\", \"rb\") as f:\n",
    "    prep_time_label_map = pickle.load(f)\n",
    "with open(\"difficulty_classifier_label_mapping.pkl\", \"rb\") as f:\n",
    "    difficulty_label_map = pickle.load(f)\n",
    "\n",
    "# Inverti i dizionari per decodifica predizioni\n",
    "inv_prep_time_label_map = {v: k for k, v in prep_time_label_map.items()}\n",
    "inv_difficulty_label_map = {v: k for k, v in difficulty_label_map.items()}\n",
    "\n",
    "# Lista di 10 ricette con tempo e difficoltà attesi\n",
    "recipes = [\n",
    "    {\n",
    "        \"ingredients\": \"200g spaghetti, 2 cloves garlic, chili pepper, extra virgin olive oil, salt\",\n",
    "        \"instructions\": \"Cook the spaghetti. Meanwhile, sauté garlic and chili in the oil. Mix with the pasta.\",\n",
    "        \"expected_time\": 15,\n",
    "        \"expected_difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Lettuce, 1 can of tuna, tomatoes, black olives, onion, olive oil, salt\",\n",
    "        \"instructions\": \"Wash and cut the vegetables. Add the drained tuna and olives. Dress with oil and salt.\",\n",
    "        \"expected_time\": 10,\n",
    "        \"expected_difficulty\": \"easy\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"300g Carnaroli rice, 1 saffron sachet, vegetable broth, butter, onion, Parmesan cheese\",\n",
    "        \"instructions\": \"Sauté the onion, add rice and deglaze with wine. Gradually pour in the broth. Add saffron and stir in butter and cheese.\",\n",
    "        \"expected_time\": 30,\n",
    "        \"expected_difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"3 eggs, salt, pepper, butter\",\n",
    "        \"instructions\": \"Beat the eggs with salt and pepper. Pour into a pan with melted butter and stir until set.\",\n",
    "        \"expected_time\": 5,\n",
    "        \"expected_difficulty\": \"easy\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Lasagna sheets, meat sauce, béchamel, Parmesan cheese\",\n",
    "        \"instructions\": \"Layer pasta, sauce, béchamel and cheese. Bake at 180°C for 40 minutes.\",\n",
    "        \"expected_time\": 60,\n",
    "        \"expected_difficulty\": \"hard\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Apples, bananas, oranges, strawberries, lemon juice, sugar\",\n",
    "        \"instructions\": \"Cut the fruit into pieces, mix with lemon juice and sugar, refrigerate.\",\n",
    "        \"expected_time\": 10,\n",
    "        \"expected_difficulty\": \"easy\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Chicken breast, onion, curry powder, cream or coconut milk, oil, salt\",\n",
    "        \"instructions\": \"Brown the onion, add diced chicken, curry, and cream. Cook until the chicken is tender.\",\n",
    "        \"expected_time\": 25,\n",
    "        \"expected_difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Ladyfingers, mascarpone, eggs, sugar, coffee, cocoa powder\",\n",
    "        \"instructions\": \"Make a cream with yolks, sugar and mascarpone. Layer ladyfingers soaked in coffee and the cream. Dust with cocoa and chill.\",\n",
    "        \"expected_time\": 30,\n",
    "        \"expected_difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"2 eggs, 250ml milk, 125g flour, sugar, butter\",\n",
    "        \"instructions\": \"Mix flour, eggs and milk into a batter. Cook in pan with butter. Fill as desired.\",\n",
    "        \"expected_time\": 20,\n",
    "        \"expected_difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Mixed dried legumes, onion, carrot, celery, vegetable broth, oil, salt\",\n",
    "        \"instructions\": \"Soak legumes for 12 hours. Sauté onion, carrot, celery. Add legumes and broth and cook for at least an hour.\",\n",
    "        \"expected_time\": 90,\n",
    "        \"expected_difficulty\": \"hard\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prepara testi input per modelli (ingredienti + istruzioni)\n",
    "texts = [r[\"ingredients\"] + \" \" + r[\"instructions\"] for r in recipes]\n",
    "\n",
    "# Tokenizza e pad per tempo\n",
    "prep_sequences = prep_time_tokenizer.texts_to_sequences(texts)\n",
    "prep_padded = pad_sequences(prep_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Tokenizza e pad per difficoltà\n",
    "diff_sequences = difficulty_tokenizer.texts_to_sequences(texts)\n",
    "diff_padded = pad_sequences(diff_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Predizione tempo con probabilità\n",
    "prep_preds = prep_time_model.predict(prep_padded)\n",
    "prep_classes = prep_preds.argmax(axis=1)\n",
    "prep_labels = [inv_prep_time_label_map[c] for c in prep_classes]\n",
    "\n",
    "# Predizione difficoltà con probabilità\n",
    "diff_preds = difficulty_model.predict(diff_padded)\n",
    "diff_classes = diff_preds.argmax(axis=1)\n",
    "diff_labels = [inv_difficulty_label_map[c] for c in diff_classes]\n",
    "\n",
    "# Stampa risultati con probabilità\n",
    "for i, recipe in enumerate(recipes):\n",
    "    print(f\"Recipe {i+1}:\")\n",
    "    print(f\"  Expected preparation time: {recipe['expected_time']} minutes\")\n",
    "    print(f\"  Predicted preparation time: {prep_labels[i]}\")\n",
    "    prep_probs_str = \", \".join([f\"{inv_prep_time_label_map[j]}: {prep_preds[i][j]*100:.2f}%\" for j in range(len(prep_preds[i]))])\n",
    "    print(f\"  Preparation time probabilities: {prep_probs_str}\")\n",
    "\n",
    "    print(f\"  Expected difficulty: {recipe['expected_difficulty']}\")\n",
    "    print(f\"  Predicted difficulty: {diff_labels[i]}\")\n",
    "    diff_probs_str = \", \".join([f\"{inv_difficulty_label_map[j]}: {diff_preds[i][j]*100:.2f}%\" for j in range(len(diff_preds[i]))])\n",
    "    print(f\"  Difficulty probabilities: {diff_probs_str}\\n\")\n"
   ],
   "id": "8c4c90ec82e24a52",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 77ms/step\n",
      "Recipe 1:\n",
      "  Expected preparation time: 15 minutes\n",
      "  Predicted preparation time: Medium (20-40 mins)\n",
      "  Preparation time probabilities: Fast (10-20 mins): 7.97%, Medium (20-40 mins): 75.07%, Slow (40-90 mins): 6.99%, Very fast (0-10 mins): 8.54%, Very slow (90+ mins): 1.44%\n",
      "  Expected difficulty: medium\n",
      "  Predicted difficulty: Easy\n",
      "  Difficulty probabilities: Easy: 61.60%, Hard: 38.40%\n",
      "\n",
      "Recipe 2:\n",
      "  Expected preparation time: 10 minutes\n",
      "  Predicted preparation time: Fast (10-20 mins)\n",
      "  Preparation time probabilities: Fast (10-20 mins): 52.36%, Medium (20-40 mins): 15.98%, Slow (40-90 mins): 13.54%, Very fast (0-10 mins): 3.09%, Very slow (90+ mins): 15.03%\n",
      "  Expected difficulty: easy\n",
      "  Predicted difficulty: Hard\n",
      "  Difficulty probabilities: Easy: 41.51%, Hard: 58.49%\n",
      "\n",
      "Recipe 3:\n",
      "  Expected preparation time: 30 minutes\n",
      "  Predicted preparation time: Medium (20-40 mins)\n",
      "  Preparation time probabilities: Fast (10-20 mins): 11.08%, Medium (20-40 mins): 62.18%, Slow (40-90 mins): 16.97%, Very fast (0-10 mins): 4.16%, Very slow (90+ mins): 5.61%\n",
      "  Expected difficulty: medium\n",
      "  Predicted difficulty: Easy\n",
      "  Difficulty probabilities: Easy: 61.47%, Hard: 38.53%\n",
      "\n",
      "Recipe 4:\n",
      "  Expected preparation time: 5 minutes\n",
      "  Predicted preparation time: Medium (20-40 mins)\n",
      "  Preparation time probabilities: Fast (10-20 mins): 1.73%, Medium (20-40 mins): 51.22%, Slow (40-90 mins): 38.26%, Very fast (0-10 mins): 3.11%, Very slow (90+ mins): 5.67%\n",
      "  Expected difficulty: easy\n",
      "  Predicted difficulty: Easy\n",
      "  Difficulty probabilities: Easy: 90.80%, Hard: 9.20%\n",
      "\n",
      "Recipe 5:\n",
      "  Expected preparation time: 60 minutes\n",
      "  Predicted preparation time: Slow (40-90 mins)\n",
      "  Preparation time probabilities: Fast (10-20 mins): 2.24%, Medium (20-40 mins): 0.69%, Slow (40-90 mins): 88.61%, Very fast (0-10 mins): 2.51%, Very slow (90+ mins): 5.95%\n",
      "  Expected difficulty: hard\n",
      "  Predicted difficulty: Easy\n",
      "  Difficulty probabilities: Easy: 93.09%, Hard: 6.91%\n",
      "\n",
      "Recipe 6:\n",
      "  Expected preparation time: 10 minutes\n",
      "  Predicted preparation time: Fast (10-20 mins)\n",
      "  Preparation time probabilities: Fast (10-20 mins): 85.65%, Medium (20-40 mins): 7.69%, Slow (40-90 mins): 2.42%, Very fast (0-10 mins): 2.36%, Very slow (90+ mins): 1.89%\n",
      "  Expected difficulty: easy\n",
      "  Predicted difficulty: Easy\n",
      "  Difficulty probabilities: Easy: 95.09%, Hard: 4.91%\n",
      "\n",
      "Recipe 7:\n",
      "  Expected preparation time: 25 minutes\n",
      "  Predicted preparation time: Medium (20-40 mins)\n",
      "  Preparation time probabilities: Fast (10-20 mins): 2.68%, Medium (20-40 mins): 89.24%, Slow (40-90 mins): 6.15%, Very fast (0-10 mins): 1.22%, Very slow (90+ mins): 0.70%\n",
      "  Expected difficulty: medium\n",
      "  Predicted difficulty: Easy\n",
      "  Difficulty probabilities: Easy: 54.18%, Hard: 45.82%\n",
      "\n",
      "Recipe 8:\n",
      "  Expected preparation time: 30 minutes\n",
      "  Predicted preparation time: Medium (20-40 mins)\n",
      "  Preparation time probabilities: Fast (10-20 mins): 6.49%, Medium (20-40 mins): 54.11%, Slow (40-90 mins): 29.39%, Very fast (0-10 mins): 3.20%, Very slow (90+ mins): 6.80%\n",
      "  Expected difficulty: medium\n",
      "  Predicted difficulty: Easy\n",
      "  Difficulty probabilities: Easy: 79.51%, Hard: 20.49%\n",
      "\n",
      "Recipe 9:\n",
      "  Expected preparation time: 20 minutes\n",
      "  Predicted preparation time: Medium (20-40 mins)\n",
      "  Preparation time probabilities: Fast (10-20 mins): 3.99%, Medium (20-40 mins): 74.01%, Slow (40-90 mins): 17.40%, Very fast (0-10 mins): 2.11%, Very slow (90+ mins): 2.48%\n",
      "  Expected difficulty: medium\n",
      "  Predicted difficulty: Easy\n",
      "  Difficulty probabilities: Easy: 92.04%, Hard: 7.96%\n",
      "\n",
      "Recipe 10:\n",
      "  Expected preparation time: 90 minutes\n",
      "  Predicted preparation time: Very slow (90+ mins)\n",
      "  Preparation time probabilities: Fast (10-20 mins): 0.00%, Medium (20-40 mins): 0.03%, Slow (40-90 mins): 4.21%, Very fast (0-10 mins): 0.00%, Very slow (90+ mins): 95.76%\n",
      "  Expected difficulty: hard\n",
      "  Predicted difficulty: Hard\n",
      "  Difficulty probabilities: Easy: 3.16%, Hard: 96.84%\n",
      "\n"
     ]
    }
   ],
   "execution_count": 42
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
