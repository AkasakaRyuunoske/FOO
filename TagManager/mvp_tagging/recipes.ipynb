{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Caricamento dei file contenenti il dataset",
   "id": "c202d9e7cd83c915"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                           TITLE  \\\n",
       "0                  Western Sizzlin Bread Pudding   \n",
       "1      Creamy Tuna And Bacon Salad (Reduced Fat)   \n",
       "2                  Spinach And Mushroom Lasagna    \n",
       "3                               Three-Bean Tacos   \n",
       "4                          Hearty Hamburger Soup   \n",
       "...                                          ...   \n",
       "44618             California Chili Powder Recipe   \n",
       "44619                           Pizza Margherita   \n",
       "44620   Frenchish Chicken And Red Wine Casserole   \n",
       "44621   Blueberry Almond Streusel Muffins Recipe   \n",
       "44622                            Sugar Cream Pie   \n",
       "\n",
       "                                                     NER difficulty     time  \\\n",
       "0      ['eggs', 'milk', 'sugar', 'vanilla', 'gallon w...       Hard     Slow   \n",
       "1      ['tuna', 'mayonnaise', 'low-fat sour cream', '...       Hard  Average   \n",
       "2      ['olive oil', 'onion', 'garlic', 'salt', 'grou...       Hard    Quick   \n",
       "3      ['olive oil', 'onion', 'red bell pepper', 'gre...       Hard    Quick   \n",
       "4      ['lean ground beef', 'white onion', 'ground bl...       Hard    Quick   \n",
       "...                                                  ...        ...      ...   \n",
       "44618  ['Mexico chilies', 'grnd cumin', 'cayenne', 'o...       Easy  Average   \n",
       "44619  ['batch pizza', 'marinara sauce', 'mozzarella'...       Easy    Quick   \n",
       "44620  ['tomatoes', 'red wine', 'chicken stock', 'gar...       Hard     Slow   \n",
       "44621  ['flour', 'sugar', 'baking pwdr', 'baking soda...       Hard  Average   \n",
       "44622  ['Pastry', 'cornstarch', 'milk', 'butter', 'va...       Hard    Quick   \n",
       "\n",
       "            cost method                                        INGREDIENTS  \\\n",
       "0          Cheap   Bake  [\"16 eggs, beaten\", \"6 -8 evaporated milk, 360...   \n",
       "1          Cheap  Other  [\"1 (6 ounce) can tuna, drained and flaked\", \"...   \n",
       "2          Cheap   Bake  [\"1 tablespoon olive oil\", \"1 medium onion , c...   \n",
       "3          Cheap   Boil  [\"1 teaspoon olive oil\", \"1 cup diced onion\", ...   \n",
       "4      Expensive   Boil  [\"900.0 gs lean ground beef\", \"1 white onion, ...   \n",
       "...          ...    ...                                                ...   \n",
       "44618      Cheap  Other  [\"0.25 c. grnd Calif or possibly New Mexico ch...   \n",
       "44619      Cheap   Bake  [\"1 batch pizza dough (see preceding recipe)\",...   \n",
       "44620      Cheap   Boil  [\"1 (14 ounce) can chopped tomatoes\", \"1.25 cu...   \n",
       "44621      Cheap   Bake  [\"2.5 c. all-purpose flour\", \"1 c. granulated ...   \n",
       "44622      Cheap   Bake  [\"Pastry for single-crust pie (22.5 cmes)\", \"1...   \n",
       "\n",
       "                                              DIRECTIONS  total_time  \\\n",
       "0      [\"Distribute rolls and cinnamon roll in (4) 2\\...       150.0   \n",
       "1      [\"Put everything in a bowl and mix together un...        15.0   \n",
       "2      [\"Heat oven to 375 degrees.\", \"\", \"In medium s...        70.0   \n",
       "3      [\"Heat oil in a large skillet over medium-high...        22.0   \n",
       "4      [\"Brown ground beef and onion in a large pot. ...        45.0   \n",
       "...                                                  ...         ...   \n",
       "44618  [\"In a 0.3333333333- to 0.5-c. tall, narrow cl...        30.0   \n",
       "44619  [\"Preheat oven to 450 degrees F. Place a pizza...        10.0   \n",
       "44620  [\"Place the tomatoes, wine, stock, garlic, ros...       100.0   \n",
       "44621  [\"Preheat oven to 400 .\", \"Combine first 5 ing...        18.0   \n",
       "44622  [\"Preheat oven to 232Â°C. Roll out dough to fit...        37.0   \n",
       "\n",
       "           PREPARATION_TIME  \\\n",
       "0      Very slow (90+ mins)   \n",
       "1         Fast (10-20 mins)   \n",
       "2         Slow (40-90 mins)   \n",
       "3       Medium (20-40 mins)   \n",
       "4         Slow (40-90 mins)   \n",
       "...                     ...   \n",
       "44618   Medium (20-40 mins)   \n",
       "44619     Fast (10-20 mins)   \n",
       "44620  Very slow (90+ mins)   \n",
       "44621     Fast (10-20 mins)   \n",
       "44622   Medium (20-40 mins)   \n",
       "\n",
       "                                               NER_clean  VEGAN  \\\n",
       "0      ['eggs', 'milk', 'sugar', 'vanilla', 'gallon w...  False   \n",
       "1      ['tuna', 'mayonnaise', 'low-fat sour cream', '...  False   \n",
       "2      ['olive oil', 'onion', 'garlic', 'salt', 'grou...  False   \n",
       "3      ['olive oil', 'onion', 'red bell pepper', 'gre...  False   \n",
       "4      ['lean ground beef', 'white onion', 'ground bl...  False   \n",
       "...                                                  ...    ...   \n",
       "44618  ['Mexico chilies', 'grnd cumin', 'cayenne', 'o...   True   \n",
       "44619  ['batch pizza', 'marinara sauce', 'mozzarella'...   True   \n",
       "44620  ['tomatoes', 'red wine', 'chicken stock', 'gar...  False   \n",
       "44621  ['flour', 'sugar', 'baking pwdr', 'baking soda...  False   \n",
       "44622  ['Pastry', 'cornstarch', 'milk', 'butter', 'va...  False   \n",
       "\n",
       "                                   NON_VEGAN_INGREDIENTS  VEGETARIAN  \\\n",
       "0                                       ['eggs', 'milk']        True   \n",
       "1           ['tuna', 'low-fat sour cream', 'bacon bits']       False   \n",
       "2           ['ricotta cheese', 'Parmesan cheese', 'egg']        True   \n",
       "3                                     ['cheddar cheese']        True   \n",
       "4      ['lean ground beef', 'parmesan cheese', 'sour ...       False   \n",
       "...                                                  ...         ...   \n",
       "44618                                                 []        True   \n",
       "44619                                                 []        True   \n",
       "44620     ['chicken stock', 'chicken thighs', 'chicken']       False   \n",
       "44621  ['milk', 'low-fat buttermilk', 'light Ricotta ...        True   \n",
       "44622                                 ['milk', 'butter']        True   \n",
       "\n",
       "                           NON_VEGETARIAN_INGREDIENTS  total_cost   price_tag  \\\n",
       "0                                                  []        49.2      medium   \n",
       "1                              ['tuna', 'bacon bits']        52.5   expensive   \n",
       "2                                                  []        78.0        rich   \n",
       "3                                                  []        87.5        rich   \n",
       "4                                ['lean ground beef']        96.3        rich   \n",
       "...                                               ...         ...         ...   \n",
       "44618                                              []        78.8        rich   \n",
       "44619                                              []        18.0  very cheap   \n",
       "44620  ['chicken stock', 'chicken thighs', 'chicken']        60.3   expensive   \n",
       "44621                                              []        91.4        rich   \n",
       "44622                                              []        52.0   expensive   \n",
       "\n",
       "                                              categories  vegan  vegetarian  \n",
       "0      ['seafood', 'dairy', 'sweetener', 'spice/herb'...  False       False  \n",
       "1      ['seafood', 'condiment', None, 'meat', 'spice/...  False       False  \n",
       "2      ['condiment', 'vegetable', 'vegetable', 'condi...  False       False  \n",
       "3      ['condiment', 'vegetable', 'vegetable', 'veget...  False       False  \n",
       "4      ['meat', 'vegetable', 'condiment', 'spice/herb...  False       False  \n",
       "...                                                  ...    ...         ...  \n",
       "44618  [None, 'spice/herb', 'spice/herb', 'spice/herb...   True        True  \n",
       "44619          ['processed', 'condiment', 'dairy', None]  False        True  \n",
       "44620  ['vegetable', 'processed', 'processed', 'veget...  False       False  \n",
       "44621  ['processed', 'sweetener', 'processed', 'spice...  False        True  \n",
       "44622  [None, 'processed', 'dairy', 'dairy', 'spice/h...  False        True  \n",
       "\n",
       "[44623 rows x 20 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>NER</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>time</th>\n",
       "      <th>cost</th>\n",
       "      <th>method</th>\n",
       "      <th>INGREDIENTS</th>\n",
       "      <th>DIRECTIONS</th>\n",
       "      <th>total_time</th>\n",
       "      <th>PREPARATION_TIME</th>\n",
       "      <th>NER_clean</th>\n",
       "      <th>VEGAN</th>\n",
       "      <th>NON_VEGAN_INGREDIENTS</th>\n",
       "      <th>VEGETARIAN</th>\n",
       "      <th>NON_VEGETARIAN_INGREDIENTS</th>\n",
       "      <th>total_cost</th>\n",
       "      <th>price_tag</th>\n",
       "      <th>categories</th>\n",
       "      <th>vegan</th>\n",
       "      <th>vegetarian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Western Sizzlin Bread Pudding</td>\n",
       "      <td>['eggs', 'milk', 'sugar', 'vanilla', 'gallon w...</td>\n",
       "      <td>Hard</td>\n",
       "      <td>Slow</td>\n",
       "      <td>Cheap</td>\n",
       "      <td>Bake</td>\n",
       "      <td>[\"16 eggs, beaten\", \"6 -8 evaporated milk, 360...</td>\n",
       "      <td>[\"Distribute rolls and cinnamon roll in (4) 2\\...</td>\n",
       "      <td>150.0</td>\n",
       "      <td>Very slow (90+ mins)</td>\n",
       "      <td>['eggs', 'milk', 'sugar', 'vanilla', 'gallon w...</td>\n",
       "      <td>False</td>\n",
       "      <td>['eggs', 'milk']</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>49.2</td>\n",
       "      <td>medium</td>\n",
       "      <td>['seafood', 'dairy', 'sweetener', 'spice/herb'...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Creamy Tuna And Bacon Salad (Reduced Fat)</td>\n",
       "      <td>['tuna', 'mayonnaise', 'low-fat sour cream', '...</td>\n",
       "      <td>Hard</td>\n",
       "      <td>Average</td>\n",
       "      <td>Cheap</td>\n",
       "      <td>Other</td>\n",
       "      <td>[\"1 (6 ounce) can tuna, drained and flaked\", \"...</td>\n",
       "      <td>[\"Put everything in a bowl and mix together un...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Fast (10-20 mins)</td>\n",
       "      <td>['tuna', 'mayonnaise', 'low-fat sour cream', '...</td>\n",
       "      <td>False</td>\n",
       "      <td>['tuna', 'low-fat sour cream', 'bacon bits']</td>\n",
       "      <td>False</td>\n",
       "      <td>['tuna', 'bacon bits']</td>\n",
       "      <td>52.5</td>\n",
       "      <td>expensive</td>\n",
       "      <td>['seafood', 'condiment', None, 'meat', 'spice/...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spinach And Mushroom Lasagna</td>\n",
       "      <td>['olive oil', 'onion', 'garlic', 'salt', 'grou...</td>\n",
       "      <td>Hard</td>\n",
       "      <td>Quick</td>\n",
       "      <td>Cheap</td>\n",
       "      <td>Bake</td>\n",
       "      <td>[\"1 tablespoon olive oil\", \"1 medium onion , c...</td>\n",
       "      <td>[\"Heat oven to 375 degrees.\", \"\", \"In medium s...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Slow (40-90 mins)</td>\n",
       "      <td>['olive oil', 'onion', 'garlic', 'salt', 'grou...</td>\n",
       "      <td>False</td>\n",
       "      <td>['ricotta cheese', 'Parmesan cheese', 'egg']</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>78.0</td>\n",
       "      <td>rich</td>\n",
       "      <td>['condiment', 'vegetable', 'vegetable', 'condi...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three-Bean Tacos</td>\n",
       "      <td>['olive oil', 'onion', 'red bell pepper', 'gre...</td>\n",
       "      <td>Hard</td>\n",
       "      <td>Quick</td>\n",
       "      <td>Cheap</td>\n",
       "      <td>Boil</td>\n",
       "      <td>[\"1 teaspoon olive oil\", \"1 cup diced onion\", ...</td>\n",
       "      <td>[\"Heat oil in a large skillet over medium-high...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Medium (20-40 mins)</td>\n",
       "      <td>['olive oil', 'onion', 'red bell pepper', 'gre...</td>\n",
       "      <td>False</td>\n",
       "      <td>['cheddar cheese']</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>87.5</td>\n",
       "      <td>rich</td>\n",
       "      <td>['condiment', 'vegetable', 'vegetable', 'veget...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hearty Hamburger Soup</td>\n",
       "      <td>['lean ground beef', 'white onion', 'ground bl...</td>\n",
       "      <td>Hard</td>\n",
       "      <td>Quick</td>\n",
       "      <td>Expensive</td>\n",
       "      <td>Boil</td>\n",
       "      <td>[\"900.0 gs lean ground beef\", \"1 white onion, ...</td>\n",
       "      <td>[\"Brown ground beef and onion in a large pot. ...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Slow (40-90 mins)</td>\n",
       "      <td>['lean ground beef', 'white onion', 'ground bl...</td>\n",
       "      <td>False</td>\n",
       "      <td>['lean ground beef', 'parmesan cheese', 'sour ...</td>\n",
       "      <td>False</td>\n",
       "      <td>['lean ground beef']</td>\n",
       "      <td>96.3</td>\n",
       "      <td>rich</td>\n",
       "      <td>['meat', 'vegetable', 'condiment', 'spice/herb...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44618</th>\n",
       "      <td>California Chili Powder Recipe</td>\n",
       "      <td>['Mexico chilies', 'grnd cumin', 'cayenne', 'o...</td>\n",
       "      <td>Easy</td>\n",
       "      <td>Average</td>\n",
       "      <td>Cheap</td>\n",
       "      <td>Other</td>\n",
       "      <td>[\"0.25 c. grnd Calif or possibly New Mexico ch...</td>\n",
       "      <td>[\"In a 0.3333333333- to 0.5-c. tall, narrow cl...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Medium (20-40 mins)</td>\n",
       "      <td>['Mexico chilies', 'grnd cumin', 'cayenne', 'o...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>78.8</td>\n",
       "      <td>rich</td>\n",
       "      <td>[None, 'spice/herb', 'spice/herb', 'spice/herb...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44619</th>\n",
       "      <td>Pizza Margherita</td>\n",
       "      <td>['batch pizza', 'marinara sauce', 'mozzarella'...</td>\n",
       "      <td>Easy</td>\n",
       "      <td>Quick</td>\n",
       "      <td>Cheap</td>\n",
       "      <td>Bake</td>\n",
       "      <td>[\"1 batch pizza dough (see preceding recipe)\",...</td>\n",
       "      <td>[\"Preheat oven to 450 degrees F. Place a pizza...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Fast (10-20 mins)</td>\n",
       "      <td>['batch pizza', 'marinara sauce', 'mozzarella'...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>18.0</td>\n",
       "      <td>very cheap</td>\n",
       "      <td>['processed', 'condiment', 'dairy', None]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44620</th>\n",
       "      <td>Frenchish Chicken And Red Wine Casserole</td>\n",
       "      <td>['tomatoes', 'red wine', 'chicken stock', 'gar...</td>\n",
       "      <td>Hard</td>\n",
       "      <td>Slow</td>\n",
       "      <td>Cheap</td>\n",
       "      <td>Boil</td>\n",
       "      <td>[\"1 (14 ounce) can chopped tomatoes\", \"1.25 cu...</td>\n",
       "      <td>[\"Place the tomatoes, wine, stock, garlic, ros...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Very slow (90+ mins)</td>\n",
       "      <td>['tomatoes', 'red wine', 'chicken stock', 'gar...</td>\n",
       "      <td>False</td>\n",
       "      <td>['chicken stock', 'chicken thighs', 'chicken']</td>\n",
       "      <td>False</td>\n",
       "      <td>['chicken stock', 'chicken thighs', 'chicken']</td>\n",
       "      <td>60.3</td>\n",
       "      <td>expensive</td>\n",
       "      <td>['vegetable', 'processed', 'processed', 'veget...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44621</th>\n",
       "      <td>Blueberry Almond Streusel Muffins Recipe</td>\n",
       "      <td>['flour', 'sugar', 'baking pwdr', 'baking soda...</td>\n",
       "      <td>Hard</td>\n",
       "      <td>Average</td>\n",
       "      <td>Cheap</td>\n",
       "      <td>Bake</td>\n",
       "      <td>[\"2.5 c. all-purpose flour\", \"1 c. granulated ...</td>\n",
       "      <td>[\"Preheat oven to 400 .\", \"Combine first 5 ing...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Fast (10-20 mins)</td>\n",
       "      <td>['flour', 'sugar', 'baking pwdr', 'baking soda...</td>\n",
       "      <td>False</td>\n",
       "      <td>['milk', 'low-fat buttermilk', 'light Ricotta ...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>91.4</td>\n",
       "      <td>rich</td>\n",
       "      <td>['processed', 'sweetener', 'processed', 'spice...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44622</th>\n",
       "      <td>Sugar Cream Pie</td>\n",
       "      <td>['Pastry', 'cornstarch', 'milk', 'butter', 'va...</td>\n",
       "      <td>Hard</td>\n",
       "      <td>Quick</td>\n",
       "      <td>Cheap</td>\n",
       "      <td>Bake</td>\n",
       "      <td>[\"Pastry for single-crust pie (22.5 cmes)\", \"1...</td>\n",
       "      <td>[\"Preheat oven to 232Â°C. Roll out dough to fit...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>Medium (20-40 mins)</td>\n",
       "      <td>['Pastry', 'cornstarch', 'milk', 'butter', 'va...</td>\n",
       "      <td>False</td>\n",
       "      <td>['milk', 'butter']</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>52.0</td>\n",
       "      <td>expensive</td>\n",
       "      <td>[None, 'processed', 'dairy', 'dairy', 'spice/h...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44623 rows Ã 20 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load File and create a new dataframe called 'df'\n",
    "df = pd.read_csv('full_tagged_dataset_2%.csv')\n",
    "\n",
    "# Fare merge dei 5 file che contengono l'intero dataset\n",
    "\n",
    "df"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Definizione dei metodi per le conversioni di frazioni e unitÃ  di misura e applicazione di essi sul dataframe",
   "id": "8ed7947e54cfa40b"
  },
  {
   "metadata": {
    "tags": [
     "Works"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-23T14:39:38.550484Z",
     "start_time": "2025-06-23T14:39:38.537331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from fractions import Fraction\n",
    "\n",
    "\n",
    "# === UTILITY FUNCTIONS ===\n",
    "\n",
    "def convert_fractions(text):\n",
    "    \"\"\"Convert fractions in the format '1 1/2' or '3/4' to decimals (e.g., '1.5', '0.75').\"\"\"\n",
    "    # Match optional whole number + fraction, or just fraction\n",
    "    pattern = r'(?:(\\d+)\\s+)?(\\d+)/(\\d+)'\n",
    "\n",
    "    def replacer(match):\n",
    "        whole = int(match.group(1)) if match.group(1) else 0\n",
    "        numerator = int(match.group(2))\n",
    "        denominator = int(match.group(3))\n",
    "        decimal = whole + Fraction(numerator, denominator)\n",
    "        return \"{:.10g}\".format(float(decimal))  # Clean, no trailing zeros\n",
    "\n",
    "    return re.sub(pattern, replacer, text)\n",
    "\n",
    "\n",
    "# Fixed conversions\n",
    "CONVERSIONS = {\n",
    "    \"oz\": 30,      # ounce â gram\n",
    "    \"lb\": 450,      # pounds â gram\n",
    "    \"pt\": 475,      # pint â milliliter\n",
    "    \"qt\": 950,      # quart â milliliter\n",
    "    \"inch\": 2.5        # inches â centimeter\n",
    "}\n",
    "\n",
    "def unit_to_metric(unit):\n",
    "    \"\"\"Return the metric equivalent of the given imperial unit.\"\"\"\n",
    "    return {\n",
    "        \"oz\": \"g\",\n",
    "        \"lb\": \"g\",\n",
    "        \"pt\": \"ml\",\n",
    "        \"qt\": \"ml\",\n",
    "        \"inch\": \"cm\"\n",
    "    }[unit]\n",
    "\n",
    "def convert_units(text):\n",
    "    \"\"\"Convert imperial units (oz, lb, pt, qt, inch) to metric units in the text.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "\n",
    "    # Convert oz, lb, pt, qt (form: \"4 oz\", \"2.5 lb\", etc.)\n",
    "    for unit, factor in CONVERSIONS.items():\n",
    "        pattern = r'(\\d+(\\.\\d+)?)\\s*' + unit\n",
    "        text = re.sub(pattern, lambda m: f\"{round(float(m.group(1)) * factor, 1)} {unit_to_metric(unit)}\", text)\n",
    "\n",
    "    # Convert measure in inches in pans\n",
    "    text = re.sub(r'(\\d+)\\s*x\\s*(\\d+)[-\\s]*inch', lambda\n",
    "        m: f\"{int(m.group(1)) * CONVERSIONS['inch']:.0f} x {int(m.group(2)) * CONVERSIONS['inch']:.0f} cm\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def convert_fahrenheit_to_celsius(text):\n",
    "    \"\"\"Convert Fahrenheit temperatures to Celsius (e.g., 350Â°F â 177Â°C).\"\"\"\n",
    "    # Match degrees like 275Â°, 275 Â°F, 275\\u00b0F (escaped), etc.\n",
    "    pattern = r'(\\d+)\\s*(?:Â°|\\\\u00b0|degrees)'\n",
    "\n",
    "    def replacer(match):\n",
    "        fahrenheit = int(match.group(1))\n",
    "        celsius = round((fahrenheit - 32) * 5 / 9)\n",
    "        return f\"{celsius}Â°C\"\n",
    "\n",
    "    return re.sub(pattern, replacer, text)\n",
    "\n",
    "# righe non utilizzabili perche le colonne non sono presenti nel dataset gia taggato, risolvere come ti avevo gia detto\n",
    "'''\n",
    "# === APPLY TRANSFORMATIONS ===\n",
    "\n",
    "# Convert fractions and units in ingredients and directions\n",
    "df['INGREDIENTS'] = df['ingredients'].apply(convert_fractions).apply(convert_units)\n",
    "df['DIRECTIONS'] = df['directions'].apply(convert_fractions).apply(convert_units).apply(convert_fahrenheit_to_celsius)\n",
    "\n",
    "# Rename title column and drop raw ingredients/directions columns\n",
    "df = df.rename(columns={\"title\": \"TITLE\"}).drop(columns=[\"ingredients\", \"directions\"])\n",
    "'''"
   ],
   "id": "f6177b908ebf022c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# === APPLY TRANSFORMATIONS ===\\n\\n# Convert fractions and units in ingredients and directions\\ndf[\\'INGREDIENTS\\'] = df[\\'ingredients\\'].apply(convert_fractions).apply(convert_units)\\ndf[\\'DIRECTIONS\\'] = df[\\'directions\\'].apply(convert_fractions).apply(convert_units).apply(convert_fahrenheit_to_celsius)\\n\\n# Rename title column and drop raw ingredients/directions columns\\ndf = df.rename(columns={\"title\": \"TITLE\"}).drop(columns=[\"ingredients\", \"directions\"])\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Definizione di una euristica in grado di categorizzare il tempo di preparazione di una ricetta",
   "id": "7836a60998abffef"
  },
  {
   "metadata": {
    "tags": [
     "Works"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-23T14:47:34.579363Z",
     "start_time": "2025-06-23T14:47:33.007240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "# Patterns to detect time expressions\n",
    "TIME_PATTERN = re.compile(\n",
    "    r'(\\d+(?:\\.\\d+)?)\\s*(minute|minutes|min|mins|m|hour|hours|hr|hrs|h|day|days|d)\\b',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "RANGE_PATTERN = re.compile(\n",
    "    r'(\\d+)\\s*-\\s*(\\d+)\\s*(minute|minutes|min|mins|m|hour|hours|hr|hrs|h)\\b',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Time estimates for common preparation methods\n",
    "PREP_ESTIMATES = {\n",
    "    'bake': 45, 'boil': 20, 'fry': 15, 'grill': 25, 'chill': 120,\n",
    "    'simmer': 30, 'marinate': 60, 'microwave': 10, 'no-bake': 20,\n",
    "    'refrigerate': 180, 'freeze': 240\n",
    "}\n",
    "\n",
    "# Special cases for recipe types\n",
    "RECIPE_TYPE_ESTIMATES = {\n",
    "    'pasta': 10, 'salad': 15, 'cake': 45, 'pie': 60, 'stew': 120,\n",
    "    'casserole': 60, 'soup': 30, 'cookies': 30, 'bread': 90,\n",
    "    'fudge': 20, 'candy': 30\n",
    "}\n",
    "\n",
    "def convert_to_minutes(qty, unit):\n",
    "    \"\"\"Converts time quantity to minutes.\"\"\"\n",
    "    qty = float(qty)\n",
    "    unit = unit.lower()\n",
    "    if unit.startswith('m'): return qty\n",
    "    if unit.startswith('h'): return qty * 60\n",
    "    if unit.startswith('d'): return qty * 1440\n",
    "    return 0\n",
    "\n",
    "def estimate_by_recipe_type(title, ingredients):\n",
    "    \"\"\"Fallback estimation based on recipe title or ingredients.\"\"\"\n",
    "    title = str(title).lower()\n",
    "    ingredients = str(ingredients).lower()\n",
    "\n",
    "    # Check for specific recipe types\n",
    "    for keyword, est in RECIPE_TYPE_ESTIMATES.items():\n",
    "        if keyword in title:\n",
    "            return est\n",
    "\n",
    "    if any(word in ingredients for word in ['raw', 'fresh']):\n",
    "        return 15\n",
    "    if 'frozen' in ingredients:\n",
    "        return 20\n",
    "    if 'canned' in ingredients:\n",
    "        return 10\n",
    "\n",
    "    # Default fallback\n",
    "    return 30\n",
    "\n",
    "def clean_instruction_step(step):\n",
    "    \"\"\"Normalize range formats and identify special cases.\"\"\"\n",
    "    # Replace ranges (e.g., \"10-15 minutes\" â \"15 minutes\")\n",
    "    step = RANGE_PATTERN.sub(lambda m: f\"{m.group(2)} {m.group(3)}\", step)\n",
    "\n",
    "    # Handle special keywords\n",
    "    lowered = step.lower()\n",
    "    if 'overnight' in lowered:\n",
    "        return 480\n",
    "    if 'until set' in lowered or 'until firm' in lowered:\n",
    "        return 60\n",
    "\n",
    "    # Sum all time expressions\n",
    "    return sum(\n",
    "        convert_to_minutes(qty, unit)\n",
    "        for qty, unit in TIME_PATTERN.findall(step)\n",
    "    )\n",
    "\n",
    "def parse_instructions(instructions):\n",
    "    \"\"\"Parses instructions and extracts total estimated time in minutes.\"\"\"\n",
    "    total_time = 0\n",
    "\n",
    "    if not isinstance(instructions, list):\n",
    "        try:\n",
    "            instructions = ast.literal_eval(str(instructions))\n",
    "        except:\n",
    "            instructions = [instructions]\n",
    "\n",
    "    for step in instructions:\n",
    "        if isinstance(step, str):\n",
    "            total_time += clean_instruction_step(step)\n",
    "\n",
    "    return total_time\n",
    "\n",
    "def categorize_time(total_time):\n",
    "    \"\"\"Classify total time into labeled categories.\"\"\"\n",
    "    if total_time == 0:\n",
    "        return 'Not specified'\n",
    "    if total_time < 10:\n",
    "        return 'Very fast (0-10 mins)'\n",
    "    if total_time < 20:\n",
    "        return 'Fast (10-20 mins)'\n",
    "    if total_time < 40:\n",
    "        return 'Medium (20-40 mins)'\n",
    "    if total_time < 90:\n",
    "        return 'Slow (40-90 mins)'\n",
    "    return 'Very slow (90+ mins)'\n",
    "\n",
    "# Calculate total preparation time\n",
    "df['total_time'] = df['DIRECTIONS'].apply(parse_instructions)\n",
    "\n",
    "# Estimate time for unspecified recipes\n",
    "missing = df['total_time'] == 0\n",
    "df.loc[missing, 'total_time'] = df[missing].apply(\n",
    "    lambda row: estimate_by_recipe_type(row['TITLE'], row['INGREDIENTS']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Categorize preparation times\n",
    "df['PREPARATION_TIME'] = df['total_time'].apply(categorize_time)\n",
    "\n",
    "df[['TITLE', 'INGREDIENTS', 'DIRECTIONS', 'PREPARATION_TIME']].head(200)"
   ],
   "id": "4ca47d472c93a067",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 TITLE  \\\n",
       "0                        Western Sizzlin Bread Pudding   \n",
       "1            Creamy Tuna And Bacon Salad (Reduced Fat)   \n",
       "2                        Spinach And Mushroom Lasagna    \n",
       "3                                     Three-Bean Tacos   \n",
       "4                                Hearty Hamburger Soup   \n",
       "..                                                 ...   \n",
       "195                Sweet Potato Brotchen (Bread Rolls)   \n",
       "196  Green Cleaner and Bug Repellant in One (Concen...   \n",
       "197                                 Chocolate Chip Pie   \n",
       "198                                Poppy Seed Dressing   \n",
       "199                                    Scallop Ceviche   \n",
       "\n",
       "                                           INGREDIENTS  \\\n",
       "0    [\"16 eggs, beaten\", \"6 -8 evaporated milk, 360...   \n",
       "1    [\"1 (6 ounce) can tuna, drained and flaked\", \"...   \n",
       "2    [\"1 tablespoon olive oil\", \"1 medium onion , c...   \n",
       "3    [\"1 teaspoon olive oil\", \"1 cup diced onion\", ...   \n",
       "4    [\"900.0 gs lean ground beef\", \"1 white onion, ...   \n",
       "..                                                 ...   \n",
       "195  [\"450 g sweet potatoes\", \"15 g butter or olive...   \n",
       "196  [\"1 tablespoon essential oil (part)\", \"3 table...   \n",
       "197  [\"1 graham cracker crust\", \"20 large marshmall...   \n",
       "198  [\"0.75 c. sugar\", \"2 tsp. dry mustard\", \"2 tsp...   \n",
       "199  [\"1 pound sea scallops\", \"Salt\", \"1 cup choppe...   \n",
       "\n",
       "                                            DIRECTIONS       PREPARATION_TIME  \n",
       "0    [\"Distribute rolls and cinnamon roll in (4) 2\\...   Very slow (90+ mins)  \n",
       "1    [\"Put everything in a bowl and mix together un...      Fast (10-20 mins)  \n",
       "2    [\"Heat oven to 375 degrees.\", \"\", \"In medium s...      Slow (40-90 mins)  \n",
       "3    [\"Heat oil in a large skillet over medium-high...    Medium (20-40 mins)  \n",
       "4    [\"Brown ground beef and onion in a large pot. ...      Slow (40-90 mins)  \n",
       "..                                                 ...                    ...  \n",
       "195  [\"Peel and cut sweet potatoes into chunks.\", \"...   Very slow (90+ mins)  \n",
       "196  [\"Mix all ingredients.\", \"Store in a glass jar...    Medium (20-40 mins)  \n",
       "197  [\"Make crust (I buy the prepared ones from the...      Slow (40-90 mins)  \n",
       "198  [\"Combine first 6 ingredients in a blender con...  Very fast (0-10 mins)  \n",
       "199  [\"Bring a pot of water to a boil, salt the wat...      Fast (10-20 mins)  \n",
       "\n",
       "[200 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>INGREDIENTS</th>\n",
       "      <th>DIRECTIONS</th>\n",
       "      <th>PREPARATION_TIME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Western Sizzlin Bread Pudding</td>\n",
       "      <td>[\"16 eggs, beaten\", \"6 -8 evaporated milk, 360...</td>\n",
       "      <td>[\"Distribute rolls and cinnamon roll in (4) 2\\...</td>\n",
       "      <td>Very slow (90+ mins)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Creamy Tuna And Bacon Salad (Reduced Fat)</td>\n",
       "      <td>[\"1 (6 ounce) can tuna, drained and flaked\", \"...</td>\n",
       "      <td>[\"Put everything in a bowl and mix together un...</td>\n",
       "      <td>Fast (10-20 mins)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spinach And Mushroom Lasagna</td>\n",
       "      <td>[\"1 tablespoon olive oil\", \"1 medium onion , c...</td>\n",
       "      <td>[\"Heat oven to 375 degrees.\", \"\", \"In medium s...</td>\n",
       "      <td>Slow (40-90 mins)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three-Bean Tacos</td>\n",
       "      <td>[\"1 teaspoon olive oil\", \"1 cup diced onion\", ...</td>\n",
       "      <td>[\"Heat oil in a large skillet over medium-high...</td>\n",
       "      <td>Medium (20-40 mins)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hearty Hamburger Soup</td>\n",
       "      <td>[\"900.0 gs lean ground beef\", \"1 white onion, ...</td>\n",
       "      <td>[\"Brown ground beef and onion in a large pot. ...</td>\n",
       "      <td>Slow (40-90 mins)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Sweet Potato Brotchen (Bread Rolls)</td>\n",
       "      <td>[\"450 g sweet potatoes\", \"15 g butter or olive...</td>\n",
       "      <td>[\"Peel and cut sweet potatoes into chunks.\", \"...</td>\n",
       "      <td>Very slow (90+ mins)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Green Cleaner and Bug Repellant in One (Concen...</td>\n",
       "      <td>[\"1 tablespoon essential oil (part)\", \"3 table...</td>\n",
       "      <td>[\"Mix all ingredients.\", \"Store in a glass jar...</td>\n",
       "      <td>Medium (20-40 mins)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Chocolate Chip Pie</td>\n",
       "      <td>[\"1 graham cracker crust\", \"20 large marshmall...</td>\n",
       "      <td>[\"Make crust (I buy the prepared ones from the...</td>\n",
       "      <td>Slow (40-90 mins)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Poppy Seed Dressing</td>\n",
       "      <td>[\"0.75 c. sugar\", \"2 tsp. dry mustard\", \"2 tsp...</td>\n",
       "      <td>[\"Combine first 6 ingredients in a blender con...</td>\n",
       "      <td>Very fast (0-10 mins)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Scallop Ceviche</td>\n",
       "      <td>[\"1 pound sea scallops\", \"Salt\", \"1 cup choppe...</td>\n",
       "      <td>[\"Bring a pot of water to a boil, salt the wat...</td>\n",
       "      <td>Fast (10-20 mins)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## La cella sottostante contiene una logica per calcolare la categoria di costo di una ricetta, che Ã¨ stata poi sostituita da un metodo piÃ¹ efficace",
   "id": "474fb8ab5e198a53"
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "# Conversione unitÃ  â kg\n",
    "UNIT_CONVERSION = {\n",
    "    'cup': 0.24, 'c.': 0.24, 'c': 0.24,\n",
    "    'teaspoon': 0.005, 'tsp': 0.005, 'tsp.': 0.005,\n",
    "    'tablespoon': 0.015, 'tbsp': 0.015, 'tbsp.': 0.015,\n",
    "    'package': 0.2, 'pkg': 0.2, 'pkg.': 0.2,\n",
    "    'can': 0.4, 'carton': 1.0,\n",
    "    'g': 0.001, 'g.': 0.001, 'gram': 0.001,\n",
    "    'kg': 1.0, 'kilogram': 1.0,\n",
    "    'lb': 0.4536, 'pound': 0.4536,\n",
    "    'oz': 0.02835, 'ounce': 0.02835,\n",
    "    'large package': 0.5, 'large pkg.': 0.5, 'large pkg': 0.5\n",
    "}\n",
    "\n",
    "# 3. Parsing function (qty, unit, name, grams_in_paren)\n",
    "def parse_ingredient(ing_str):\n",
    "    # peso fra parentesi\n",
    "    m = re.search(r'\\(\\s*([0-9]+(?:\\.[0-9]+)?)\\s*g\\.?\\s*\\)', ing_str, re.IGNORECASE)\n",
    "    grams = float(m.group(1)) if m else None\n",
    "\n",
    "    # quantity e unit\n",
    "    patt = r'^\\s*([\\d\\/.\\s]+)?\\s*([a-zA-Z\\.]+)?'\n",
    "    m2 = re.match(patt, ing_str)\n",
    "    qty_raw = m2.group(1) if m2 else None\n",
    "    try:\n",
    "        qty = eval(qty_raw.replace(' ', '+')) if qty_raw else 1.0\n",
    "    except:\n",
    "        qty = 1.0\n",
    "    unit = (m2.group(2) or '').strip('.').lower() if m2 else ''\n",
    "\n",
    "    # name pulito\n",
    "    name = re.sub(r'\\(.*?\\)|optional', '', ing_str, flags=re.IGNORECASE)\n",
    "    # rimuovo quantitÃ /unitÃ \n",
    "    name = re.sub(r'^[\\d\\/.\\s]+\\s*[a-zA-Z\\.]*', '', name).strip().lower()\n",
    "    return qty, unit, name, grams\n",
    "\n",
    "# 4. Calcolo posizionale con fallback substring e token\n",
    "def calculate_recipe_cost_positional(ingredients_list, ner_list):\n",
    "    total = 0.0\n",
    "    missing = []\n",
    "\n",
    "    for ing_str, ner_item in zip(ingredients_list, ner_list):\n",
    "        key = ner_item.lower()\n",
    "        qty, unit, name, grams = parse_ingredient(ing_str)\n",
    "\n",
    "        # scorporo se passo i grams\n",
    "        if grams is not None:\n",
    "            kg = (grams / 1000) * qty\n",
    "        elif unit == '':  # a pezzo\n",
    "            # prendo prezzo direct key\n",
    "            price = price_dict.get(key)\n",
    "            # fallback substring/token\n",
    "            if price is None:\n",
    "                # substring match\n",
    "                for k, v in price_dict.items():\n",
    "                    if k in key or key in k:\n",
    "                        price = v\n",
    "                        break\n",
    "                # token match\n",
    "                if price is None:\n",
    "                    for token in key.split():\n",
    "                        if token in price_dict:\n",
    "                            price = price_dict[token]\n",
    "                            break\n",
    "            if price:\n",
    "                total += qty * price\n",
    "            else:\n",
    "                missing.append(key)\n",
    "            continue\n",
    "        else:\n",
    "            conv = UNIT_CONVERSION.get(unit, 0.0)\n",
    "            if conv == 0:\n",
    "                missing.append(key)\n",
    "                continue\n",
    "            kg = qty * conv\n",
    "\n",
    "        # prezzo per kg\n",
    "        price = price_dict.get(key)\n",
    "        if price is None:\n",
    "            for k, v in price_dict.items():\n",
    "                if k in key or key in k:\n",
    "                    price = v\n",
    "                    break\n",
    "        if price is None:\n",
    "            missing.append(key)\n",
    "            continue\n",
    "\n",
    "        total += kg * price\n",
    "\n",
    "    return round(total, 2), missing\n",
    "\n",
    "# 5. Applica al DataFrame\n",
    "    df['INGREDIENTS'] = df['INGREDIENTS'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    df['NER_clean']  = df['NER_clean'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "def compute_cost_row(row):\n",
    "     cost, _ = calculate_recipe_cost_positional(row['INGREDIENTS'], row['NER_clean'])\n",
    "     return cost\n",
    "\n",
    "df['total_cost'] = df.apply(compute_cost_row, axis=1)\n",
    "\n",
    "df['CATEGORY_COST'] = df['total_cost'].apply(categorize_cost)\n",
    "\n",
    "# Mostra risultati\n",
    "df[['INGREDIENTS', 'NER', 'CATEGORY_COST']].head(200)'''"
   ],
   "metadata": {
    "collapsed": false,
    "tags": [
     "potrebbe essere eliminata"
    ]
   },
   "id": "99c5e6f021ec1a8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Le celle successive sono in grado di determinare se una ricetta appartiene alle categorie 'vegan' o 'vegetarian'",
   "id": "a21050239bedf7a3"
  },
  {
   "cell_type": "code",
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "# Cleaning the NER column from special symbols\n",
    "df['NER_clean'] = df['NER'].apply(ast.literal_eval)\n",
    "\n",
    "# List containing keywords\n",
    "NON_VEGAN_KEYWORDS = {\n",
    "    # meat\n",
    "    'meat', 'red meat', 'white meat', 'poultry', 'veal', 'lamb', 'duck',\n",
    "    'goose', 'chicken', 'beef', 'pork', 'bacon', 'ham', 'prosciutto',\n",
    "    'sausage', 'salami', 'hot dog', 'pepperoni', 'mortadella', 'chorizo',\n",
    "    'ground beef', 'minced meat', 'steak', 'ribs', 'brisket', 'lardons',\n",
    "\n",
    "    # fish or seafood\n",
    "    'fish', 'anchovy', 'anchovies', 'tuna', 'salmon', 'sardines', 'mackerel',\n",
    "    'cod', 'haddock', 'tilapia', 'trout', 'snapper', 'catfish', 'bass',\n",
    "    'shellfish', 'shrimp', 'prawns', 'crab', 'lobster', 'scampi', 'clam',\n",
    "    'mussels', 'oysters', 'scallops', 'caviar', 'roe',\n",
    "\n",
    "    # dairy\n",
    "    'milk', 'whole milk', 'skim milk', 'cheese', 'butter', 'cream',\n",
    "    'heavy cream', 'sour cream', 'whipped cream', 'yogurt', 'ghee',\n",
    "    'lactose', 'casein', 'whey', 'buttermilk', 'condensed milk',\n",
    "    'evaporated milk', 'parmesan', 'mozzarella', 'cheddar',\n",
    "    'shredded cheese', 'blue cheese', 'goat cheese', 'cream cheese',\n",
    "    'cottage cheese', 'ricotta', 'cocoa butter',\n",
    "\n",
    "    # eggs\n",
    "    'egg', 'eggs', 'egg yolk', 'egg white', 'albumen', 'mayonnaise',\n",
    "\n",
    "    # bees\n",
    "    'honey', 'royal jelly', 'propolis', 'bee pollen',\n",
    "\n",
    "    # animal derivatives\n",
    "    'gelatin', 'gelatina', 'collagen', 'isinglass', 'lard', 'suet',\n",
    "    'carmine', 'shellac', 'pepsin', 'bone broth', 'bone marrow',\n",
    "    'broth (meat)', 'stock (meat)', 'chicken broth', 'beef broth',\n",
    "    'animal fat'\n",
    "}\n",
    "\n",
    "VEGAN_EXCEPTIONS = {\n",
    "    'milk': {'soy', 'almond', 'oat', 'rice', 'coconut', 'cashew', 'hazelnut',\n",
    "             'hemp', 'pea', 'flax', 'macadamia', 'barley', 'plant-based milk'},\n",
    "    'cheese': {'vegan cheese', 'nutritional yeast', 'cashew cheese', 'tofu cheese',\n",
    "               'almond cheese', 'plant-based cheese'},\n",
    "    'meat': {'soy', 'seitan', 'tofu', 'tempeh', 'jackfruit', 'plant-based', 'textured soy protein', 'mushroom-based'},\n",
    "    'butter': {'vegan butter', 'plant butter', 'peanut butter', 'almond butter',\n",
    "               'sunflower butter', 'coconut butter', 'cashew butter'},\n",
    "    'cream': {'coconut cream', 'soy cream', 'oat cream', 'vegan cream', 'cashew cream'},\n",
    "    'bacon': {'vegan bacon', 'tempeh bacon', 'coconut bacon', 'mushroom bacon'},\n",
    "    'egg': {'flaxseed', 'chia', 'aquafaba', 'vegan egg', 'plant-based egg', 'apple sauce'}\n",
    "}\n",
    "\n",
    "VEGAN_MODIFIERS = {\n",
    "    'vegan', '100% vegan', 'plant-based', 'dairy-free', 'egg-free',\n",
    "    'no milk', 'no eggs', 'cruelty-free', 'without animal derivatives',\n",
    "    '100% plant-based', 'meat-free', 'no animal products'\n",
    "}\n",
    "\n",
    "# --- UTILITY (Used in vegan, vegetarian, lactose free and gluten free)\n",
    "def parse_ingredients(ingredients_input):\n",
    "    \"\"\"Always returning a parsed list of ingredients\"\"\"\n",
    "    if isinstance(ingredients_input, list):\n",
    "        return ingredients_input\n",
    "    if isinstance(ingredients_input, str) and ingredients_input.strip():\n",
    "        try:\n",
    "            return ast.literal_eval(ingredients_input)\n",
    "        except:\n",
    "            return [\n",
    "                x.strip().strip('\"').strip(\"'\")\n",
    "                for x in re.split(r',(?![^[]*\\])', ingredients_input.strip('[]'))\n",
    "                if x.strip()\n",
    "            ]\n",
    "    return []\n",
    "\n",
    "def contains_vegan_exception(ingredient, keyword):\n",
    "    return any(re.search(rf'\\b{re.escape(ex)}\\b', ingredient, re.IGNORECASE)\n",
    "               for ex in VEGAN_EXCEPTIONS.get(keyword, set()))\n",
    "\n",
    "# --- Classification\n",
    "def classify_vegan(ingredients_input):\n",
    "    \"\"\"Returning True or False based on the ingredients\"\"\"\n",
    "    ingredients = parse_ingredients(ingredients_input)\n",
    "    for ing in ingredients:\n",
    "        if not isinstance(ing, str):\n",
    "            continue\n",
    "        ing_lower = ing.lower()\n",
    "        if any(skip in ing_lower for skip in ('water', 'salt')):\n",
    "            continue\n",
    "        if any(re.search(rf'\\b{re.escape(mod)}\\b', ing_lower) for mod in VEGAN_MODIFIERS):\n",
    "            continue\n",
    "        for keyword in NON_VEGAN_KEYWORDS:\n",
    "            if re.search(rf'\\b{re.escape(keyword)}(?:s)?\\b', ing_lower):\n",
    "                if not contains_vegan_exception(ing_lower, keyword):\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "def get_non_vegan_ingredients(ingredients_input):\n",
    "    \"\"\"Returning list of ingredients without vegan exceptions\"\"\"\n",
    "    ingredients = parse_ingredients(ingredients_input)\n",
    "    non_vegan = []\n",
    "    for ing in ingredients:\n",
    "        if not isinstance(ing, str):\n",
    "            continue\n",
    "        ing_lower = ing.lower()\n",
    "        if any(skip in ing_lower for skip in ('water', 'salt')):\n",
    "            continue\n",
    "        for keyword in NON_VEGAN_KEYWORDS:\n",
    "            if re.search(rf'\\b{re.escape(keyword)}(?:s)?\\b', ing_lower):\n",
    "                if not contains_vegan_exception(ing_lower, keyword):\n",
    "                    non_vegan.append(ing)\n",
    "                break\n",
    "    return non_vegan\n",
    "\n",
    "# --- Apply\n",
    "df['VEGAN'] = df['NER_clean'].apply(classify_vegan)\n",
    "df['NON_VEGAN_INGREDIENTS'] = df['NER_clean'].apply(get_non_vegan_ingredients)\n",
    "\n",
    "# --- Final validation\n",
    "anti_patterns = re.compile(\n",
    "    r'\\b(not vegan|meat|steak|fish|cheese|egg|eggs|beef|chicken|pork|bacon|cream)\\b',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "df['VEGAN'] = df.apply(\n",
    "    lambda row: False if row['VEGAN'] and isinstance(row['TITLE'], str) and anti_patterns.search(row['TITLE']) else row['VEGAN'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# View results\n",
    "df[['TITLE', 'NER', 'VEGAN', 'NON_VEGAN_INGREDIENTS']].head(200)"
   ],
   "metadata": {
    "collapsed": false,
    "tags": [
     "Works"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-23T15:08:50.305716Z",
     "start_time": "2025-06-23T15:07:43.195481Z"
    }
   },
   "id": "fd85d06e00f4b10c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 TITLE  \\\n",
       "0                        Western Sizzlin Bread Pudding   \n",
       "1            Creamy Tuna And Bacon Salad (Reduced Fat)   \n",
       "2                        Spinach And Mushroom Lasagna    \n",
       "3                                     Three-Bean Tacos   \n",
       "4                                Hearty Hamburger Soup   \n",
       "..                                                 ...   \n",
       "195                Sweet Potato Brotchen (Bread Rolls)   \n",
       "196  Green Cleaner and Bug Repellant in One (Concen...   \n",
       "197                                 Chocolate Chip Pie   \n",
       "198                                Poppy Seed Dressing   \n",
       "199                                    Scallop Ceviche   \n",
       "\n",
       "                                                   NER  VEGAN  \\\n",
       "0    ['eggs', 'milk', 'sugar', 'vanilla', 'gallon w...  False   \n",
       "1    ['tuna', 'mayonnaise', 'low-fat sour cream', '...  False   \n",
       "2    ['olive oil', 'onion', 'garlic', 'salt', 'grou...  False   \n",
       "3    ['olive oil', 'onion', 'red bell pepper', 'gre...  False   \n",
       "4    ['lean ground beef', 'white onion', 'ground bl...  False   \n",
       "..                                                 ...    ...   \n",
       "195  ['sweet potatoes', 'butter', 'salt', 'flour', ...  False   \n",
       "196          ['essential oil', 'vodka', 'liquid soap']   True   \n",
       "197  ['graham cracker crust', 'marshmallows', 'milk...  False   \n",
       "198  ['sugar', 'dry mustard', 'salt', 'vinegar', 'o...   True   \n",
       "199  ['scallops', 'Salt', 'ginger', 'garlic', 'stal...  False   \n",
       "\n",
       "                                 NON_VEGAN_INGREDIENTS  \n",
       "0                                         [eggs, milk]  \n",
       "1    [tuna, mayonnaise, low-fat sour cream, bacon b...  \n",
       "2    [ricotta cheese, Parmesan cheese, egg, mozzare...  \n",
       "3                                     [cheddar cheese]  \n",
       "4      [lean ground beef, parmesan cheese, sour cream]  \n",
       "..                                                 ...  \n",
       "195                                           [butter]  \n",
       "196                                                 []  \n",
       "197                             [milk, whipping cream]  \n",
       "198                                                 []  \n",
       "199                                         [scallops]  \n",
       "\n",
       "[200 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>NER</th>\n",
       "      <th>VEGAN</th>\n",
       "      <th>NON_VEGAN_INGREDIENTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Western Sizzlin Bread Pudding</td>\n",
       "      <td>['eggs', 'milk', 'sugar', 'vanilla', 'gallon w...</td>\n",
       "      <td>False</td>\n",
       "      <td>[eggs, milk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Creamy Tuna And Bacon Salad (Reduced Fat)</td>\n",
       "      <td>['tuna', 'mayonnaise', 'low-fat sour cream', '...</td>\n",
       "      <td>False</td>\n",
       "      <td>[tuna, mayonnaise, low-fat sour cream, bacon b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spinach And Mushroom Lasagna</td>\n",
       "      <td>['olive oil', 'onion', 'garlic', 'salt', 'grou...</td>\n",
       "      <td>False</td>\n",
       "      <td>[ricotta cheese, Parmesan cheese, egg, mozzare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three-Bean Tacos</td>\n",
       "      <td>['olive oil', 'onion', 'red bell pepper', 'gre...</td>\n",
       "      <td>False</td>\n",
       "      <td>[cheddar cheese]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hearty Hamburger Soup</td>\n",
       "      <td>['lean ground beef', 'white onion', 'ground bl...</td>\n",
       "      <td>False</td>\n",
       "      <td>[lean ground beef, parmesan cheese, sour cream]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Sweet Potato Brotchen (Bread Rolls)</td>\n",
       "      <td>['sweet potatoes', 'butter', 'salt', 'flour', ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[butter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Green Cleaner and Bug Repellant in One (Concen...</td>\n",
       "      <td>['essential oil', 'vodka', 'liquid soap']</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Chocolate Chip Pie</td>\n",
       "      <td>['graham cracker crust', 'marshmallows', 'milk...</td>\n",
       "      <td>False</td>\n",
       "      <td>[milk, whipping cream]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Poppy Seed Dressing</td>\n",
       "      <td>['sugar', 'dry mustard', 'salt', 'vinegar', 'o...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Scallop Ceviche</td>\n",
       "      <td>['scallops', 'Salt', 'ginger', 'garlic', 'stal...</td>\n",
       "      <td>False</td>\n",
       "      <td>[scallops]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "tags": [
     "Works"
    ],
    "ExecuteTime": {
     "end_time": "2025-06-23T15:17:26.197903Z",
     "start_time": "2025-06-23T15:16:29.587067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# List of non vegetarian ingredients\n",
    "NON_VEGETARIAN_KEYWORDS = {\n",
    "    'meat', 'red meat', 'white meat', 'poultry', 'veal', 'lamb', 'duck',\n",
    "    'goose', 'chicken', 'beef', 'pork', 'bacon', 'ham', 'prosciutto',\n",
    "    'sausage', 'salami', 'hot dog', 'pepperoni', 'mortadella', 'chorizo',\n",
    "    'ground beef', 'minced meat', 'steak', 'ribs', 'brisket', 'lardons',\n",
    "    'fish', 'anchovy', 'anchovies', 'tuna', 'salmon', 'sardines', 'mackerel',\n",
    "    'cod', 'haddock', 'tilapia', 'trout', 'snapper', 'catfish', 'bass',\n",
    "    'shellfish', 'shrimp', 'prawns', 'crab', 'lobster', 'scampi', 'clam',\n",
    "    'mussels', 'oysters', 'scallops', 'caviar', 'roe',\n",
    "    'gelatin', 'lard', 'collagen', 'isinglass', 'pepsin', 'suet', 'bone broth',\n",
    "    'bone marrow', 'stock (meat)', 'bouillon (meat)', 'chicken broth', 'beef broth'\n",
    "}\n",
    "\n",
    "# Exceptions for vegetarian subs\n",
    "VEGETARIAN_EXCEPTIONS = {\n",
    "    'meat': {'soy', 'soy protein', 'textured vegetable protein', 'tvp', 'seitan', 'tofu',\n",
    "             'tempeh', 'jackfruit', 'lentils', 'chickpeas', 'plant-based', 'mycoprotein', 'quorn'},\n",
    "    'bacon': {'vegan bacon', 'tempeh bacon', 'coconut bacon', 'rice paper bacon'},\n",
    "    'gelatin': {'agar', 'pectin', 'carrageenan', 'guar gum', 'xanthan gum'},\n",
    "    'fish': {'banana blossom', 'tofu', 'plant-based', 'seaweed', 'nori', 'smoked carrot'}\n",
    "}\n",
    "\n",
    "VEGETARIAN_MODIFIERS = {\n",
    "    'vegetarian', 'veggie', 'plant-based', 'meatless', 'no meat',\n",
    "    'without meat', 'cruelty-free', 'ovo-vegetarian', 'lacto-vegetarian',\n",
    "    '100% vegetarian', 'meat-free', 'green option'\n",
    "}\n",
    "\n",
    "def contains_vegetarian_exception(ingredient, keyword):\n",
    "    exceptions = VEGETARIAN_EXCEPTIONS.get(keyword, set())\n",
    "    return any(re.search(rf'\\b{re.escape(ex)}\\b', ingredient, re.IGNORECASE) for ex in exceptions)\n",
    "\n",
    "# --- Classification\n",
    "def classify_vegetarian(ingredients_input):\n",
    "    \"\"\"Returning True or False based on the ingredients\"\"\"\n",
    "    ingredients = parse_ingredients(ingredients_input) # using the function 'parse_ingredients' defined in the previous cell\n",
    "    for ing in ingredients:\n",
    "        if not isinstance(ing, str):\n",
    "            continue\n",
    "        ing_lower = ing.lower()\n",
    "        if any(skip in ing_lower for skip in ('water', 'salt')):\n",
    "            continue\n",
    "        if any(re.search(rf'\\b{re.escape(mod)}\\b', ing_lower) for mod in VEGETARIAN_MODIFIERS):\n",
    "            continue\n",
    "        for keyword in NON_VEGETARIAN_KEYWORDS:\n",
    "            if re.search(rf'\\b{re.escape(keyword)}(?:s)?\\b', ing_lower):\n",
    "                if not contains_vegetarian_exception(ing_lower, keyword):\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "def get_non_vegetarian_ingredients(ingredients_input):\n",
    "    \"\"\"Returning list of ingredients without vegetarian exceptions\"\"\"\n",
    "    ingredients = parse_ingredients(ingredients_input)\n",
    "    non_vegetarian = []\n",
    "    for ing in ingredients:\n",
    "        if not isinstance(ing, str):\n",
    "            continue\n",
    "        ing_lower = ing.lower()\n",
    "        if any(skip in ing_lower for skip in ('water', 'salt')):\n",
    "            continue\n",
    "        for keyword in NON_VEGETARIAN_KEYWORDS:\n",
    "            if re.search(rf'\\b{re.escape(keyword)}(?:s)?\\b', ing_lower):\n",
    "                if not contains_vegetarian_exception(ing_lower, keyword):\n",
    "                    non_vegetarian.append(ing)\n",
    "                break\n",
    "    return non_vegetarian\n",
    "\n",
    "# Apply to dataframe\n",
    "df['VEGETARIAN'] = df['NER_clean'].apply(classify_vegetarian)\n",
    "df['NON_VEGETARIAN_INGREDIENTS'] = df['NER_clean'].apply(get_non_vegetarian_ingredients)\n",
    "\n",
    "anti_patterns = re.compile(\n",
    "    r'\\b(not vegetarian|not veg|meat|steak|fish|beef|chicken|pork|bacon)\\b',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "df['VEGETARIAN'] = df.apply(\n",
    "    lambda row: False if row['VEGETARIAN'] and isinstance(row['TITLE'], str) and anti_patterns.search(row['TITLE']) else row['VEGETARIAN'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Show results\n",
    "df[['TITLE', 'NER', 'VEGETARIAN', 'NON_VEGETARIAN_INGREDIENTS']].head(200)"
   ],
   "id": "8813043f994e6a4e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 TITLE  \\\n",
       "0                        Western Sizzlin Bread Pudding   \n",
       "1            Creamy Tuna And Bacon Salad (Reduced Fat)   \n",
       "2                        Spinach And Mushroom Lasagna    \n",
       "3                                     Three-Bean Tacos   \n",
       "4                                Hearty Hamburger Soup   \n",
       "..                                                 ...   \n",
       "195                Sweet Potato Brotchen (Bread Rolls)   \n",
       "196  Green Cleaner and Bug Repellant in One (Concen...   \n",
       "197                                 Chocolate Chip Pie   \n",
       "198                                Poppy Seed Dressing   \n",
       "199                                    Scallop Ceviche   \n",
       "\n",
       "                                                   NER  VEGETARIAN  \\\n",
       "0    ['eggs', 'milk', 'sugar', 'vanilla', 'gallon w...        True   \n",
       "1    ['tuna', 'mayonnaise', 'low-fat sour cream', '...       False   \n",
       "2    ['olive oil', 'onion', 'garlic', 'salt', 'grou...        True   \n",
       "3    ['olive oil', 'onion', 'red bell pepper', 'gre...        True   \n",
       "4    ['lean ground beef', 'white onion', 'ground bl...       False   \n",
       "..                                                 ...         ...   \n",
       "195  ['sweet potatoes', 'butter', 'salt', 'flour', ...        True   \n",
       "196          ['essential oil', 'vodka', 'liquid soap']        True   \n",
       "197  ['graham cracker crust', 'marshmallows', 'milk...        True   \n",
       "198  ['sugar', 'dry mustard', 'salt', 'vinegar', 'o...        True   \n",
       "199  ['scallops', 'Salt', 'ginger', 'garlic', 'stal...       False   \n",
       "\n",
       "    NON_VEGETARIAN_INGREDIENTS  \n",
       "0                           []  \n",
       "1           [tuna, bacon bits]  \n",
       "2                           []  \n",
       "3                           []  \n",
       "4           [lean ground beef]  \n",
       "..                         ...  \n",
       "195                         []  \n",
       "196                         []  \n",
       "197                         []  \n",
       "198                         []  \n",
       "199                 [scallops]  \n",
       "\n",
       "[200 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>NER</th>\n",
       "      <th>VEGETARIAN</th>\n",
       "      <th>NON_VEGETARIAN_INGREDIENTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Western Sizzlin Bread Pudding</td>\n",
       "      <td>['eggs', 'milk', 'sugar', 'vanilla', 'gallon w...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Creamy Tuna And Bacon Salad (Reduced Fat)</td>\n",
       "      <td>['tuna', 'mayonnaise', 'low-fat sour cream', '...</td>\n",
       "      <td>False</td>\n",
       "      <td>[tuna, bacon bits]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spinach And Mushroom Lasagna</td>\n",
       "      <td>['olive oil', 'onion', 'garlic', 'salt', 'grou...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three-Bean Tacos</td>\n",
       "      <td>['olive oil', 'onion', 'red bell pepper', 'gre...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hearty Hamburger Soup</td>\n",
       "      <td>['lean ground beef', 'white onion', 'ground bl...</td>\n",
       "      <td>False</td>\n",
       "      <td>[lean ground beef]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Sweet Potato Brotchen (Bread Rolls)</td>\n",
       "      <td>['sweet potatoes', 'butter', 'salt', 'flour', ...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Green Cleaner and Bug Repellant in One (Concen...</td>\n",
       "      <td>['essential oil', 'vodka', 'liquid soap']</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Chocolate Chip Pie</td>\n",
       "      <td>['graham cracker crust', 'marshmallows', 'milk...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Poppy Seed Dressing</td>\n",
       "      <td>['sugar', 'dry mustard', 'salt', 'vinegar', 'o...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Scallop Ceviche</td>\n",
       "      <td>['scallops', 'Salt', 'ginger', 'garlic', 'stal...</td>\n",
       "      <td>False</td>\n",
       "      <td>[scallops]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Definizione del metodo di preparazione della ricetta",
   "id": "8a072aee5d154480"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T10:33:56.525105Z",
     "start_time": "2025-06-21T10:33:53.316608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extended map for methods and keywords\n",
    "cooking_methods = {\n",
    "    'Baked': [\n",
    "        'bake', 'baked', 'baking', 'preheat oven', 'pre-heated oven', 'ovenproof',\n",
    "        'place in oven', 'roast in oven', 'bake at', 'oven-bake', 'put in oven', 'oven cook'\n",
    "    ],\n",
    "    'Boiled': [\n",
    "        'boil', 'boiled', 'boiling', 'bring to a boil', 'simmer', 'parboil', 'blanch',\n",
    "        'blanched', 'boiled in', 'simmered', 'cook in boiling water', 'gentle boil', 'rapid boil'\n",
    "    ],\n",
    "    'Broil': [\n",
    "        'broil', 'broiled', 'broiling', 'under the broiler', 'place under broiler', 'broiler setting'\n",
    "    ],\n",
    "    'Fried': [\n",
    "        'fry', 'fried', 'frying', 'deep fry', 'deep-fry', 'pan fry', 'pan-fried', 'shallow fry',\n",
    "        'stir-fry', 'sautÃ©', 'sautÃ©e', 'sautÃ©ed', 'sauted', 'sautÃ©ing', 'sear', 'seared',\n",
    "        'brown in a pan', 'browned', 'cook in oil', 'crisp in pan', 'fry in skillet', 'flash fry'\n",
    "    ],\n",
    "    'Grilled': [\n",
    "        'grill', 'grilled', 'grilling', 'grill pan', 'on the grill', 'barbecue', 'bbq',\n",
    "        'cook over grill', 'char-grill', 'chargrilled', 'griddle pan', 'grilled over flame'\n",
    "    ],\n",
    "    'Roasted': [\n",
    "        'roast', 'roasted', 'roasting', 'oven-roasted', 'slow roast', 'roast in oven',\n",
    "        'roast at', 'roasted until golden', 'dry-roast', 'dry roast'\n",
    "    ],\n",
    "    'Slow Cooking': [\n",
    "        'slow cook', 'slow-cooked', 'slow-cook', 'slow cooking', 'crockpot', 'crock pot',\n",
    "        'slow cooker', 'cook on low for', 'long cook time', 'set to low', 'set to high and cook for hours'\n",
    "    ],\n",
    "    'Steamed': [\n",
    "        'steam', 'steamed', 'steaming', 'in steamer', 'steamer basket', 'double boiler',\n",
    "        'steam until tender', 'cook with steam', 'steam over water', 'steam for minutes'\n",
    "    ],\n",
    "    'Pressure Cooked': [\n",
    "        'pressure cook', 'pressure cooked', 'pressure cooker', 'instant pot',\n",
    "        'cooked under pressure', 'cook under pressure', 'high pressure', 'manual pressure cook'\n",
    "    ],\n",
    "    'Microwaved': [\n",
    "        'microwave', 'microwaved', 'microwaving', 'cook in microwave', 'microwave on high',\n",
    "        'microwave-safe bowl', 'microwave for minutes'\n",
    "    ],\n",
    "    'Sous Vide': [\n",
    "        'sous vide', 'sous-vide', 'precision cooker', 'immersion circulator',\n",
    "        'vacuum sealed and cooked in water bath', 'vacuum seal and cook at temperature',\n",
    "        'cook sous vide'\n",
    "    ],\n",
    "    'Toasted': [\n",
    "        'toast', 'toasted', 'toasting', 'in toaster', 'toasted until golden',\n",
    "        'under grill until golden', 'toast under broiler', 'toast on skillet'\n",
    "    ],\n",
    "    'Blended': [\n",
    "        'blend', 'blended', 'blending', 'blender', 'puree', 'pureed', 'pureeing',\n",
    "        'food processor', 'process until smooth', 'blend until creamy', 'blend until combined',\n",
    "        'whizz in blender', 'pulse until smooth'\n",
    "    ],\n",
    "    'Raw': [\n",
    "        'serve raw', 'uncooked', 'no cook', 'eat raw', 'fresh only', 'as is', 'ready to eat',\n",
    "        'no heating required', 'chilled and served', 'consume without cooking', 'fresh and raw'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def detect_cooking_method(instruction_list):\n",
    "    '''A method to detect cooking method'''\n",
    "    if isinstance(instruction_list, str):\n",
    "        try:\n",
    "            steps = ast.literal_eval(instruction_list.lower())\n",
    "        except:\n",
    "            steps = [instruction_list.lower()]\n",
    "    else:\n",
    "        steps = [str(instruction).lower() for instruction in instruction_list]\n",
    "\n",
    "    for method, keywords in cooking_methods.items():\n",
    "        if any(any(kw in step for kw in keywords) for step in steps):\n",
    "            return method\n",
    "    return 'Other'\n",
    "\n",
    "df['COOKING_METHOD'] = df['DIRECTIONS'].apply(detect_cooking_method)\n",
    "\n",
    "df[['TITLE', 'DIRECTIONS', 'COOKING_METHOD']].head(200)"
   ],
   "id": "bbef5cc1efcc40d0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 TITLE  \\\n",
       "0                        Western Sizzlin Bread Pudding   \n",
       "1            Creamy Tuna And Bacon Salad (Reduced Fat)   \n",
       "2                        Spinach And Mushroom Lasagna    \n",
       "3                                     Three-Bean Tacos   \n",
       "4                                Hearty Hamburger Soup   \n",
       "..                                                 ...   \n",
       "195                Sweet Potato Brotchen (Bread Rolls)   \n",
       "196  Green Cleaner and Bug Repellant in One (Concen...   \n",
       "197                                 Chocolate Chip Pie   \n",
       "198                                Poppy Seed Dressing   \n",
       "199                                    Scallop Ceviche   \n",
       "\n",
       "                                            DIRECTIONS COOKING_METHOD  \n",
       "0    [\"Distribute rolls and cinnamon roll in (4) 2\\...          Baked  \n",
       "1    [\"Put everything in a bowl and mix together un...          Other  \n",
       "2    [\"Heat oven to 375 degrees.\", \"\", \"In medium s...          Baked  \n",
       "3    [\"Heat oil in a large skillet over medium-high...         Boiled  \n",
       "4    [\"Brown ground beef and onion in a large pot. ...         Boiled  \n",
       "..                                                 ...            ...  \n",
       "195  [\"Peel and cut sweet potatoes into chunks.\", \"...          Baked  \n",
       "196  [\"Mix all ingredients.\", \"Store in a glass jar...          Other  \n",
       "197  [\"Make crust (I buy the prepared ones from the...         Boiled  \n",
       "198  [\"Combine first 6 ingredients in a blender con...        Blended  \n",
       "199  [\"Bring a pot of water to a boil, salt the wat...         Boiled  \n",
       "\n",
       "[200 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>DIRECTIONS</th>\n",
       "      <th>COOKING_METHOD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Western Sizzlin Bread Pudding</td>\n",
       "      <td>[\"Distribute rolls and cinnamon roll in (4) 2\\...</td>\n",
       "      <td>Baked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Creamy Tuna And Bacon Salad (Reduced Fat)</td>\n",
       "      <td>[\"Put everything in a bowl and mix together un...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spinach And Mushroom Lasagna</td>\n",
       "      <td>[\"Heat oven to 375 degrees.\", \"\", \"In medium s...</td>\n",
       "      <td>Baked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three-Bean Tacos</td>\n",
       "      <td>[\"Heat oil in a large skillet over medium-high...</td>\n",
       "      <td>Boiled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hearty Hamburger Soup</td>\n",
       "      <td>[\"Brown ground beef and onion in a large pot. ...</td>\n",
       "      <td>Boiled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Sweet Potato Brotchen (Bread Rolls)</td>\n",
       "      <td>[\"Peel and cut sweet potatoes into chunks.\", \"...</td>\n",
       "      <td>Baked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Green Cleaner and Bug Repellant in One (Concen...</td>\n",
       "      <td>[\"Mix all ingredients.\", \"Store in a glass jar...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Chocolate Chip Pie</td>\n",
       "      <td>[\"Make crust (I buy the prepared ones from the...</td>\n",
       "      <td>Boiled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Poppy Seed Dressing</td>\n",
       "      <td>[\"Combine first 6 ingredients in a blender con...</td>\n",
       "      <td>Blended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Scallop Ceviche</td>\n",
       "      <td>[\"Bring a pot of water to a boil, salt the wat...</td>\n",
       "      <td>Boiled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Euristica in grado di stimare la difficoltÃ  di una ricetta",
   "id": "ccfd5160a6abf303"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T10:34:26.656403Z",
     "start_time": "2025-06-21T10:33:56.689698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ast\n",
    "\n",
    "def estimate_difficulty(row):\n",
    "    try:\n",
    "        ingredients = ast.literal_eval(row['INGREDIENTS'])\n",
    "        instructions = ast.literal_eval(row['DIRECTIONS'])\n",
    "    except Exception:\n",
    "        return 'Unknown'\n",
    "\n",
    "    num_ingredients = len(ingredients)\n",
    "    num_steps = len([s for s in instructions if s.strip()])\n",
    "    instruction_text = ' '.join(instructions).lower()\n",
    "\n",
    "    mild_keywords = [\n",
    "        'mix', 'combine', 'stir', 'spread', 'pour', 'season',\n",
    "        'slice', 'chop', 'cut', 'heat', 'grease', 'add'\n",
    "    ]\n",
    "    medium_keywords = [\n",
    "        'simmer', 'boil', 'bake', 'grill', 'saute', 'blend',\n",
    "        'fry', 'knead', 'rest', 'whisk', 'preheat', 'chill',\n",
    "        'melt', 'marinate', 'brown'\n",
    "    ]\n",
    "    hard_keywords = [\n",
    "        'sous vide', 'caramelize', 'double boiler', 'bain-marie',\n",
    "        'julienne', 'poach', 'deglaze', 'layer', 'water bath',\n",
    "        'refrigerate overnight', 'fold', 'temper', 'pass through sieve',\n",
    "        'glaze', 'rotate pans', 'cool completely', 'remove from mold'\n",
    "    ]\n",
    "\n",
    "    def keyword_score(keywords, text, max_points=3):\n",
    "        score = 0\n",
    "        for kw in keywords:\n",
    "            matches = len(re.findall(rf'\\b{re.escape(kw)}\\b', text))\n",
    "            score += matches\n",
    "        return min(score, max_points)\n",
    "\n",
    "    mild_score = keyword_score(mild_keywords, instruction_text)\n",
    "    medium_score = keyword_score(medium_keywords, instruction_text)\n",
    "    hard_score = keyword_score(hard_keywords, instruction_text, max_points=4)\n",
    "\n",
    "    bonus = 0\n",
    "    if 'overnight' in instruction_text or 'refrigerate overnight' in instruction_text:\n",
    "        bonus += 3\n",
    "    if 'cool completely' in instruction_text or 'let cool' in instruction_text:\n",
    "        bonus += 2\n",
    "    if 'food processor' in instruction_text:\n",
    "        bonus += 1\n",
    "    if 'strain' in instruction_text or 'sieve' in instruction_text:\n",
    "        bonus += 1\n",
    "    if 'double boiler' in instruction_text or 'bain-marie' in instruction_text:\n",
    "        bonus += 2\n",
    "    if 'fold' in instruction_text:\n",
    "        bonus += 1\n",
    "\n",
    "    complex_ingredients = ['gelatin', 'saffron', 'truffle', 'wasabi', 'caviar']\n",
    "    complex_ingredients_count = sum(1 for ingr in ingredients if any(ci in ingr.lower() for ci in complex_ingredients))\n",
    "    bonus += complex_ingredients_count * 1.5\n",
    "\n",
    "    score = 0\n",
    "    score += num_ingredients * 0.25\n",
    "    score += num_steps * 1.5\n",
    "    score += mild_score * 1\n",
    "    score += medium_score * 2\n",
    "    score += hard_score * 3\n",
    "    score += bonus\n",
    "\n",
    "    # thresholds for the 3 classes (da rivedere)\n",
    "    if score <= 10:\n",
    "        return 'Easy'\n",
    "    elif score <= 22:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Hard'\n",
    "\n",
    "# Apply\n",
    "df['DIFFICULTY'] = df.apply(estimate_difficulty, axis=1)\n",
    "\n",
    "df[['TITLE', 'DIRECTIONS', 'DIFFICULTY']].head(200)"
   ],
   "id": "6bea9985f08ba998",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 TITLE  \\\n",
       "0                        Western Sizzlin Bread Pudding   \n",
       "1            Creamy Tuna And Bacon Salad (Reduced Fat)   \n",
       "2                        Spinach And Mushroom Lasagna    \n",
       "3                                     Three-Bean Tacos   \n",
       "4                                Hearty Hamburger Soup   \n",
       "..                                                 ...   \n",
       "195                Sweet Potato Brotchen (Bread Rolls)   \n",
       "196  Green Cleaner and Bug Repellant in One (Concen...   \n",
       "197                                 Chocolate Chip Pie   \n",
       "198                                Poppy Seed Dressing   \n",
       "199                                    Scallop Ceviche   \n",
       "\n",
       "                                            DIRECTIONS DIFFICULTY  \n",
       "0    [\"Distribute rolls and cinnamon roll in (4) 2\\...     Medium  \n",
       "1    [\"Put everything in a bowl and mix together un...       Easy  \n",
       "2    [\"Heat oven to 375 degrees.\", \"\", \"In medium s...       Hard  \n",
       "3    [\"Heat oil in a large skillet over medium-high...       Hard  \n",
       "4    [\"Brown ground beef and onion in a large pot. ...       Hard  \n",
       "..                                                 ...        ...  \n",
       "195  [\"Peel and cut sweet potatoes into chunks.\", \"...       Hard  \n",
       "196  [\"Mix all ingredients.\", \"Store in a glass jar...       Easy  \n",
       "197  [\"Make crust (I buy the prepared ones from the...       Hard  \n",
       "198  [\"Combine first 6 ingredients in a blender con...     Medium  \n",
       "199  [\"Bring a pot of water to a boil, salt the wat...       Hard  \n",
       "\n",
       "[200 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>DIRECTIONS</th>\n",
       "      <th>DIFFICULTY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Western Sizzlin Bread Pudding</td>\n",
       "      <td>[\"Distribute rolls and cinnamon roll in (4) 2\\...</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Creamy Tuna And Bacon Salad (Reduced Fat)</td>\n",
       "      <td>[\"Put everything in a bowl and mix together un...</td>\n",
       "      <td>Easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spinach And Mushroom Lasagna</td>\n",
       "      <td>[\"Heat oven to 375 degrees.\", \"\", \"In medium s...</td>\n",
       "      <td>Hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three-Bean Tacos</td>\n",
       "      <td>[\"Heat oil in a large skillet over medium-high...</td>\n",
       "      <td>Hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hearty Hamburger Soup</td>\n",
       "      <td>[\"Brown ground beef and onion in a large pot. ...</td>\n",
       "      <td>Hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Sweet Potato Brotchen (Bread Rolls)</td>\n",
       "      <td>[\"Peel and cut sweet potatoes into chunks.\", \"...</td>\n",
       "      <td>Hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Green Cleaner and Bug Repellant in One (Concen...</td>\n",
       "      <td>[\"Mix all ingredients.\", \"Store in a glass jar...</td>\n",
       "      <td>Easy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Chocolate Chip Pie</td>\n",
       "      <td>[\"Make crust (I buy the prepared ones from the...</td>\n",
       "      <td>Hard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Poppy Seed Dressing</td>\n",
       "      <td>[\"Combine first 6 ingredients in a blender con...</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Scallop Ceviche</td>\n",
       "      <td>[\"Bring a pot of water to a boil, salt the wat...</td>\n",
       "      <td>Hard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Le celle sottostanti contengono una logica in grado di determinare se le ricette contengono oppure no lattosio e glutine",
   "id": "5d5ed8487a6ded"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T10:34:50.375614Z",
     "start_time": "2025-06-21T10:34:26.818732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# List of lactose ingredients\n",
    "LACTOSE_KEYWORDS = {\n",
    "    # milks and derivatives\n",
    "    'milk', 'whole milk', 'skim milk', 'raw milk', 'lactose', 'lactose monohydrate',\n",
    "    'cream', 'heavy cream', 'double cream', 'single cream', 'whipping cream',\n",
    "    'butter', 'ghee', 'clarified butter',\n",
    "\n",
    "    # cheese\n",
    "    'cheese', 'cream cheese', 'soft cheese', 'hard cheese',\n",
    "    'ricotta', 'mozzarella', 'parmesan', 'pecorino', 'feta', 'brie', 'camembert',\n",
    "    'blue cheese', 'goat cheese', 'cheddar', 'colby', 'gouda', 'gruyere',\n",
    "    'swiss cheese', 'romano cheese', 'mascarpone', 'paneer', 'provolone',\n",
    "\n",
    "    # Yogurt and similar\n",
    "    'yogurt', 'greek yogurt', 'kefir', 'curd',\n",
    "\n",
    "    # desserts and industrials\n",
    "    'custard', 'ice cream', 'gelato', 'milkshake', 'pudding', 'white chocolate',\n",
    "\n",
    "    # technical ingredients\n",
    "    'whey', 'whey protein', 'whey powder', 'casein', 'caseinate', 'milk solids',\n",
    "    'milk powder', 'dried milk', 'nonfat milk', 'low-fat milk', 'milk protein',\n",
    "    'milkfat', 'buttermilk', 'evaporated milk', 'condensed milk', 'recombined milk'\n",
    "}\n",
    "\n",
    "# Exceptions for lactose free variants\n",
    "LACTOSE_EXCEPTIONS = {\n",
    "    'milk': {\n",
    "        'almond milk', 'soy milk', 'oat milk', 'coconut milk', 'rice milk',\n",
    "        'lactose-free milk', 'hemp milk', 'pea milk', 'cashew milk',\n",
    "        'hazelnut milk', 'macadamia milk', 'flax milk'\n",
    "    },\n",
    "    'cheese': {\n",
    "        'vegan cheese', 'plant-based cheese', 'nut cheese', 'cashew cheese',\n",
    "        'almond cheese', 'tofu cheese', 'soy cheese'\n",
    "    },\n",
    "    'butter': {\n",
    "        'vegan butter', 'plant-based butter', 'margarine', 'oil spread'\n",
    "    },\n",
    "    'cream': {\n",
    "        'coconut cream', 'soy cream', 'oat cream', 'cashew cream', 'almond cream'\n",
    "    },\n",
    "    'yogurt': {\n",
    "        'coconut yogurt', 'soy yogurt', 'almond yogurt', 'oat yogurt',\n",
    "        'lactose-free yogurt', 'rice yogurt', 'cashew yogurt'\n",
    "    }\n",
    "}\n",
    "\n",
    "def contains_lactose_exception(ingredient, keyword):\n",
    "    exceptions = LACTOSE_EXCEPTIONS.get(keyword, set())\n",
    "    return any(re.search(rf'\\b{re.escape(ex)}\\b', ingredient, re.IGNORECASE) for ex in exceptions)\n",
    "\n",
    "def classify_lactose_free(ingredients_input):\n",
    "    ingredients = parse_ingredients(ingredients_input)\n",
    "    for ing in ingredients:\n",
    "        if not isinstance(ing, str):\n",
    "            continue\n",
    "        ing_lower = ing.lower()\n",
    "        for keyword in LACTOSE_KEYWORDS:\n",
    "            if re.search(rf'\\b{re.escape(keyword)}(?:s)?\\b', ing_lower):\n",
    "                if not contains_lactose_exception(ing_lower, keyword):\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "def get_lactose_ingredients(ingredients_input):\n",
    "    ingredients = parse_ingredients(ingredients_input)\n",
    "    lactose = []\n",
    "    for ing in ingredients:\n",
    "        if not isinstance(ing, str):\n",
    "            continue\n",
    "        ing_lower = ing.lower()\n",
    "        for keyword in LACTOSE_KEYWORDS:\n",
    "            if re.search(rf'\\b{re.escape(keyword)}(?:s)?\\b', ing_lower):\n",
    "                if not contains_lactose_exception(ing_lower, keyword):\n",
    "                    lactose.append(ing)\n",
    "                break\n",
    "    return lactose\n",
    "\n",
    "# Apply to dataframe\n",
    "df['LACTOSE_FREE'] = df['NER_clean'].apply(classify_lactose_free)\n",
    "df['LACTOSE_INGREDIENTS'] = df['NER_clean'].apply(get_lactose_ingredients)\n",
    "\n",
    "# Show results\n",
    "df[['TITLE', 'NER', 'LACTOSE_FREE', 'LACTOSE_INGREDIENTS']].head(200)"
   ],
   "id": "edb8b565bb49e1cf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 TITLE  \\\n",
       "0                        Western Sizzlin Bread Pudding   \n",
       "1            Creamy Tuna And Bacon Salad (Reduced Fat)   \n",
       "2                        Spinach And Mushroom Lasagna    \n",
       "3                                     Three-Bean Tacos   \n",
       "4                                Hearty Hamburger Soup   \n",
       "..                                                 ...   \n",
       "195                Sweet Potato Brotchen (Bread Rolls)   \n",
       "196  Green Cleaner and Bug Repellant in One (Concen...   \n",
       "197                                 Chocolate Chip Pie   \n",
       "198                                Poppy Seed Dressing   \n",
       "199                                    Scallop Ceviche   \n",
       "\n",
       "                                                   NER  LACTOSE_FREE  \\\n",
       "0    ['eggs', 'milk', 'sugar', 'vanilla', 'gallon w...         False   \n",
       "1    ['tuna', 'mayonnaise', 'low-fat sour cream', '...         False   \n",
       "2    ['olive oil', 'onion', 'garlic', 'salt', 'grou...         False   \n",
       "3    ['olive oil', 'onion', 'red bell pepper', 'gre...         False   \n",
       "4    ['lean ground beef', 'white onion', 'ground bl...         False   \n",
       "..                                                 ...           ...   \n",
       "195  ['sweet potatoes', 'butter', 'salt', 'flour', ...         False   \n",
       "196          ['essential oil', 'vodka', 'liquid soap']          True   \n",
       "197  ['graham cracker crust', 'marshmallows', 'milk...         False   \n",
       "198  ['sugar', 'dry mustard', 'salt', 'vinegar', 'o...          True   \n",
       "199  ['scallops', 'Salt', 'ginger', 'garlic', 'stal...          True   \n",
       "\n",
       "                               LACTOSE_INGREDIENTS  \n",
       "0                                           [milk]  \n",
       "1                             [low-fat sour cream]  \n",
       "2    [ricotta cheese, Parmesan cheese, mozzarella]  \n",
       "3                                 [cheddar cheese]  \n",
       "4                    [parmesan cheese, sour cream]  \n",
       "..                                             ...  \n",
       "195                                       [butter]  \n",
       "196                                             []  \n",
       "197                         [milk, whipping cream]  \n",
       "198                                             []  \n",
       "199                                             []  \n",
       "\n",
       "[200 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>NER</th>\n",
       "      <th>LACTOSE_FREE</th>\n",
       "      <th>LACTOSE_INGREDIENTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Western Sizzlin Bread Pudding</td>\n",
       "      <td>['eggs', 'milk', 'sugar', 'vanilla', 'gallon w...</td>\n",
       "      <td>False</td>\n",
       "      <td>[milk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Creamy Tuna And Bacon Salad (Reduced Fat)</td>\n",
       "      <td>['tuna', 'mayonnaise', 'low-fat sour cream', '...</td>\n",
       "      <td>False</td>\n",
       "      <td>[low-fat sour cream]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spinach And Mushroom Lasagna</td>\n",
       "      <td>['olive oil', 'onion', 'garlic', 'salt', 'grou...</td>\n",
       "      <td>False</td>\n",
       "      <td>[ricotta cheese, Parmesan cheese, mozzarella]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three-Bean Tacos</td>\n",
       "      <td>['olive oil', 'onion', 'red bell pepper', 'gre...</td>\n",
       "      <td>False</td>\n",
       "      <td>[cheddar cheese]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hearty Hamburger Soup</td>\n",
       "      <td>['lean ground beef', 'white onion', 'ground bl...</td>\n",
       "      <td>False</td>\n",
       "      <td>[parmesan cheese, sour cream]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Sweet Potato Brotchen (Bread Rolls)</td>\n",
       "      <td>['sweet potatoes', 'butter', 'salt', 'flour', ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[butter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Green Cleaner and Bug Repellant in One (Concen...</td>\n",
       "      <td>['essential oil', 'vodka', 'liquid soap']</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Chocolate Chip Pie</td>\n",
       "      <td>['graham cracker crust', 'marshmallows', 'milk...</td>\n",
       "      <td>False</td>\n",
       "      <td>[milk, whipping cream]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Poppy Seed Dressing</td>\n",
       "      <td>['sugar', 'dry mustard', 'salt', 'vinegar', 'o...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Scallop Ceviche</td>\n",
       "      <td>['scallops', 'Salt', 'ginger', 'garlic', 'stal...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T10:35:55.927698Z",
     "start_time": "2025-06-21T10:34:50.534872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# List of gluten ingredients\n",
    "GLUTEN_KEYWORDS = {\n",
    "    # cereals containing gluten\n",
    "    'wheat', 'barley', 'rye', 'spelt', 'kamut', 'triticale', 'farro', 'einkorn', 'emmer',\n",
    "\n",
    "    # derivatives\n",
    "    'semolina', 'durum', 'graham flour', 'matzo', 'gluten', 'vital wheat gluten',\n",
    "    'hydrolyzed wheat protein', 'wheat starch', 'modified wheat starch',\n",
    "    'malted barley', 'malted milk', 'malt', 'malt extract', 'malt syrup', 'malt vinegar',\n",
    "\n",
    "    # industrial ingredients\n",
    "    'brewerâs yeast', 'brewer yeast', 'yeast extract', 'textured vegetable protein (wheat)',\n",
    "    'vegetable starch (wheat)', 'vegetable gum (wheat)', 'natural flavor (wheat)',\n",
    "    'seasoned flour', 'spice mix (with wheat)',\n",
    "\n",
    "    # common products\n",
    "    'seitan', 'bran', 'bread', 'pasta', 'noodles', 'crackers', 'breadcrumbs', 'biscuit',\n",
    "    'cake flour', 'pastry flour', 'self-rising flour', 'bagel', 'croissant', 'brioche',\n",
    "    'pancake mix', 'waffle mix', 'muffin mix', 'pie crust', 'pizza crust', 'tortilla (wheat)',\n",
    "\n",
    "    # transformed products\n",
    "    'beer', 'ale', 'lager', 'stuffing mix', 'gravy mix', 'roux', 'soups (with pasta/noodles)',\n",
    "    'sauces (with flour)', 'soy sauce', 'teriyaki sauce', 'tempura', 'coating mix', 'breading',\n",
    "\n",
    "    # desserts and snacks\n",
    "    'ladyfinger', 'graham cracker', 'pretzel', 'cereal bar', 'energy bar (with gluten grains)'\n",
    "}\n",
    "\n",
    "# Exceptions for gluten free\n",
    "GLUTEN_EXCEPTIONS = {\n",
    "    'wheat': {'wheatgrass', 'wheat-free tamari'},\n",
    "    'malt': {'maltodextrin (from corn)', 'maltodextrin from corn', 'rice malt', 'corn malt'},\n",
    "    'starch': {\n",
    "        'corn starch', 'potato starch', 'tapioca starch', 'arrowroot starch',\n",
    "        'sago starch', 'rice starch'\n",
    "    },\n",
    "    'flour': {\n",
    "        'almond flour', 'coconut flour', 'chickpea flour', 'rice flour', 'corn flour',\n",
    "        'potato flour', 'soy flour', 'buckwheat flour', 'tapioca flour', 'quinoa flour',\n",
    "        'sorghum flour', 'teff flour', 'millet flour', 'amaranth flour', 'cassava flour',\n",
    "        'oat flour (gluten-free)', 'oat flour gluten-free', 'lentil flour', 'pea flour'\n",
    "    },\n",
    "    'bran': {\n",
    "        'rice bran', 'corn bran', 'oat bran (gluten-free)', 'oat bran gluten-free'\n",
    "    },\n",
    "    'bread': {'gluten-free bread'},\n",
    "    'pasta': {'gluten-free pasta', 'rice pasta', 'quinoa pasta'},\n",
    "    'noodles': {'rice noodles', 'glass noodles', 'sweet potato noodles', 'mung bean noodles'},\n",
    "    'beer': {'gluten-free beer', 'sorghum beer'},\n",
    "    'cracker': {'gluten-free crackers'},\n",
    "    'breadcrumbs': {'gluten-free breadcrumbs', 'crushed cornflakes (GF)'},\n",
    "    'biscuit': {'gluten-free biscuit'},\n",
    "    'soy sauce': {'tamari (gluten-free)', 'gluten-free soy sauce'},\n",
    "    'gravy mix': {'gluten-free gravy'},\n",
    "    'stuffing mix': {'gluten-free stuffing'},\n",
    "    'pizza crust': {'gluten-free pizza crust'},\n",
    "    'cereal bar': {'gluten-free cereal bar'},\n",
    "    'breading': {'gluten-free breading'},\n",
    "    'coating mix': {'gluten-free coating'}\n",
    "}\n",
    "\n",
    "def contains_gluten_exception(ingredient, keyword):\n",
    "    exceptions = GLUTEN_EXCEPTIONS.get(keyword, set())\n",
    "    return any(re.search(rf'\\b{re.escape(ex)}\\b', ingredient, re.IGNORECASE) for ex in exceptions)\n",
    "\n",
    "def classify_gluten_free(ingredients_input):\n",
    "    ingredients = parse_ingredients(ingredients_input)\n",
    "    for ing in ingredients:\n",
    "        if not isinstance(ing, str):\n",
    "            continue\n",
    "        ing_lower = ing.lower()\n",
    "        for keyword in GLUTEN_KEYWORDS:\n",
    "            if re.search(rf'\\b{re.escape(keyword)}(?:s)?\\b', ing_lower):\n",
    "                if not contains_gluten_exception(ing_lower, keyword):\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "def get_gluten_ingredients(ingredients_input):\n",
    "    ingredients = parse_ingredients(ingredients_input)\n",
    "    gluten = []\n",
    "    for ing in ingredients:\n",
    "        if not isinstance(ing, str):\n",
    "            continue\n",
    "        ing_lower = ing.lower()\n",
    "        for keyword in GLUTEN_KEYWORDS:\n",
    "            if re.search(rf'\\b{re.escape(keyword)}(?:s)?\\b', ing_lower):\n",
    "                if not contains_gluten_exception(ing_lower, keyword):\n",
    "                    gluten.append(ing)\n",
    "                break\n",
    "    return gluten\n",
    "\n",
    "# Apply to dataframe\n",
    "df['GLUTEN_FREE'] = df['NER_clean'].apply(classify_gluten_free)\n",
    "df['GLUTEN_INGREDIENTS'] = df['NER_clean'].apply(get_gluten_ingredients)\n",
    "\n",
    "# Show results\n",
    "df[['TITLE', 'NER', 'GLUTEN_FREE', 'GLUTEN_INGREDIENTS']].head(200)"
   ],
   "id": "932750b9b31960fc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 TITLE  \\\n",
       "0                        Western Sizzlin Bread Pudding   \n",
       "1            Creamy Tuna And Bacon Salad (Reduced Fat)   \n",
       "2                        Spinach And Mushroom Lasagna    \n",
       "3                                     Three-Bean Tacos   \n",
       "4                                Hearty Hamburger Soup   \n",
       "..                                                 ...   \n",
       "195                Sweet Potato Brotchen (Bread Rolls)   \n",
       "196  Green Cleaner and Bug Repellant in One (Concen...   \n",
       "197                                 Chocolate Chip Pie   \n",
       "198                                Poppy Seed Dressing   \n",
       "199                                    Scallop Ceviche   \n",
       "\n",
       "                                                   NER  GLUTEN_FREE  \\\n",
       "0    ['eggs', 'milk', 'sugar', 'vanilla', 'gallon w...         True   \n",
       "1    ['tuna', 'mayonnaise', 'low-fat sour cream', '...         True   \n",
       "2    ['olive oil', 'onion', 'garlic', 'salt', 'grou...        False   \n",
       "3    ['olive oil', 'onion', 'red bell pepper', 'gre...         True   \n",
       "4    ['lean ground beef', 'white onion', 'ground bl...        False   \n",
       "..                                                 ...          ...   \n",
       "195  ['sweet potatoes', 'butter', 'salt', 'flour', ...        False   \n",
       "196          ['essential oil', 'vodka', 'liquid soap']         True   \n",
       "197  ['graham cracker crust', 'marshmallows', 'milk...        False   \n",
       "198  ['sugar', 'dry mustard', 'salt', 'vinegar', 'o...         True   \n",
       "199  ['scallops', 'Salt', 'ginger', 'garlic', 'stal...         True   \n",
       "\n",
       "         GLUTEN_INGREDIENTS  \n",
       "0                        []  \n",
       "1                        []  \n",
       "2         [lasagna noodles]  \n",
       "3                        []  \n",
       "4               [soy sauce]  \n",
       "..                      ...  \n",
       "195           [bread flour]  \n",
       "196                      []  \n",
       "197  [graham cracker crust]  \n",
       "198                      []  \n",
       "199                      []  \n",
       "\n",
       "[200 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>NER</th>\n",
       "      <th>GLUTEN_FREE</th>\n",
       "      <th>GLUTEN_INGREDIENTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Western Sizzlin Bread Pudding</td>\n",
       "      <td>['eggs', 'milk', 'sugar', 'vanilla', 'gallon w...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Creamy Tuna And Bacon Salad (Reduced Fat)</td>\n",
       "      <td>['tuna', 'mayonnaise', 'low-fat sour cream', '...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spinach And Mushroom Lasagna</td>\n",
       "      <td>['olive oil', 'onion', 'garlic', 'salt', 'grou...</td>\n",
       "      <td>False</td>\n",
       "      <td>[lasagna noodles]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Three-Bean Tacos</td>\n",
       "      <td>['olive oil', 'onion', 'red bell pepper', 'gre...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hearty Hamburger Soup</td>\n",
       "      <td>['lean ground beef', 'white onion', 'ground bl...</td>\n",
       "      <td>False</td>\n",
       "      <td>[soy sauce]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Sweet Potato Brotchen (Bread Rolls)</td>\n",
       "      <td>['sweet potatoes', 'butter', 'salt', 'flour', ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[bread flour]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Green Cleaner and Bug Repellant in One (Concen...</td>\n",
       "      <td>['essential oil', 'vodka', 'liquid soap']</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Chocolate Chip Pie</td>\n",
       "      <td>['graham cracker crust', 'marshmallows', 'milk...</td>\n",
       "      <td>False</td>\n",
       "      <td>[graham cracker crust]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Poppy Seed Dressing</td>\n",
       "      <td>['sugar', 'dry mustard', 'salt', 'vinegar', 'o...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Scallop Ceviche</td>\n",
       "      <td>['scallops', 'Salt', 'ginger', 'garlic', 'stal...</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## La cella sottostante puÃ² essere eseguita solo su computer potenti, e il suo scopo principale Ã¨ quello di generare la categoria di prezzo della ricetta, e usa un diverso approccio anche per definire altre categorie a cui appartengono le ricette",
   "id": "9ed949338ed488c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Qui inizia la magia:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "345918a28f5a9fc0"
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "gpu = torch.cuda.get_device_name(0)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "print(f\"Device name: {gpu}\")\n",
    "\n",
    "# Eseguire questa cella solo se runnata dal Barone\n",
    "if gpu == \"NVIDIA GeForce RTX 3060 Ti\":\n",
    "    classifier = pipeline(\n",
    "        task=\"zero-shot-classification\",\n",
    "        model=\"MoritzLaurer/deberta-v3-base-zeroshot-v1.1-all-33\",\n",
    "        device=0,\n",
    "        torch_dtype=torch.float16,\n",
    "        model_kwargs={\"cache_dir\": \"./cache\"},\n",
    "        batch_size=32,  # Optimal for RTX 3060 Ti\n",
    "        framework=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # %% Data Loading\n",
    "    df['NER'] = df['NER'].apply(eval)  # Convert string lists to actual lists\n",
    "    \n",
    "    # %% Food Categories & Prices (EUR/kg)\n",
    "    CATEGORIES = [\n",
    "        \"dairy\", \"meat\", \"seafood\", \"grain\", \"vegetable\",\n",
    "        \"fruit\", \"spice/herb\", \"processed\", \"sweetener\",\n",
    "        \"condiment\", \"legume\", \"oil/fat\"\n",
    "    ]\n",
    "    \n",
    "    MEDIAN_PRICES = {\n",
    "        \"dairy\": 3.50,       # Milk, cheese\n",
    "        \"meat\": 7.50,        # Chicken, beef\n",
    "        \"seafood\": 12.00,    # Fish, shrimp\n",
    "        \"grain\": 2.20,       # Flour, rice\n",
    "        \"vegetable\": 1.80,   # Onions, garlic\n",
    "        \"fruit\": 2.50,       # Tomatoes, bananas\n",
    "        \"spice/herb\": 18.00, # Vanilla, cinnamon\n",
    "        \"processed\": 4.50,   # Pasta, canned goods\n",
    "        \"sweetener\": 2.20,   # Sugar\n",
    "        \"condiment\": 5.00,   # Mayo, dressings\n",
    "        \"legume\": 3.00,      # Beans, lentils\n",
    "        \"oil/fat\": 8.00      # Olive oil\n",
    "    }\n",
    "    \n",
    "    # %% Ingredient Cleaning (Fixed)\n",
    "    def clean_ingredient(ingredient: str) -> str:\n",
    "        \"\"\"Conservative cleaning preserving ingredient names\"\"\"\n",
    "        # Remove quantities (e.g., \"200g\", \"1/2 cup\")\n",
    "        cleaned = re.sub(r'\\b\\d+[\\d/\\.]*\\s*[a-z]*\\b', '', ingredient, flags=re.IGNORECASE)\n",
    "        # Remove special chars except spaces\n",
    "        cleaned = re.sub(r'[^\\w\\s]', '', cleaned).strip().lower()\n",
    "        return cleaned if cleaned else \"unknown\"\n",
    "    \n",
    "    # %% Batch Classification (GPU-optimized)\n",
    "    classification_cache = {}\n",
    "    \n",
    "    def batch_classify(ingredients: list, batch_size: int = 16) -> dict:  # Reduced batch size\n",
    "        unique_ingredients = list(set(ingredients))\n",
    "    \n",
    "        with torch.no_grad():  # Disable gradient tracking\n",
    "            for batch in tqdm([unique_ingredients[i:i+batch_size]\n",
    "                               for i in range(0, len(unique_ingredients), batch_size)],\n",
    "                              desc=\"Classifying Ingredients\"):\n",
    "                # Process batch on GPU\n",
    "                results = classifier(batch, CATEGORIES, multi_label=False)\n",
    "    \n",
    "                # Cache results\n",
    "                for ing, result in zip(batch, results):\n",
    "                    classification_cache[ing] = result['labels'][0]\n",
    "    \n",
    "                # Clear GPU cache\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "        return classification_cache\n",
    "    \n",
    "    # %% Process Entire Dataset\n",
    "    # Get all unique ingredients\n",
    "    all_ingredients = [clean_ingredient(ing)\n",
    "                       for recipe in df['NER']\n",
    "                       for ing in recipe]\n",
    "    unique_ingredients = list(set(all_ingredients))\n",
    "    \n",
    "    # Batch classify with progress bar\n",
    "    _ = batch_classify(unique_ingredients, batch_size=64)\n",
    "    \n",
    "    # Calculate recipe costs\n",
    "    df['total_cost'] = df['NER'].apply(\n",
    "        lambda x: round(sum(\n",
    "            MEDIAN_PRICES.get(classification_cache[clean_ingredient(ing)], 3.00)\n",
    "            for ing in x\n",
    "        ), 2)\n",
    "    )\n",
    "    \n",
    "    # Dynamic price categorization\n",
    "    costs = df['total_cost'].values\n",
    "    \n",
    "    # threshold per le categorie degli prezzi\n",
    "    very_cheap, cheap, medium, expensive = np.percentile(costs, [20, 40, 60, 85])\n",
    "    \n",
    "    df['price_tag'] = df['total_cost'].apply(\n",
    "        lambda x: 'very cheap' if x <= very_cheap\n",
    "        else 'cheap' if x <= cheap\n",
    "        else 'medium' if x <= medium\n",
    "        else 'expensive' if x <= expensive\n",
    "        else 'rich'\n",
    "    )\n",
    "\n",
    "    def get_categories(ingredient_list):\n",
    "        return [classification_cache.get(ing) for ing in ingredient_list]\n",
    "    \n",
    "    def is_vegan(cat_list):\n",
    "        return all(cat not in ['dairy', 'meat', 'seafood'] for cat in cat_list if cat is not None)\n",
    "    \n",
    "    def is_vegetarian(cat_list):\n",
    "        return all(cat not in ['meat', 'seafood'] for cat in cat_list if cat is not None)\n",
    "    \n",
    "    df['categories'] = df['NER_clean'].apply(get_categories)\n",
    "    df['vegan'] = df['categories'].apply(is_vegan)\n",
    "    df['vegetarian'] = df['categories'].apply(is_vegetarian)\n",
    "    \n",
    "    # Save outputs\n",
    "    df.to_csv('full_tagged_dataset_10%', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-22T15:17:30.056725Z",
     "start_time": "2025-05-22T15:16:54.432594Z"
    }
   },
   "id": "6c57cbd4e0ba36f6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAssertionError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mre\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtqdm\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m gpu = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcuda\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_device_name\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mPyTorch version: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtorch.__version__\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m      9\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCUDA available: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtorch.cuda.is_available()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:544\u001B[39m, in \u001B[36mget_device_name\u001B[39m\u001B[34m(device)\u001B[39m\n\u001B[32m    532\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_device_name\u001B[39m(device: Optional[_device_t] = \u001B[38;5;28;01mNone\u001B[39;00m) -> \u001B[38;5;28mstr\u001B[39m:\n\u001B[32m    533\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"Get the name of a device.\u001B[39;00m\n\u001B[32m    534\u001B[39m \n\u001B[32m    535\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    542\u001B[39m \u001B[33;03m        str: the name of the device\u001B[39;00m\n\u001B[32m    543\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m544\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mget_device_properties\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m.name\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:576\u001B[39m, in \u001B[36mget_device_properties\u001B[39m\u001B[34m(device)\u001B[39m\n\u001B[32m    564\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_device_properties\u001B[39m(device: Optional[_device_t] = \u001B[38;5;28;01mNone\u001B[39;00m) -> _CudaDeviceProperties:\n\u001B[32m    565\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"Get the properties of a device.\u001B[39;00m\n\u001B[32m    566\u001B[39m \n\u001B[32m    567\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    574\u001B[39m \u001B[33;03m        _CudaDeviceProperties: the properties of the device\u001B[39;00m\n\u001B[32m    575\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m576\u001B[39m     \u001B[43m_lazy_init\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# will define _get_device_properties\u001B[39;00m\n\u001B[32m    577\u001B[39m     device = _get_device_index(device, optional=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    578\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m device < \u001B[32m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m device >= device_count():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:363\u001B[39m, in \u001B[36m_lazy_init\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    358\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    359\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    360\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mmultiprocessing, you must use the \u001B[39m\u001B[33m'\u001B[39m\u001B[33mspawn\u001B[39m\u001B[33m'\u001B[39m\u001B[33m start method\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    361\u001B[39m     )\n\u001B[32m    362\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch._C, \u001B[33m\"\u001B[39m\u001B[33m_cuda_getDeviceCount\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m363\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mTorch not compiled with CUDA enabled\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    364\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    365\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[32m    366\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    367\u001B[39m     )\n",
      "\u001B[31mAssertionError\u001B[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "Qui la magia finisce :)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eade76ff96b7ad22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creazione del modello per la classificazione del tag 'PREPARATION_TIME' e testing",
   "id": "f22b17441612ef05"
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Default parameters\n",
    "MAX_LEN = 500\n",
    "NUM_WORDS = 20000\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# Method to preprocess a dataframe\n",
    "def preprocess(df, text_columns, label_column, n=10000):\n",
    "    df = df[text_columns + [label_column]].dropna().head(n).copy()\n",
    "    df[text_columns[0]] = df[text_columns[0]].apply(eval)\n",
    "    df[text_columns[1]] = df[text_columns[1]].apply(eval)\n",
    "    df[\"full_text\"] = df[text_columns[0]].apply(lambda x: \" \".join(x)) + \" \" + df[text_columns[1]].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"encoded_label\"] = label_encoder.fit_transform(df[label_column])\n",
    "\n",
    "    return df, label_encoder\n",
    "\n",
    "# Method to train a classificator\n",
    "def train_text_classifier(df, label_encoder, model_name_prefix):\n",
    "    X = df[\"full_text\"]\n",
    "    y = df[\"encoded_label\"]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "    X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=NUM_WORDS, output_dim=128, input_length=MAX_LEN),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(len(label_encoder.classes_), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train_pad, y_train,\n",
    "        validation_data=(X_val_pad, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "    # Saving model\n",
    "    model.save(f\"classifiers/prep_time/{model_name_prefix}.h5\")\n",
    "    with open(f\"classifiers/prep_time/{model_name_prefix}_label_mapping.pkl\", \"wb\") as f:\n",
    "        pickle.dump(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))), f)\n",
    "\n",
    "    with open(f\"classifiers/prep_time/{model_name_prefix}_tokenizer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "    print(f\"Modello '{model_name_prefix}' salvato con successo.\")\n",
    "    return model\n",
    "\n",
    "# === FIRST MODEL: PREPARATION_TIME ===\n",
    "df_time = df[[\"INGREDIENTS\", \"DIRECTIONS\", \"PREPARATION_TIME\"]].copy()\n",
    "df_time, prep_time_encoder = preprocess(df_time, [\"INGREDIENTS\", \"DIRECTIONS\"], \"PREPARATION_TIME\")\n",
    "prep_time_model = train_text_classifier(df_time, prep_time_encoder, model_name_prefix=\"prep_time_classifier\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-22T14:09:19.366012Z",
     "start_time": "2025-05-22T14:07:44.306112Z"
    }
   },
   "id": "f4190c03acb0a826",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 40ms/step - accuracy: 0.3440 - loss: 1.5039 - val_accuracy: 0.5595 - val_loss: 1.0874\n",
      "Epoch 2/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 36ms/step - accuracy: 0.5825 - loss: 1.0628 - val_accuracy: 0.6415 - val_loss: 0.9110\n",
      "Epoch 3/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 34ms/step - accuracy: 0.6505 - loss: 0.9110 - val_accuracy: 0.6780 - val_loss: 0.8400\n",
      "Epoch 4/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 34ms/step - accuracy: 0.6937 - loss: 0.8323 - val_accuracy: 0.6880 - val_loss: 0.8132\n",
      "Epoch 5/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 32ms/step - accuracy: 0.7184 - loss: 0.7549 - val_accuracy: 0.6755 - val_loss: 0.8112\n",
      "Epoch 6/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 30ms/step - accuracy: 0.7518 - loss: 0.6918 - val_accuracy: 0.6895 - val_loss: 0.8040\n",
      "Epoch 7/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 31ms/step - accuracy: 0.7634 - loss: 0.6626 - val_accuracy: 0.6875 - val_loss: 0.8012\n",
      "Epoch 8/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 32ms/step - accuracy: 0.7805 - loss: 0.6040 - val_accuracy: 0.6870 - val_loss: 0.8150\n",
      "Epoch 9/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 30ms/step - accuracy: 0.8110 - loss: 0.5464 - val_accuracy: 0.6900 - val_loss: 0.8216\n",
      "Epoch 10/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 31ms/step - accuracy: 0.8343 - loss: 0.4890 - val_accuracy: 0.6790 - val_loss: 0.8459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello 'prep_time_classifier' salvato con successo.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T14:10:22.215224Z",
     "start_time": "2025-05-22T14:10:21.981130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "MAX_LEN = 500  # same as training\n",
    "\n",
    "# --- Load model and tokenizer ---\n",
    "prep_time_model = load_model(\"classifiers/prep_time/prep_time_classifier.h5\")\n",
    "\n",
    "with open(\"classifiers/prep_time/prep_time_classifier_tokenizer.pkl\", \"rb\") as f:\n",
    "    prep_time_tokenizer = pickle.load(f)\n",
    "\n",
    "with open(\"classifiers/prep_time/prep_time_classifier_label_mapping.pkl\", \"rb\") as f:\n",
    "    prep_time_label_map = pickle.load(f)\n",
    "\n",
    "# Reverse dictionaries to decode predictions\n",
    "inv_prep_time_label_map = {v: k for k, v in prep_time_label_map.items()}\n",
    "\n",
    "# List of 10 recipes with expected time\n",
    "recipes = [\n",
    "    {\n",
    "        \"ingredients\": \"200g spaghetti, 2 cloves garlic, chili pepper, extra virgin olive oil, salt\",\n",
    "        \"instructions\": \"Cook the spaghetti. Meanwhile, sautÃ© garlic and chili in the oil. Mix with the pasta.\",\n",
    "        \"expected_time\": \"15 min\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Lettuce, 1 can of tuna, tomatoes, black olives, onion, olive oil, salt\",\n",
    "        \"instructions\": \"Wash and cut the vegetables. Add the drained tuna and olives. Dress with oil and salt.\",\n",
    "        \"expected_time\": \"10 min\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"300g Carnaroli rice, 1 saffron sachet, vegetable broth, butter, onion, Parmesan cheese\",\n",
    "        \"instructions\": \"SautÃ© the onion, add rice and deglaze with wine. Gradually pour in the broth. Add saffron and stir in butter and cheese.\",\n",
    "        \"expected_time\": \"30 min\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"3 eggs, salt, pepper, butter\",\n",
    "        \"instructions\": \"Beat the eggs with salt and pepper. Pour into a pan with melted butter and stir until set.\",\n",
    "        \"expected_time\": \"5 min\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Lasagna sheets, meat sauce, bÃ©chamel, Parmesan cheese\",\n",
    "        \"instructions\": \"Layer pasta, sauce, bÃ©chamel and cheese. Bake at 180Â°C for 40 minutes.\",\n",
    "        \"expected_time\": \"60 min\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Apples, bananas, oranges, strawberries, lemon juice, sugar\",\n",
    "        \"instructions\": \"Cut the fruit into pieces, mix with lemon juice and sugar, refrigerate.\",\n",
    "        \"expected_time\": \"10 min\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Chicken breast, onion, curry powder, cream or coconut milk, oil, salt\",\n",
    "        \"instructions\": \"Brown the onion, add diced chicken, curry, and cream. Cook until the chicken is tender.\",\n",
    "        \"expected_time\": \"25 min\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Ladyfingers, mascarpone, eggs, sugar, coffee, cocoa powder\",\n",
    "        \"instructions\": \"Make a cream with yolks, sugar and mascarpone. Layer ladyfingers soaked in coffee and the cream. Dust with cocoa and chill.\",\n",
    "        \"expected_time\": \"30 min\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"2 eggs, 250ml milk, 125g flour, sugar, butter\",\n",
    "        \"instructions\": \"Mix flour, eggs and milk into a batter. Cook in pan with butter. Fill as desired.\",\n",
    "        \"expected_time\": \"20 min\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Mixed dried legumes, onion, carrot, celery, vegetable broth, oil, salt\",\n",
    "        \"instructions\": \"Soak legumes for 12 hours. SautÃ© onion, carrot, celery. Add legumes and broth and cook for at least an hour.\",\n",
    "        \"expected_time\": \"90 min\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prepare input texts for models (ingredients + instructions)\n",
    "texts = [r[\"ingredients\"] + \" \" + r[\"instructions\"] for r in recipes]\n",
    "\n",
    "# Tokenize and padding\n",
    "prep_sequences = prep_time_tokenizer.texts_to_sequences(texts)\n",
    "prep_padded = pad_sequences(prep_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Predicting time with probabilities\n",
    "prep_preds = prep_time_model.predict(prep_padded)\n",
    "prep_classes = prep_preds.argmax(axis=1)\n",
    "prep_labels = [inv_prep_time_label_map[c] for c in prep_classes]\n",
    "\n",
    "# Print results with probabilities\n",
    "for i, recipe in enumerate(recipes):\n",
    "    print(f\"Recipe {i+1}:\")\n",
    "    print(f\"  Expected preparation time: {recipe['expected_time']} minutes\")\n",
    "    print(f\"  Predicted preparation time: {prep_labels[i]}\")\n",
    "    prep_probs_str = \", \".join([f\"{inv_prep_time_label_map[j]}: {prep_preds[i][j]*100:.2f}%\" for j in range(len(prep_preds[i]))])\n",
    "    print(f\"  Preparation time probabilities: {prep_probs_str}\")"
   ],
   "id": "367571519be110ac",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 79ms/step\n",
      "Recipe 1:\n",
      "  Expected preparation time: 15 min minutes\n",
      "  Predicted preparation time: Fast (10-20 mins)\n",
      "  Preparation time probabilities: Fast (10-20 mins): 47.13%, Medium (20-40 mins): 32.66%, Slow (40-90 mins): 5.50%, Very fast (0-10 mins): 12.92%, Very slow (90+ mins): 1.79%\n",
      "Recipe 2:\n",
      "  Expected preparation time: 10 min minutes\n",
      "  Predicted preparation time: Fast (10-20 mins)\n",
      "  Preparation time probabilities: Fast (10-20 mins): 68.13%, Medium (20-40 mins): 23.42%, Slow (40-90 mins): 1.01%, Very fast (0-10 mins): 7.07%, Very slow (90+ mins): 0.36%\n",
      "Recipe 3:\n",
      "  Expected preparation time: 30 min minutes\n",
      "  Predicted preparation time: Medium (20-40 mins)\n",
      "  Preparation time probabilities: Fast (10-20 mins): 13.88%, Medium (20-40 mins): 62.13%, Slow (40-90 mins): 14.24%, Very fast (0-10 mins): 7.64%, Very slow (90+ mins): 2.11%\n",
      "Recipe 4:\n",
      "  Expected preparation time: 5 min minutes\n",
      "  Predicted preparation time: Medium (20-40 mins)\n",
      "  Preparation time probabilities: Fast (10-20 mins): 10.17%, Medium (20-40 mins): 69.86%, Slow (40-90 mins): 12.77%, Very fast (0-10 mins): 6.10%, Very slow (90+ mins): 1.10%\n",
      "Recipe 5:\n",
      "  Expected preparation time: 60 min minutes\n",
      "  Predicted preparation time: Slow (40-90 mins)\n",
      "  Preparation time probabilities: Fast (10-20 mins): 6.70%, Medium (20-40 mins): 3.22%, Slow (40-90 mins): 83.20%, Very fast (0-10 mins): 1.34%, Very slow (90+ mins): 5.55%\n",
      "Recipe 6:\n",
      "  Expected preparation time: 10 min minutes\n",
      "  Predicted preparation time: Fast (10-20 mins)\n",
      "  Preparation time probabilities: Fast (10-20 mins): 78.58%, Medium (20-40 mins): 11.56%, Slow (40-90 mins): 6.30%, Very fast (0-10 mins): 3.01%, Very slow (90+ mins): 0.55%\n",
      "Recipe 7:\n",
      "  Expected preparation time: 25 min minutes\n",
      "  Predicted preparation time: Medium (20-40 mins)\n",
      "  Preparation time probabilities: Fast (10-20 mins): 26.66%, Medium (20-40 mins): 65.08%, Slow (40-90 mins): 1.45%, Very fast (0-10 mins): 6.73%, Very slow (90+ mins): 0.09%\n",
      "Recipe 8:\n",
      "  Expected preparation time: 30 min minutes\n",
      "  Predicted preparation time: Medium (20-40 mins)\n",
      "  Preparation time probabilities: Fast (10-20 mins): 1.41%, Medium (20-40 mins): 64.18%, Slow (40-90 mins): 23.63%, Very fast (0-10 mins): 6.49%, Very slow (90+ mins): 4.30%\n",
      "Recipe 9:\n",
      "  Expected preparation time: 20 min minutes\n",
      "  Predicted preparation time: Medium (20-40 mins)\n",
      "  Preparation time probabilities: Fast (10-20 mins): 8.58%, Medium (20-40 mins): 65.82%, Slow (40-90 mins): 12.20%, Very fast (0-10 mins): 11.97%, Very slow (90+ mins): 1.43%\n",
      "Recipe 10:\n",
      "  Expected preparation time: 90 min minutes\n",
      "  Predicted preparation time: Very slow (90+ mins)\n",
      "  Preparation time probabilities: Fast (10-20 mins): 0.02%, Medium (20-40 mins): 0.19%, Slow (40-90 mins): 2.49%, Very fast (0-10 mins): 0.01%, Very slow (90+ mins): 97.30%\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creazione del modello per la classificazione del tag 'DIFFICULTY' e testing",
   "id": "2b694bd06c7801a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T09:36:06.170201Z",
     "start_time": "2025-05-30T09:35:03.409531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Default parameters\n",
    "MAX_LEN = 500\n",
    "NUM_WORDS = 20000\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "def preprocess(df, text_columns, label_column, n=10000):\n",
    "    df = df[text_columns + [label_column]].dropna().head(n).copy()\n",
    "    df[text_columns[0]] = df[text_columns[0]].apply(eval)\n",
    "    df[text_columns[1]] = df[text_columns[1]].apply(eval)\n",
    "    df[\"full_text\"] = df[text_columns[0]].apply(lambda x: \" \".join(x)) + \" \" + df[text_columns[1]].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"encoded_label\"] = label_encoder.fit_transform(df[label_column])\n",
    "\n",
    "    return df, label_encoder\n",
    "\n",
    "def train_text_classifier(df, label_encoder, model_name_prefix):\n",
    "    X = df[\"full_text\"]\n",
    "    y = df[\"encoded_label\"]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "    X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=NUM_WORDS, output_dim=128, input_length=MAX_LEN),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(len(label_encoder.classes_), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train_pad, y_train,\n",
    "        validation_data=(X_val_pad, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "    model.save(f\"classifiers/difficulty/{model_name_prefix}.h5\")\n",
    "    with open(f\"classifiers/difficulty/{model_name_prefix}_label_mapping.pkl\", \"wb\") as f:\n",
    "        pickle.dump(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))), f)\n",
    "\n",
    "    with open(f\"classifiers/difficulty/{model_name_prefix}_tokenizer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "    print(f\"Modello '{model_name_prefix}' salvato con successo.\")\n",
    "    return model\n",
    "\n",
    "# === SECOND MODEL: DIFFICULTY ===\n",
    "df_difficulty = df[[\"INGREDIENTS\", \"DIRECTIONS\", \"DIFFICULTY\"]].copy()\n",
    "df_difficulty, difficulty_encoder = preprocess(df_difficulty, [\"INGREDIENTS\", \"DIRECTIONS\"], \"DIFFICULTY\")\n",
    "difficulty_model = train_text_classifier(df_difficulty, difficulty_encoder, model_name_prefix=\"difficulty_classifier\")"
   ],
   "id": "2c63caf69e9c1eb1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 36ms/step - accuracy: 0.5306 - loss: 0.9781 - val_accuracy: 0.6540 - val_loss: 0.7504\n",
      "Epoch 2/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 36ms/step - accuracy: 0.6609 - loss: 0.7271 - val_accuracy: 0.7010 - val_loss: 0.6436\n",
      "Epoch 3/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 34ms/step - accuracy: 0.7199 - loss: 0.6200 - val_accuracy: 0.7205 - val_loss: 0.6078\n",
      "Epoch 4/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 33ms/step - accuracy: 0.7570 - loss: 0.5758 - val_accuracy: 0.7185 - val_loss: 0.6058\n",
      "Epoch 5/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 32ms/step - accuracy: 0.7746 - loss: 0.5215 - val_accuracy: 0.7125 - val_loss: 0.6461\n",
      "Epoch 6/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 31ms/step - accuracy: 0.7881 - loss: 0.4950 - val_accuracy: 0.7185 - val_loss: 0.6101\n",
      "Epoch 7/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 32ms/step - accuracy: 0.8064 - loss: 0.4509 - val_accuracy: 0.7170 - val_loss: 0.6240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello 'difficulty_classifier' salvato con successo.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T17:03:29.976590Z",
     "start_time": "2025-06-19T17:03:05.617781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "MAX_LEN = 500  # same as training\n",
    "\n",
    "# Load model and tokenizer\n",
    "difficulty_model = load_model(\"classifiers/difficulty/difficulty_classifier.h5\")\n",
    "\n",
    "with open(\"classifiers/difficulty/difficulty_classifier_tokenizer.pkl\", \"rb\") as f:\n",
    "    difficulty_tokenizer = pickle.load(f)\n",
    "\n",
    "with open(\"classifiers/difficulty/difficulty_classifier_label_mapping.pkl\", \"rb\") as f:\n",
    "    difficulty_label_map = pickle.load(f)\n",
    "\n",
    "# Reverse dictionaries to decode predictions\n",
    "inv_difficulty_label_map = {v: k for k, v in difficulty_label_map.items()}\n",
    "\n",
    "# List of 10 recipes with expected difficulty\n",
    "recipes = [\n",
    "    {\n",
    "        \"ingredients\": \"Eggs, salt, pepper\",\n",
    "        \"instructions\": \"Beat eggs with salt and pepper. Cook in a pan over medium heat until set.\",\n",
    "        \"expected_difficulty\": \"Easy\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Chicken breast, olive oil, garlic, lemon juice, herbs\",\n",
    "        \"instructions\": \"Marinate chicken with garlic, lemon juice, and herbs. Grill until cooked.\",\n",
    "        \"expected_difficulty\": \"Easy\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Potatoes, butter, milk, salt, pepper\",\n",
    "        \"instructions\": \"Boil potatoes until tender. Mash with butter and milk. Season with salt and pepper.\",\n",
    "        \"expected_difficulty\": \"Easy\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Pasta, tomato sauce, garlic, onion, basil, olive oil\",\n",
    "        \"instructions\": \"SautÃ© garlic and onion in olive oil. Add tomato sauce and basil. Cook pasta and mix with sauce.\",\n",
    "        \"expected_difficulty\": \"Medium\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Beef stew meat, carrots, potatoes, onion, beef broth, red wine, garlic\",\n",
    "        \"instructions\": \"Brown beef in a pot. Add vegetables, broth, and wine. Simmer for 2 hours until tender.\",\n",
    "        \"expected_difficulty\": \"Medium\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Salmon fillets, soy sauce, ginger, garlic, honey, sesame oil\",\n",
    "        \"instructions\": \"Marinate salmon with soy sauce, ginger, garlic, honey, and sesame oil. Bake at 180Â°C for 15 minutes.\",\n",
    "        \"expected_difficulty\": \"Medium\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Duck breast, orange juice, honey, thyme, butter\",\n",
    "        \"instructions\": \"Score duck skin and sear in pan. Prepare orange sauce with juice, honey, and thyme. Finish cooking duck and glaze with sauce.\",\n",
    "        \"expected_difficulty\": \"Hard\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Lamb shank, rosemary, garlic, red wine, beef broth, carrots, onions\",\n",
    "        \"instructions\": \"Brown lamb shanks. Add vegetables, broth, and wine. Slow cook in oven at 150Â°C for 3 hours until tender.\",\n",
    "        \"expected_difficulty\": \"Hard\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Chocolate, eggs, sugar, butter, flour, cream, gelatin, vanilla bean\",\n",
    "        \"instructions\": \"Melt chocolate and butter. Whisk eggs and sugar. Fold ingredients, bake in water bath at 160Â°C for 45 minutes. Cool and refrigerate overnight.\",\n",
    "        \"expected_difficulty\": \"Hard\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Scallops, bacon, garlic, lemon, parsley, white wine\",\n",
    "        \"instructions\": \"Wrap scallops with bacon and sear. Make sauce with garlic, lemon, parsley, and white wine. Serve scallops with sauce.\",\n",
    "        \"expected_difficulty\": \"Hard\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prepare input texts for models (ingredients + instructions)\n",
    "texts = [r[\"ingredients\"] + \" \" + r[\"instructions\"] for r in recipes]\n",
    "\n",
    "# Tokenize and padding\n",
    "diff_sequences = difficulty_tokenizer.texts_to_sequences(texts)\n",
    "diff_padded = pad_sequences(diff_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Predicting difficulty\n",
    "diff_preds = difficulty_model.predict(diff_padded)\n",
    "diff_classes = diff_preds.argmax(axis=1)\n",
    "diff_labels = [inv_difficulty_label_map[c] for c in diff_classes]\n",
    "\n",
    "# Print results\n",
    "for i, recipe in enumerate(recipes):\n",
    "    print(f\"Recipe {i+1}:\")\n",
    "    print(f\"  Expected difficulty: {recipe['expected_difficulty']}\")\n",
    "    print(f\"  Predicted difficulty: {diff_labels[i]}\")\n",
    "    diff_probs_str = \", \".join([f\"{inv_difficulty_label_map[j]}: {diff_preds[i][j]*100:.2f}%\" for j in range(len(diff_preds[i]))])\n",
    "    print(f\"  Difficulty probabilities: {diff_probs_str}\\n\")"
   ],
   "id": "8c4c90ec82e24a52",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 300ms/step\n",
      "Recipe 1:\n",
      "  Expected difficulty: Easy\n",
      "  Predicted difficulty: Easy\n",
      "  Difficulty probabilities: Easy: 87.47%, Hard: 0.05%, Medium: 12.48%\n",
      "\n",
      "Recipe 2:\n",
      "  Expected difficulty: Easy\n",
      "  Predicted difficulty: Easy\n",
      "  Difficulty probabilities: Easy: 88.35%, Hard: 0.10%, Medium: 11.55%\n",
      "\n",
      "Recipe 3:\n",
      "  Expected difficulty: Easy\n",
      "  Predicted difficulty: Easy\n",
      "  Difficulty probabilities: Easy: 76.88%, Hard: 0.32%, Medium: 22.80%\n",
      "\n",
      "Recipe 4:\n",
      "  Expected difficulty: Medium\n",
      "  Predicted difficulty: Easy\n",
      "  Difficulty probabilities: Easy: 85.94%, Hard: 0.10%, Medium: 13.96%\n",
      "\n",
      "Recipe 5:\n",
      "  Expected difficulty: Medium\n",
      "  Predicted difficulty: Easy\n",
      "  Difficulty probabilities: Easy: 55.37%, Hard: 0.47%, Medium: 44.16%\n",
      "\n",
      "Recipe 6:\n",
      "  Expected difficulty: Medium\n",
      "  Predicted difficulty: Easy\n",
      "  Difficulty probabilities: Easy: 70.07%, Hard: 0.30%, Medium: 29.62%\n",
      "\n",
      "Recipe 7:\n",
      "  Expected difficulty: Hard\n",
      "  Predicted difficulty: Medium\n",
      "  Difficulty probabilities: Easy: 45.84%, Hard: 5.75%, Medium: 48.41%\n",
      "\n",
      "Recipe 8:\n",
      "  Expected difficulty: Hard\n",
      "  Predicted difficulty: Easy\n",
      "  Difficulty probabilities: Easy: 53.12%, Hard: 0.73%, Medium: 46.15%\n",
      "\n",
      "Recipe 9:\n",
      "  Expected difficulty: Hard\n",
      "  Predicted difficulty: Medium\n",
      "  Difficulty probabilities: Easy: 0.14%, Hard: 48.63%, Medium: 51.23%\n",
      "\n",
      "Recipe 10:\n",
      "  Expected difficulty: Hard\n",
      "  Predicted difficulty: Easy\n",
      "  Difficulty probabilities: Easy: 85.85%, Hard: 0.38%, Medium: 13.77%\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creazione del modello per la classificazione del tag 'VEGAN' e testing",
   "id": "a4918a853d499001"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T20:20:50.295554300Z",
     "start_time": "2025-05-21T20:20:50.293549800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Default parameters\n",
    "MAX_LEN = 500\n",
    "NUM_WORDS = 20000\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "def preprocess(df, text_columns, label_column, n=10000):\n",
    "    df = df[text_columns + [label_column]].dropna().head(n).copy()\n",
    "    df[text_columns[0]] = df[text_columns[0]].apply(eval)\n",
    "    df[text_columns[1]] = df[text_columns[1]].apply(eval)\n",
    "    df[\"full_text\"] = df[text_columns[0]].apply(lambda x: \" \".join(x)) + \" \" + df[text_columns[1]].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"encoded_label\"] = label_encoder.fit_transform(df[label_column])\n",
    "\n",
    "    return df, label_encoder\n",
    "\n",
    "def train_text_classifier(df, label_encoder, model_name_prefix):\n",
    "    X = df[\"full_text\"]\n",
    "    y = df[\"encoded_label\"]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "    X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=NUM_WORDS, output_dim=128, input_length=MAX_LEN),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(len(label_encoder.classes_), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train_pad, y_train,\n",
    "        validation_data=(X_val_pad, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "    model.save(f\"classifiers/vegan/{model_name_prefix}.h5\")\n",
    "    with open(f\"classifiers/vegan/{model_name_prefix}_label_mapping.pkl\", \"wb\") as f:\n",
    "        pickle.dump(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))), f)\n",
    "\n",
    "    with open(f\"classifiers/vegan/{model_name_prefix}_tokenizer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "    print(f\"Modello '{model_name_prefix}' salvato con successo.\")\n",
    "    return model\n",
    "\n",
    "# === THIRD MODEL: VEGAN ===\n",
    "df_vegan = df[[\"INGREDIENTS\", \"DIRECTIONS\", \"VEGAN\"]].copy()\n",
    "df_vegan, vegan_encoder = preprocess(df_vegan, [\"INGREDIENTS\", \"DIRECTIONS\"], \"VEGAN\")\n",
    "vegan_classifier_model = train_text_classifier(df_vegan, vegan_encoder, model_name_prefix=\"vegan_classifier\")"
   ],
   "id": "a46683e1e4a3fd77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T07:43:41.902910Z",
     "start_time": "2025-05-30T07:43:41.452893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "MAX_LEN = 500  # same as training\n",
    "\n",
    "# Load model and tokenizer\n",
    "vegan_model = load_model(\"classifiers/vegan/vegan_classifier.h5\")\n",
    "\n",
    "with open(\"classifiers/vegan/vegan_classifier_tokenizer.pkl\", \"rb\") as f:\n",
    "    vegan_tokenizer = pickle.load(f)\n",
    "\n",
    "with open(\"classifiers/vegan/vegan_classifier_label_mapping.pkl\", \"rb\") as f:\n",
    "    vegan_label_map = pickle.load(f)\n",
    "\n",
    "# Reverse dictionaries to decode predictions\n",
    "inv_vegan_label_map = {v: k for k, v in vegan_label_map.items()}\n",
    "\n",
    "# List of 10 recipes with expected vegan\n",
    "recipes = [\n",
    "    {\n",
    "        \"ingredients\": \"200g spaghetti, 2 cloves garlic, chili pepper, extra virgin olive oil, salt\",\n",
    "        \"instructions\": \"Cook the spaghetti. Meanwhile, sautÃ© garlic and chili in the oil. Mix with the pasta.\",\n",
    "        \"expected_vegan\": True\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Lettuce, 1 can of tuna, tomatoes, black olives, onion, olive oil, salt\",\n",
    "        \"instructions\": \"Wash and cut the vegetables. Add the drained tuna and olives. Dress with oil and salt.\",\n",
    "        \"expected_vegan\": False\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"300g Carnaroli rice, 1 saffron sachet, vegetable broth, butter, onion, Parmesan cheese\",\n",
    "        \"instructions\": \"SautÃ© the onion, add rice and deglaze with wine. Gradually pour in the broth. Add saffron and stir in butter and cheese.\",\n",
    "        \"expected_vegan\": False\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"3 eggs, salt, pepper, butter\",\n",
    "        \"instructions\": \"Beat the eggs with salt and pepper. Pour into a pan with melted butter and stir until set.\",\n",
    "        \"expected_vegan\": False\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Lasagna sheets, meat sauce, bÃ©chamel, Parmesan cheese\",\n",
    "        \"instructions\": \"Layer pasta, sauce, bÃ©chamel and cheese. Bake at 180Â°C for 40 minutes.\",\n",
    "        \"expected_vegan\": False\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Apples, bananas, oranges, strawberries, lemon juice, sugar\",\n",
    "        \"instructions\": \"Cut the fruit into pieces, mix with lemon juice and sugar, refrigerate.\",\n",
    "        \"expected_vegan\": True\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Chicken breast, onion, curry powder, cream or coconut milk, oil, salt\",\n",
    "        \"instructions\": \"Brown the onion, add diced chicken, curry, and cream. Cook until the chicken is tender.\",\n",
    "        \"expected_vegan\": False\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Ladyfingers, mascarpone, eggs, sugar, coffee, cocoa powder\",\n",
    "        \"instructions\": \"Make a cream with yolks, sugar and mascarpone. Layer ladyfingers soaked in coffee and the cream. Dust with cocoa and chill.\",\n",
    "        \"expected_vegan\": False\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"2 eggs, 250ml milk, 125g flour, sugar, butter\",\n",
    "        \"instructions\": \"Mix flour, eggs and milk into a batter. Cook in pan with butter. Fill as desired.\",\n",
    "        \"expected_vegan\": False\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Mixed dried legumes, onion, carrot, celery, vegetable broth, oil, salt\",\n",
    "        \"instructions\": \"Soak legumes for 12 hours. SautÃ© onion, carrot, celery. Add legumes and broth and cook for at least an hour.\",\n",
    "        \"expected_vegan\": True\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prepare input texts for models (ingredients + instructions)\n",
    "texts = [r[\"ingredients\"] + \" \" + r[\"instructions\"] for r in recipes]\n",
    "\n",
    "# Tokenize and padding\n",
    "vegan_sequences = vegan_tokenizer.texts_to_sequences(texts)\n",
    "vegan_padded = pad_sequences(vegan_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Predicting vegan true or false\n",
    "vegan_preds = vegan_model.predict(vegan_padded)\n",
    "vegan_classes = vegan_preds.argmax(axis=1)\n",
    "vegan_labels = [inv_vegan_label_map[c] for c in vegan_classes]\n",
    "\n",
    "# Print results\n",
    "for i, recipe in enumerate(recipes):\n",
    "    print(f\"Recipe {i+1}:\")\n",
    "    print(f\"  Expected vegan: {recipe['expected_vegan']}\")\n",
    "    print(f\"  Predicted vegan: {vegan_labels[i]}\")\n",
    "    vegan_probs_str = \", \".join([f\"{inv_vegan_label_map[j]}: {vegan_preds[i][j]*100:.2f}%\" for j in range(len(vegan_preds[i]))])\n",
    "    print(f\"  Vegan probabilities: {vegan_probs_str}\\n\")"
   ],
   "id": "3de5d93512c1d065",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 182ms/step\n",
      "Recipe 1:\n",
      "  Expected vegan: True\n",
      "  Predicted vegan: True\n",
      "  Vegan probabilities: False: 4.01%, True: 95.99%\n",
      "\n",
      "Recipe 2:\n",
      "  Expected vegan: False\n",
      "  Predicted vegan: True\n",
      "  Vegan probabilities: False: 19.34%, True: 80.66%\n",
      "\n",
      "Recipe 3:\n",
      "  Expected vegan: False\n",
      "  Predicted vegan: False\n",
      "  Vegan probabilities: False: 99.87%, True: 0.13%\n",
      "\n",
      "Recipe 4:\n",
      "  Expected vegan: False\n",
      "  Predicted vegan: False\n",
      "  Vegan probabilities: False: 99.95%, True: 0.05%\n",
      "\n",
      "Recipe 5:\n",
      "  Expected vegan: False\n",
      "  Predicted vegan: False\n",
      "  Vegan probabilities: False: 99.07%, True: 0.93%\n",
      "\n",
      "Recipe 6:\n",
      "  Expected vegan: True\n",
      "  Predicted vegan: True\n",
      "  Vegan probabilities: False: 3.02%, True: 96.98%\n",
      "\n",
      "Recipe 7:\n",
      "  Expected vegan: False\n",
      "  Predicted vegan: False\n",
      "  Vegan probabilities: False: 99.99%, True: 0.01%\n",
      "\n",
      "Recipe 8:\n",
      "  Expected vegan: False\n",
      "  Predicted vegan: False\n",
      "  Vegan probabilities: False: 99.95%, True: 0.05%\n",
      "\n",
      "Recipe 9:\n",
      "  Expected vegan: False\n",
      "  Predicted vegan: False\n",
      "  Vegan probabilities: False: 100.00%, True: 0.00%\n",
      "\n",
      "Recipe 10:\n",
      "  Expected vegan: True\n",
      "  Predicted vegan: True\n",
      "  Vegan probabilities: False: 27.34%, True: 72.66%\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creazione del modello per la classificazione del tag 'VEGETARIAN' e testing",
   "id": "35c80420b43a76c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-21T20:20:50.297555600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Default parameters\n",
    "MAX_LEN = 500\n",
    "NUM_WORDS = 20000\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "def preprocess(df, text_columns, label_column, n=10000):\n",
    "    df = df[text_columns + [label_column]].dropna().head(n).copy()\n",
    "    df[text_columns[0]] = df[text_columns[0]].apply(eval)\n",
    "    df[text_columns[1]] = df[text_columns[1]].apply(eval)\n",
    "    df[\"full_text\"] = df[text_columns[0]].apply(lambda x: \" \".join(x)) + \" \" + df[text_columns[1]].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"encoded_label\"] = label_encoder.fit_transform(df[label_column])\n",
    "\n",
    "    return df, label_encoder\n",
    "\n",
    "def train_text_classifier(df, label_encoder, model_name_prefix):\n",
    "    X = df[\"full_text\"]\n",
    "    y = df[\"encoded_label\"]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "    X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=NUM_WORDS, output_dim=128, input_length=MAX_LEN),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(len(label_encoder.classes_), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train_pad, y_train,\n",
    "        validation_data=(X_val_pad, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "    model.save(f\"classifiers/vegetarian/{model_name_prefix}.h5\")\n",
    "    with open(f\"classifiers/vegetarian/{model_name_prefix}_label_mapping.pkl\", \"wb\") as f:\n",
    "        pickle.dump(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))), f)\n",
    "\n",
    "    with open(f\"classifiers/vegetarian/{model_name_prefix}_tokenizer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "    print(f\"Modello '{model_name_prefix}' salvato con successo.\")\n",
    "    return model\n",
    "\n",
    "# === FOURTH MODEL: VEGETARIAN ===\n",
    "df_vegetarian = df[[\"INGREDIENTS\", \"DIRECTIONS\", \"VEGETARIAN\"]].copy()\n",
    "df_vegetarian, vegetarian_encoder = preprocess(df_vegetarian, [\"INGREDIENTS\", \"DIRECTIONS\"], \"VEGETARIAN\")\n",
    "vegetarian_classifier_model = train_text_classifier(df_vegetarian, vegetarian_encoder, model_name_prefix=\"vegetarian_classifier\")"
   ],
   "id": "44472144d51bbbb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T07:45:04.317882Z",
     "start_time": "2025-05-30T07:45:03.920523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "MAX_LEN = 500  # same as training\n",
    "\n",
    "# Load model and tokenizer\n",
    "vegetarian_model = load_model(\"classifiers/vegetarian/vegetarian_classifier.h5\")\n",
    "\n",
    "with open(\"classifiers/vegetarian/vegetarian_classifier_tokenizer.pkl\", \"rb\") as f:\n",
    "    vegetarian_tokenizer = pickle.load(f)\n",
    "\n",
    "with open(\"classifiers/vegetarian/vegetarian_classifier_label_mapping.pkl\", \"rb\") as f:\n",
    "    vegetarian_label_map = pickle.load(f)\n",
    "\n",
    "# Reverse dictionaries to decode predictions\n",
    "inv_vegetarian_label_map = {v: k for k, v in vegetarian_label_map.items()}\n",
    "\n",
    "# List of 10 recipes with expected vegetarian\n",
    "recipes = [\n",
    "    {\n",
    "        \"ingredients\": \"200g spaghetti, 2 cloves garlic, chili pepper, extra virgin olive oil, salt\",\n",
    "        \"instructions\": \"Cook the spaghetti. Meanwhile, sautÃ© garlic and chili in the oil. Mix with the pasta.\",\n",
    "        \"expected_vegetarian\": True\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Lettuce, 1 can of tuna, tomatoes, black olives, onion, olive oil, salt\",\n",
    "        \"instructions\": \"Wash and cut the vegetables. Add the drained tuna and olives. Dress with oil and salt.\",\n",
    "        \"expected_vegetarian\": False  # Tuna = non vegetarian\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"300g Carnaroli rice, 1 saffron sachet, vegetable broth, butter, onion, Parmesan cheese\",\n",
    "        \"instructions\": \"SautÃ© the onion, add rice and deglaze with wine. Gradually pour in the broth. Add saffron and stir in butter and cheese.\",\n",
    "        \"expected_vegetarian\": True\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"3 eggs, salt, pepper, butter\",\n",
    "        \"instructions\": \"Beat the eggs with salt and pepper. Pour into a pan with melted butter and stir until set.\",\n",
    "        \"expected_vegetarian\": True\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Lasagna sheets, meat sauce, bÃ©chamel, Parmesan cheese\",\n",
    "        \"instructions\": \"Layer pasta, sauce, bÃ©chamel and cheese. Bake at 180Â°C for 40 minutes.\",\n",
    "        \"expected_vegetarian\": False  # Meat sauce = non vegetarian\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Apples, bananas, oranges, strawberries, lemon juice, sugar\",\n",
    "        \"instructions\": \"Cut the fruit into pieces, mix with lemon juice and sugar, refrigerate.\",\n",
    "        \"expected_vegetarian\": True\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Chicken breast, onion, curry powder, cream or coconut milk, oil, salt\",\n",
    "        \"instructions\": \"Brown the onion, add diced chicken, curry, and cream. Cook until the chicken is tender.\",\n",
    "        \"expected_vegetarian\": False  # Chicken = non vegetarian\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Ladyfingers, mascarpone, eggs, sugar, coffee, cocoa powder\",\n",
    "        \"instructions\": \"Make a cream with yolks, sugar and mascarpone. Layer ladyfingers soaked in coffee and the cream. Dust with cocoa and chill.\",\n",
    "        \"expected_vegetarian\": True\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"2 eggs, 250ml milk, 125g flour, sugar, butter\",\n",
    "        \"instructions\": \"Mix flour, eggs and milk into a batter. Cook in pan with butter. Fill as desired.\",\n",
    "        \"expected_vegetarian\": True\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Mixed dried legumes, onion, carrot, celery, vegetable broth, oil, salt\",\n",
    "        \"instructions\": \"Soak legumes for 12 hours. SautÃ© onion, carrot, celery. Add legumes and broth and cook for at least an hour.\",\n",
    "        \"expected_vegetarian\": True\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prepare input texts for models (ingredients + instructions)\n",
    "texts = [r[\"ingredients\"] + \" \" + r[\"instructions\"] for r in recipes]\n",
    "\n",
    "# Tokenize and padding\n",
    "vegetarian_sequences = vegetarian_tokenizer.texts_to_sequences(texts)\n",
    "vegetarian_padded = pad_sequences(vegetarian_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Predicting vegetarian true or false\n",
    "vegetarian_preds = vegetarian_model.predict(vegetarian_padded)\n",
    "vegetarian_classes = vegetarian_preds.argmax(axis=1)\n",
    "vegetarian_labels = [inv_vegetarian_label_map[c] for c in vegetarian_classes]\n",
    "\n",
    "# Print results\n",
    "for i, recipe in enumerate(recipes):\n",
    "    print(f\"Recipe {i+1}:\")\n",
    "    print(f\"  Expected vegetarian: {recipe['expected_vegetarian']}\")\n",
    "    print(f\"  Predicted vegetarian: {vegetarian_labels[i]}\")\n",
    "    vegetarian_probs_str = \", \".join([f\"{inv_vegetarian_label_map[j]}: {vegetarian_preds[i][j]*100:.2f}%\" for j in range(len(vegetarian_preds[i]))])\n",
    "    print(f\"  Vegetarian probabilities: {vegetarian_probs_str}\\n\")"
   ],
   "id": "63280a53e93b2d1d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 165ms/step\n",
      "Recipe 1:\n",
      "  Expected vegetarian: True\n",
      "  Predicted vegetarian: True\n",
      "  Vegetarian probabilities: False: 2.47%, True: 97.53%\n",
      "\n",
      "Recipe 2:\n",
      "  Expected vegetarian: False\n",
      "  Predicted vegetarian: False\n",
      "  Vegetarian probabilities: False: 93.06%, True: 6.94%\n",
      "\n",
      "Recipe 3:\n",
      "  Expected vegetarian: True\n",
      "  Predicted vegetarian: False\n",
      "  Vegetarian probabilities: False: 52.48%, True: 47.52%\n",
      "\n",
      "Recipe 4:\n",
      "  Expected vegetarian: True\n",
      "  Predicted vegetarian: True\n",
      "  Vegetarian probabilities: False: 0.37%, True: 99.63%\n",
      "\n",
      "Recipe 5:\n",
      "  Expected vegetarian: False\n",
      "  Predicted vegetarian: False\n",
      "  Vegetarian probabilities: False: 52.54%, True: 47.46%\n",
      "\n",
      "Recipe 6:\n",
      "  Expected vegetarian: True\n",
      "  Predicted vegetarian: True\n",
      "  Vegetarian probabilities: False: 0.11%, True: 99.89%\n",
      "\n",
      "Recipe 7:\n",
      "  Expected vegetarian: False\n",
      "  Predicted vegetarian: False\n",
      "  Vegetarian probabilities: False: 98.37%, True: 1.63%\n",
      "\n",
      "Recipe 8:\n",
      "  Expected vegetarian: True\n",
      "  Predicted vegetarian: True\n",
      "  Vegetarian probabilities: False: 0.09%, True: 99.91%\n",
      "\n",
      "Recipe 9:\n",
      "  Expected vegetarian: True\n",
      "  Predicted vegetarian: True\n",
      "  Vegetarian probabilities: False: 0.80%, True: 99.20%\n",
      "\n",
      "Recipe 10:\n",
      "  Expected vegetarian: True\n",
      "  Predicted vegetarian: True\n",
      "  Vegetarian probabilities: False: 26.53%, True: 73.47%\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creazione del modello per la classificazione del tag 'COOKING_METHOD' e testing",
   "id": "261721095f30435a"
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Default parameters\n",
    "MAX_LEN = 500\n",
    "NUM_WORDS = 20000\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "def preprocess(df, text_columns, label_column, n=10000):\n",
    "    df = df[text_columns + [label_column]].dropna().head(n).copy()\n",
    "    df[text_columns[0]] = df[text_columns[0]].apply(eval)\n",
    "    df[text_columns[1]] = df[text_columns[1]].apply(eval)\n",
    "    df[\"full_text\"] = df[text_columns[0]].apply(lambda x: \" \".join(x)) + \" \" + df[text_columns[1]].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"encoded_label\"] = label_encoder.fit_transform(df[label_column])\n",
    "\n",
    "    return df, label_encoder\n",
    "\n",
    "def train_text_classifier(df, label_encoder, model_name_prefix):\n",
    "    X = df[\"full_text\"]\n",
    "    y = df[\"encoded_label\"]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "    X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=NUM_WORDS, output_dim=128, input_length=MAX_LEN),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(len(label_encoder.classes_), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train_pad, y_train,\n",
    "        validation_data=(X_val_pad, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "    model.save(f\"classifiers/method/{model_name_prefix}.h5\")\n",
    "    with open(f\"classifiers/method/{model_name_prefix}_label_mapping.pkl\", \"wb\") as f:\n",
    "        pickle.dump(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))), f)\n",
    "\n",
    "    with open(f\"classifiers/method/{model_name_prefix}_tokenizer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "    print(f\"Modello '{model_name_prefix}' salvato con successo.\")\n",
    "    return model\n",
    "\n",
    "# === FIFTH MODEL: METHOD ===\n",
    "df_method = df[[\"INGREDIENTS\", \"DIRECTIONS\", \"COOKING_METHOD\"]].copy()\n",
    "df_method, method_encoder = preprocess(df_method, [\"INGREDIENTS\", \"DIRECTIONS\"], \"COOKING_METHOD\")\n",
    "method_classifier_model = train_text_classifier(df_method, method_encoder, model_name_prefix=\"method_classifier\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-22T16:00:33.783092Z",
     "start_time": "2025-05-22T15:58:56.932520Z"
    }
   },
   "id": "7e31b42298771f5d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 37ms/step - accuracy: 0.4523 - loss: 1.7384 - val_accuracy: 0.7650 - val_loss: 0.7940\n",
      "Epoch 2/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 37ms/step - accuracy: 0.7853 - loss: 0.7283 - val_accuracy: 0.8660 - val_loss: 0.4808\n",
      "Epoch 3/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 34ms/step - accuracy: 0.8792 - loss: 0.4444 - val_accuracy: 0.9255 - val_loss: 0.3213\n",
      "Epoch 4/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 33ms/step - accuracy: 0.9122 - loss: 0.3320 - val_accuracy: 0.9340 - val_loss: 0.2616\n",
      "Epoch 5/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 32ms/step - accuracy: 0.9250 - loss: 0.2618 - val_accuracy: 0.9315 - val_loss: 0.2384\n",
      "Epoch 6/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 32ms/step - accuracy: 0.9327 - loss: 0.2262 - val_accuracy: 0.9430 - val_loss: 0.2156\n",
      "Epoch 7/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 36ms/step - accuracy: 0.9402 - loss: 0.2127 - val_accuracy: 0.9495 - val_loss: 0.1973\n",
      "Epoch 8/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 36ms/step - accuracy: 0.9459 - loss: 0.1780 - val_accuracy: 0.9490 - val_loss: 0.1830\n",
      "Epoch 9/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 33ms/step - accuracy: 0.9601 - loss: 0.1343 - val_accuracy: 0.9485 - val_loss: 0.1751\n",
      "Epoch 10/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 32ms/step - accuracy: 0.9579 - loss: 0.1358 - val_accuracy: 0.9495 - val_loss: 0.1753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello 'method_classifier' salvato con successo.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T07:46:23.713525Z",
     "start_time": "2025-05-30T07:46:23.318616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "MAX_LEN = 500  # same as training\n",
    "\n",
    "# Load model and tokenizer\n",
    "method_model = load_model(\"classifiers/method/method_classifier.h5\")\n",
    "\n",
    "with open(\"classifiers/method/method_classifier_tokenizer.pkl\", \"rb\") as f:\n",
    "    method_tokenizer = pickle.load(f)\n",
    "\n",
    "with open(\"classifiers/method/method_classifier_label_mapping.pkl\", \"rb\") as f:\n",
    "    method_label_map = pickle.load(f)\n",
    "\n",
    "# Reverse dictionaries to decode predictions\n",
    "inv_method_label_map = {v: k for k, v in method_label_map.items()}\n",
    "\n",
    "# List of 10 recipes with expected method\n",
    "recipes = [\n",
    "    {\n",
    "        \"ingredients\": \"500g potatoes, olive oil, rosemary, salt\",\n",
    "        \"instructions\": \"Peel and cut the potatoes into chunks. Toss with olive oil and rosemary. Cook in oven at 200Â°C for 45 minutes.\",\n",
    "        \"expected_method\": \"Roasted\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"200g spaghetti, salt, water\",\n",
    "        \"instructions\": \"Boil water in a large pot, add salt and cook the spaghetti until al dente.\",\n",
    "        \"expected_method\": \"Boiled\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"2 chicken breasts, olive oil, herbs, salt\",\n",
    "        \"instructions\": \"Brush chicken with oil and herbs. Cook on a preheated grill for 6 minutes per side.\",\n",
    "        \"expected_method\": \"Grilled\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"4 eggs, butter, salt, pepper\",\n",
    "        \"instructions\": \"Heat butter in a pan. Crack eggs in and fry until edges are crispy.\",\n",
    "        \"expected_method\": \"Fried\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"1 salmon fillet, lemon juice, herbs, salt\",\n",
    "        \"instructions\": \"Place salmon in a baking dish, season, and bake at 180Â°C for 20 minutes.\",\n",
    "        \"expected_method\": \"Baked\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Broccoli, carrots, zucchini, salt\",\n",
    "        \"instructions\": \"Wash and cut vegetables. Eat raw or season with salt and olive oil.\",\n",
    "        \"expected_method\": \"Raw\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Beef steak, salt, pepper, olive oil\",\n",
    "        \"instructions\": \"Season steak and place under a broiler for 5â7 minutes on each side.\",\n",
    "        \"expected_method\": \"Broil\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"1 cauliflower, salt, oil, paprika\",\n",
    "        \"instructions\": \"Cut cauliflower into florets. Toss with oil and paprika. Roast in oven at 220Â°C for 30 minutes.\",\n",
    "        \"expected_method\": \"Roasted\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Rice, water, salt\",\n",
    "        \"instructions\": \"Rinse the rice and boil in salted water for 15 minutes.\",\n",
    "        \"expected_method\": \"Boiled\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"1 can chickpeas, tahini, lemon juice, garlic, olive oil\",\n",
    "        \"instructions\": \"Blend all ingredients in a food processor until smooth. Serve as is.\",\n",
    "        \"expected_method\": \"Other\"  # Nessuna cottura classica, solo preparazione a crudo e frullatura\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prepare input texts for models (ingredients + instructions)\n",
    "texts = [r[\"ingredients\"] + \" \" + r[\"instructions\"] for r in recipes]\n",
    "\n",
    "# Tokenize and padding\n",
    "method_sequences = method_tokenizer.texts_to_sequences(texts)\n",
    "method_padded = pad_sequences(method_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Predicting method\n",
    "method_preds = method_model.predict(method_padded)\n",
    "method_classes = method_preds.argmax(axis=1)\n",
    "method_labels = [inv_method_label_map[c] for c in method_classes]\n",
    "\n",
    "# Print results\n",
    "for i, recipe in enumerate(recipes):\n",
    "    print(f\"Recipe {i+1}:\")\n",
    "    print(f\"  Expected method: {recipe['expected_method']}\")\n",
    "    print(f\"  Predicted method: {method_labels[i]}\")\n",
    "    method_probs_str = \", \".join([f\"{inv_method_label_map[j]}: {method_preds[i][j]*100:.2f}%\" for j in range(len(method_preds[i]))])\n",
    "    print(f\"  Method probabilities: {method_probs_str}\\n\")"
   ],
   "id": "da1c262d0a0539e4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 175ms/step\n",
      "Recipe 1:\n",
      "  Expected method: Roasted\n",
      "  Predicted method: Other\n",
      "  Method probabilities: Baked: 3.94%, Blended: 2.66%, Boiled: 1.07%, Broil: 4.60%, Fried: 4.42%, Grilled: 2.63%, Microwaved: 0.64%, Other: 53.05%, Pressure Cooked: 2.72%, Raw: 3.47%, Roasted: 7.69%, Slow Cooking: 0.96%, Steamed: 4.04%, Toasted: 8.10%\n",
      "\n",
      "Recipe 2:\n",
      "  Expected method: Boiled\n",
      "  Predicted method: Boiled\n",
      "  Method probabilities: Baked: 0.04%, Blended: 0.00%, Boiled: 99.95%, Broil: 0.00%, Fried: 0.00%, Grilled: 0.00%, Microwaved: 0.00%, Other: 0.00%, Pressure Cooked: 0.00%, Raw: 0.00%, Roasted: 0.00%, Slow Cooking: 0.00%, Steamed: 0.00%, Toasted: 0.00%\n",
      "\n",
      "Recipe 3:\n",
      "  Expected method: Grilled\n",
      "  Predicted method: Grilled\n",
      "  Method probabilities: Baked: 0.01%, Blended: 0.00%, Boiled: 0.07%, Broil: 1.44%, Fried: 1.18%, Grilled: 96.66%, Microwaved: 0.00%, Other: 0.06%, Pressure Cooked: 0.04%, Raw: 0.01%, Roasted: 0.14%, Slow Cooking: 0.23%, Steamed: 0.03%, Toasted: 0.12%\n",
      "\n",
      "Recipe 4:\n",
      "  Expected method: Fried\n",
      "  Predicted method: Fried\n",
      "  Method probabilities: Baked: 0.08%, Blended: 0.00%, Boiled: 0.24%, Broil: 0.20%, Fried: 98.93%, Grilled: 0.05%, Microwaved: 0.02%, Other: 0.03%, Pressure Cooked: 0.03%, Raw: 0.11%, Roasted: 0.02%, Slow Cooking: 0.07%, Steamed: 0.13%, Toasted: 0.10%\n",
      "\n",
      "Recipe 5:\n",
      "  Expected method: Baked\n",
      "  Predicted method: Baked\n",
      "  Method probabilities: Baked: 100.00%, Blended: 0.00%, Boiled: 0.00%, Broil: 0.00%, Fried: 0.00%, Grilled: 0.00%, Microwaved: 0.00%, Other: 0.00%, Pressure Cooked: 0.00%, Raw: 0.00%, Roasted: 0.00%, Slow Cooking: 0.00%, Steamed: 0.00%, Toasted: 0.00%\n",
      "\n",
      "Recipe 6:\n",
      "  Expected method: Raw\n",
      "  Predicted method: Other\n",
      "  Method probabilities: Baked: 0.28%, Blended: 5.24%, Boiled: 2.94%, Broil: 0.44%, Fried: 7.64%, Grilled: 1.34%, Microwaved: 1.63%, Other: 60.57%, Pressure Cooked: 1.42%, Raw: 5.89%, Roasted: 1.46%, Slow Cooking: 7.15%, Steamed: 1.53%, Toasted: 2.47%\n",
      "\n",
      "Recipe 7:\n",
      "  Expected method: Broil\n",
      "  Predicted method: Broil\n",
      "  Method probabilities: Baked: 1.07%, Blended: 0.13%, Boiled: 1.92%, Broil: 43.58%, Fried: 4.94%, Grilled: 31.59%, Microwaved: 2.54%, Other: 0.55%, Pressure Cooked: 1.41%, Raw: 0.59%, Roasted: 2.67%, Slow Cooking: 2.02%, Steamed: 4.19%, Toasted: 2.80%\n",
      "\n",
      "Recipe 8:\n",
      "  Expected method: Roasted\n",
      "  Predicted method: Roasted\n",
      "  Method probabilities: Baked: 21.76%, Blended: 3.07%, Boiled: 0.98%, Broil: 8.65%, Fried: 2.63%, Grilled: 4.23%, Microwaved: 0.27%, Other: 6.49%, Pressure Cooked: 1.93%, Raw: 1.29%, Roasted: 40.14%, Slow Cooking: 0.60%, Steamed: 4.28%, Toasted: 3.68%\n",
      "\n",
      "Recipe 9:\n",
      "  Expected method: Boiled\n",
      "  Predicted method: Boiled\n",
      "  Method probabilities: Baked: 0.07%, Blended: 0.00%, Boiled: 99.92%, Broil: 0.00%, Fried: 0.00%, Grilled: 0.00%, Microwaved: 0.00%, Other: 0.00%, Pressure Cooked: 0.00%, Raw: 0.00%, Roasted: 0.00%, Slow Cooking: 0.00%, Steamed: 0.00%, Toasted: 0.00%\n",
      "\n",
      "Recipe 10:\n",
      "  Expected method: Other\n",
      "  Predicted method: Blended\n",
      "  Method probabilities: Baked: 0.03%, Blended: 98.33%, Boiled: 0.02%, Broil: 0.04%, Fried: 0.12%, Grilled: 0.18%, Microwaved: 0.37%, Other: 0.07%, Pressure Cooked: 0.03%, Raw: 0.09%, Roasted: 0.19%, Slow Cooking: 0.30%, Steamed: 0.07%, Toasted: 0.17%\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creazione del modello per la classificazione del tag 'price_tag' e testing",
   "id": "7f6c9f41b341d99b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T10:30:21.070632Z",
     "start_time": "2025-06-21T10:28:56.429398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Default parameters\n",
    "MAX_LEN = 500\n",
    "NUM_WORDS = 20000\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# Method to preprocess a dataframe\n",
    "def preprocess(df, text_columns, label_column, n=10000):\n",
    "    df = df[text_columns + [label_column]].dropna().head(n).copy()\n",
    "    df[text_columns[0]] = df[text_columns[0]].apply(eval)\n",
    "    df[text_columns[1]] = df[text_columns[1]].apply(eval)\n",
    "    df[\"full_text\"] = df[text_columns[0]].apply(lambda x: \" \".join(x)) + \" \" + df[text_columns[1]].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"encoded_label\"] = label_encoder.fit_transform(df[label_column])\n",
    "\n",
    "    return df, label_encoder\n",
    "\n",
    "# Method to train a classificator\n",
    "def train_text_classifier(df, label_encoder, model_name_prefix):\n",
    "    X = df[\"full_text\"]\n",
    "    y = df[\"encoded_label\"]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "    X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=NUM_WORDS, output_dim=128, input_length=MAX_LEN),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(len(label_encoder.classes_), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train_pad, y_train,\n",
    "        validation_data=(X_val_pad, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "    # Saving model\n",
    "    model.save(f\"classifiers/price/{model_name_prefix}.h5\")\n",
    "    with open(f\"classifiers/price/{model_name_prefix}_label_mapping.pkl\", \"wb\") as f:\n",
    "        pickle.dump(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))), f)\n",
    "\n",
    "    with open(f\"classifiers/price/{model_name_prefix}_tokenizer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "    print(f\"Modello '{model_name_prefix}' salvato con successo.\")\n",
    "    return model\n",
    "\n",
    "# === SIXTH MODEL: price_tag ===\n",
    "df_price = df[[\"INGREDIENTS\", \"DIRECTIONS\", \"price_tag\"]].copy()\n",
    "df_price, price_encoder = preprocess(df_price, [\"INGREDIENTS\", \"DIRECTIONS\"], \"price_tag\")\n",
    "price_model = train_text_classifier(df_price, price_encoder, model_name_prefix=\"price_classifier\")"
   ],
   "id": "8eba9080102471a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 39ms/step - accuracy: 0.2718 - loss: 1.5594 - val_accuracy: 0.4255 - val_loss: 1.2593\n",
      "Epoch 2/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 35ms/step - accuracy: 0.4498 - loss: 1.2270 - val_accuracy: 0.5410 - val_loss: 1.0384\n",
      "Epoch 3/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 33ms/step - accuracy: 0.5099 - loss: 1.0964 - val_accuracy: 0.5890 - val_loss: 0.9638\n",
      "Epoch 4/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 33ms/step - accuracy: 0.5546 - loss: 1.0165 - val_accuracy: 0.5920 - val_loss: 0.9262\n",
      "Epoch 5/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 32ms/step - accuracy: 0.5782 - loss: 0.9685 - val_accuracy: 0.6065 - val_loss: 0.9025\n",
      "Epoch 6/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 31ms/step - accuracy: 0.6090 - loss: 0.8995 - val_accuracy: 0.6070 - val_loss: 0.8942\n",
      "Epoch 7/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 31ms/step - accuracy: 0.6142 - loss: 0.9076 - val_accuracy: 0.6235 - val_loss: 0.8834\n",
      "Epoch 8/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 33ms/step - accuracy: 0.6273 - loss: 0.8741 - val_accuracy: 0.6235 - val_loss: 0.8781\n",
      "Epoch 9/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 30ms/step - accuracy: 0.6360 - loss: 0.8547 - val_accuracy: 0.6360 - val_loss: 0.8773\n",
      "Epoch 10/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 30ms/step - accuracy: 0.6655 - loss: 0.8143 - val_accuracy: 0.6320 - val_loss: 0.8772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello 'price_classifier' salvato con successo.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T10:41:34.570039Z",
     "start_time": "2025-06-21T10:41:27.839119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "MAX_LEN = 500  # same as training\n",
    "\n",
    "# --- Load model and tokenizer ---\n",
    "price_model = load_model(\"classifiers/price/price_classifier.h5\")\n",
    "\n",
    "with open(\"classifiers/price/price_classifier_tokenizer.pkl\", \"rb\") as f:\n",
    "    price_tokenizer = pickle.load(f)\n",
    "\n",
    "with open(\"classifiers/price/price_classifier_label_mapping.pkl\", \"rb\") as f:\n",
    "    price_label_map = pickle.load(f)\n",
    "\n",
    "# Reverse dictionaries to decode predictions\n",
    "inv_price_label_map = {v: k for k, v in price_label_map.items()}\n",
    "\n",
    "# List of 10 recipes with expected cost category\n",
    "recipes = [\n",
    "    {\n",
    "        \"ingredients\": \"200g spaghetti, 2 cloves garlic, chili pepper, extra virgin olive oil, salt\",\n",
    "        \"instructions\": \"Cook the spaghetti. Meanwhile, sautÃ© garlic and chili in the oil. Mix with the pasta.\",\n",
    "        \"expected_cost\": \"very cheap\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Lettuce, 1 can of tuna, tomatoes, black olives, onion, olive oil, salt\",\n",
    "        \"instructions\": \"Wash and cut the vegetables. Add the drained tuna and olives. Dress with oil and salt.\",\n",
    "        \"expected_cost\": \"cheap\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"300g Carnaroli rice, 1 saffron sachet, vegetable broth, butter, onion, Parmesan cheese\",\n",
    "        \"instructions\": \"SautÃ© the onion, add rice and deglaze with wine. Gradually pour in the broth. Add saffron and stir in butter and cheese.\",\n",
    "        \"expected_cost\": \"expensive\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"3 eggs, salt, pepper, butter\",\n",
    "        \"instructions\": \"Beat the eggs with salt and pepper. Pour into a pan with melted butter and stir until set.\",\n",
    "        \"expected_cost\": \"very cheap\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Lasagna sheets, meat sauce, bÃ©chamel, Parmesan cheese\",\n",
    "        \"instructions\": \"Layer pasta, sauce, bÃ©chamel and cheese. Bake at 180Â°C for 40 minutes.\",\n",
    "        \"expected_cost\": \"expensive\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Apples, bananas, oranges, strawberries, lemon juice, sugar\",\n",
    "        \"instructions\": \"Cut the fruit into pieces, mix with lemon juice and sugar, refrigerate.\",\n",
    "        \"expected_cost\": \"cheap\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Chicken breast, onion, curry powder, cream or coconut milk, oil, salt\",\n",
    "        \"instructions\": \"Brown the onion, add diced chicken, curry, and cream. Cook until the chicken is tender.\",\n",
    "        \"expected_cost\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Ladyfingers, mascarpone, eggs, sugar, coffee, cocoa powder\",\n",
    "        \"instructions\": \"Make a cream with yolks, sugar and mascarpone. Layer ladyfingers soaked in coffee and the cream. Dust with cocoa and chill.\",\n",
    "        \"expected_cost\": \"expensive\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"2 eggs, 250ml milk, 125g flour, sugar, butter\",\n",
    "        \"instructions\": \"Mix flour, eggs and milk into a batter. Cook in pan with butter. Fill as desired.\",\n",
    "        \"expected_cost\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Mixed dried legumes, onion, carrot, celery, vegetable broth, oil, salt\",\n",
    "        \"instructions\": \"Soak legumes for 12 hours. SautÃ© onion, carrot, celery. Add legumes and broth and cook for at least an hour.\",\n",
    "        \"expected_cost\": \"rich\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prepare input texts for models (ingredients + instructions)\n",
    "texts = [r[\"ingredients\"] + \" \" + r[\"instructions\"] for r in recipes]\n",
    "\n",
    "# Tokenize and padding\n",
    "price_sequences = price_tokenizer.texts_to_sequences(texts)\n",
    "price_padded = pad_sequences(price_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Predicting cost tag with probabilities\n",
    "price_preds = price_model.predict(price_padded)\n",
    "price_classes = price_preds.argmax(axis=1)\n",
    "price_labels = [inv_price_label_map[c] for c in price_classes]\n",
    "\n",
    "# Print results with probabilities\n",
    "for i, recipe in enumerate(recipes):\n",
    "    print(f\"Recipe {i+1}:\")\n",
    "    print(f\"  Expected price tag: {recipe['expected_cost']}\")\n",
    "    print(f\"  Predicted price tag: {price_labels[i]}\")\n",
    "    price_probs_str = \", \".join([f\"{inv_price_label_map[j]}: {price_preds[i][j]*100:.2f}%\" for j in range(len(price_preds[i]))])\n",
    "    print(f\"  Price tag probabilities: {price_probs_str}\")"
   ],
   "id": "dd3a64b0a183ae9b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 107ms/step\n",
      "Recipe 1:\n",
      "  Expected price tag: very cheap\n",
      "  Predicted price tag: cheap\n",
      "  Price tag probabilities: cheap: 42.60%, expensive: 7.08%, medium: 27.69%, rich: 0.46%, very cheap: 22.18%\n",
      "Recipe 2:\n",
      "  Expected price tag: cheap\n",
      "  Predicted price tag: cheap\n",
      "  Price tag probabilities: cheap: 59.37%, expensive: 0.89%, medium: 11.98%, rich: 0.02%, very cheap: 27.75%\n",
      "Recipe 3:\n",
      "  Expected price tag: expensive\n",
      "  Predicted price tag: cheap\n",
      "  Price tag probabilities: cheap: 52.41%, expensive: 5.25%, medium: 24.34%, rich: 0.27%, very cheap: 17.73%\n",
      "Recipe 4:\n",
      "  Expected price tag: very cheap\n",
      "  Predicted price tag: cheap\n",
      "  Price tag probabilities: cheap: 50.75%, expensive: 0.19%, medium: 6.28%, rich: 0.00%, very cheap: 42.79%\n",
      "Recipe 5:\n",
      "  Expected price tag: expensive\n",
      "  Predicted price tag: very cheap\n",
      "  Price tag probabilities: cheap: 24.44%, expensive: 0.18%, medium: 3.15%, rich: 0.00%, very cheap: 72.24%\n",
      "Recipe 6:\n",
      "  Expected price tag: cheap\n",
      "  Predicted price tag: very cheap\n",
      "  Price tag probabilities: cheap: 2.11%, expensive: 0.00%, medium: 0.06%, rich: 0.00%, very cheap: 97.83%\n",
      "Recipe 7:\n",
      "  Expected price tag: medium\n",
      "  Predicted price tag: medium\n",
      "  Price tag probabilities: cheap: 31.67%, expensive: 17.66%, medium: 46.09%, rich: 0.95%, very cheap: 3.63%\n",
      "Recipe 8:\n",
      "  Expected price tag: expensive\n",
      "  Predicted price tag: medium\n",
      "  Price tag probabilities: cheap: 37.73%, expensive: 13.09%, medium: 44.22%, rich: 0.87%, very cheap: 4.10%\n",
      "Recipe 9:\n",
      "  Expected price tag: medium\n",
      "  Predicted price tag: cheap\n",
      "  Price tag probabilities: cheap: 65.18%, expensive: 3.37%, medium: 24.06%, rich: 0.07%, very cheap: 7.32%\n",
      "Recipe 10:\n",
      "  Expected price tag: rich\n",
      "  Predicted price tag: cheap\n",
      "  Price tag probabilities: cheap: 54.24%, expensive: 3.76%, medium: 23.95%, rich: 0.13%, very cheap: 17.93%\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creazione del modello per la classificazione del tag 'LACTOSE_FREE' e testing",
   "id": "215d763d5bea7189"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T10:47:37.212216Z",
     "start_time": "2025-06-21T10:46:31.431481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Default parameters\n",
    "MAX_LEN = 500\n",
    "NUM_WORDS = 20000\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "def preprocess(df, text_columns, label_column, n=10000):\n",
    "    df = df[text_columns + [label_column]].dropna().head(n).copy()\n",
    "    df[text_columns[0]] = df[text_columns[0]].apply(eval)\n",
    "    df[text_columns[1]] = df[text_columns[1]].apply(eval)\n",
    "    df[\"full_text\"] = df[text_columns[0]].apply(lambda x: \" \".join(x)) + \" \" + df[text_columns[1]].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"encoded_label\"] = label_encoder.fit_transform(df[label_column])\n",
    "\n",
    "    return df, label_encoder\n",
    "\n",
    "def train_text_classifier(df, label_encoder, model_name_prefix):\n",
    "    X = df[\"full_text\"]\n",
    "    y = df[\"encoded_label\"]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "    X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=NUM_WORDS, output_dim=128, input_length=MAX_LEN),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(len(label_encoder.classes_), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train_pad, y_train,\n",
    "        validation_data=(X_val_pad, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "    model.save(f\"classifiers/lactose_free/{model_name_prefix}.h5\")\n",
    "    with open(f\"classifiers/lactose_free/{model_name_prefix}_label_mapping.pkl\", \"wb\") as f:\n",
    "        pickle.dump(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))), f)\n",
    "\n",
    "    with open(f\"classifiers/lactose_free/{model_name_prefix}_tokenizer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "    print(f\"Modello '{model_name_prefix}' salvato con successo.\")\n",
    "    return model\n",
    "\n",
    "# === SEVENTH MODEL: LACTOSE_FREE ===\n",
    "df_lactose = df[[\"INGREDIENTS\", \"DIRECTIONS\", \"LACTOSE_FREE\"]].copy()\n",
    "df_lactose, lactose_free_encoder = preprocess(df_lactose, [\"INGREDIENTS\", \"DIRECTIONS\"], \"LACTOSE_FREE\")\n",
    "lactose_free_classifier_model = train_text_classifier(df_lactose, lactose_free_encoder, model_name_prefix=\"lactose_free_classifier\")"
   ],
   "id": "7c2fa2525478b9cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 41ms/step - accuracy: 0.7130 - loss: 0.5332 - val_accuracy: 0.9330 - val_loss: 0.2126\n",
      "Epoch 2/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 41ms/step - accuracy: 0.9294 - loss: 0.2114 - val_accuracy: 0.9450 - val_loss: 0.1756\n",
      "Epoch 3/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 38ms/step - accuracy: 0.9385 - loss: 0.1870 - val_accuracy: 0.9515 - val_loss: 0.1558\n",
      "Epoch 4/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 33ms/step - accuracy: 0.9539 - loss: 0.1379 - val_accuracy: 0.9535 - val_loss: 0.1518\n",
      "Epoch 5/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 32ms/step - accuracy: 0.9586 - loss: 0.1220 - val_accuracy: 0.9525 - val_loss: 0.1540\n",
      "Epoch 6/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 32ms/step - accuracy: 0.9557 - loss: 0.1161 - val_accuracy: 0.9535 - val_loss: 0.1525\n",
      "Epoch 7/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 31ms/step - accuracy: 0.9680 - loss: 0.0914 - val_accuracy: 0.9540 - val_loss: 0.1520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello 'lactose_free_classifier' salvato con successo.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T10:55:42.836566Z",
     "start_time": "2025-06-21T10:55:42.624522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "MAX_LEN = 500  # same as training\n",
    "\n",
    "# --- Load model and tokenizer ---\n",
    "lact_model = load_model(\"classifiers/lactose_free/lactose_free_classifier.h5\")\n",
    "\n",
    "with open(\"classifiers/lactose_free/lactose_free_classifier_tokenizer.pkl\", \"rb\") as f:\n",
    "    lact_tokenizer = pickle.load(f)\n",
    "\n",
    "with open(\"classifiers/lactose_free/lactose_free_classifier_label_mapping.pkl\", \"rb\") as f:\n",
    "    lact_label_map = pickle.load(f)\n",
    "\n",
    "# Reverse dictionaries to decode predictions\n",
    "inv_lact_label_map = {v: k for k, v in lact_label_map.items()}\n",
    "\n",
    "# List of 10 recipes with expected lactose free\n",
    "recipes = [\n",
    "    {\n",
    "        \"ingredients\": \"200g spaghetti, 2 cloves garlic, chili pepper, extra virgin olive oil, salt\",\n",
    "        \"instructions\": \"Cook the spaghetti. Meanwhile, sautÃ© garlic and chili in the oil. Mix with the pasta.\",\n",
    "        \"lactose_free\": True\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Lettuce, 1 can of tuna, tomatoes, black olives, onion, olive oil, salt\",\n",
    "        \"instructions\": \"Wash and cut the vegetables. Add the drained tuna and olives. Dress with oil and salt.\",\n",
    "        \"lactose_free\": True\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"300g Carnaroli rice, 1 saffron sachet, vegetable broth, butter, onion, Parmesan cheese\",\n",
    "        \"instructions\": \"SautÃ© the onion, add rice and deglaze with wine. Gradually pour in the broth. Add saffron and stir in butter and cheese.\",\n",
    "        \"lactose_free\": False\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"3 eggs, salt, pepper, butter\",\n",
    "        \"instructions\": \"Beat the eggs with salt and pepper. Pour into a pan with melted butter and stir until set.\",\n",
    "        \"lactose_free\": False\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Lasagna sheets, meat sauce, bÃ©chamel, Parmesan cheese\",\n",
    "        \"instructions\": \"Layer pasta, sauce, bÃ©chamel and cheese. Bake at 180Â°C for 40 minutes.\",\n",
    "        \"lactose_free\": False\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Apples, bananas, oranges, strawberries, lemon juice, sugar\",\n",
    "        \"instructions\": \"Cut the fruit into pieces, mix with lemon juice and sugar, refrigerate.\",\n",
    "        \"lactose_free\": True\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Chicken breast, onion, curry powder, cream or coconut milk, oil, salt\",\n",
    "        \"instructions\": \"Brown the onion, add diced chicken, curry, and cream. Cook until the chicken is tender.\",\n",
    "        \"lactose_free\": False  # assumiamo panna non vegetale\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Ladyfingers, mascarpone, eggs, sugar, coffee, cocoa powder\",\n",
    "        \"instructions\": \"Make a cream with yolks, sugar and mascarpone. Layer ladyfingers soaked in coffee and the cream. Dust with cocoa and chill.\",\n",
    "        \"lactose_free\": False\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"2 eggs, 250ml milk, 125g flour, sugar, butter\",\n",
    "        \"instructions\": \"Mix flour, eggs and milk into a batter. Cook in pan with butter. Fill as desired.\",\n",
    "        \"lactose_free\": False\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Mixed dried legumes, onion, carrot, celery, vegetable broth, oil, salt\",\n",
    "        \"instructions\": \"Soak legumes for 12 hours. SautÃ© onion, carrot, celery. Add legumes and broth and cook for at least an hour.\",\n",
    "        \"lactose_free\": True\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prepare input texts for models (ingredients + instructions)\n",
    "texts = [r[\"ingredients\"] + \" \" + r[\"instructions\"] for r in recipes]\n",
    "\n",
    "# Tokenize and padding\n",
    "lact_sequences = lact_tokenizer.texts_to_sequences(texts)\n",
    "lact_padded = pad_sequences(lact_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Predicting lactose free with probabilities\n",
    "lact_preds = lact_model.predict(lact_padded)\n",
    "lact_classes = lact_preds.argmax(axis=1)\n",
    "lact_labels = [inv_lact_label_map[c] for c in lact_classes]\n",
    "\n",
    "# Print results with probabilities\n",
    "for i, recipe in enumerate(recipes):\n",
    "    print(f\"Recipe {i+1}:\")\n",
    "    print(f\"  Expected lactose free: {recipe['lactose_free']}\")\n",
    "    print(f\"  Predicted lactose free: {lact_labels[i]}\")\n",
    "    lact_probs_str = \", \".join([f\"{inv_lact_label_map[j]}: {lact_preds[i][j]*100:.2f}%\" for j in range(len(lact_preds[i]))])\n",
    "    print(f\"  Lactose free probabilities: {lact_probs_str}\")"
   ],
   "id": "5de8f2a7ca5a90c3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 78ms/step\n",
      "Recipe 1:\n",
      "  Expected lactose free: True\n",
      "  Predicted lactose free: True\n",
      "  Lactose free probabilities: False: 12.30%, True: 87.70%\n",
      "Recipe 2:\n",
      "  Expected lactose free: True\n",
      "  Predicted lactose free: True\n",
      "  Lactose free probabilities: False: 0.54%, True: 99.46%\n",
      "Recipe 3:\n",
      "  Expected lactose free: False\n",
      "  Predicted lactose free: False\n",
      "  Lactose free probabilities: False: 99.04%, True: 0.96%\n",
      "Recipe 4:\n",
      "  Expected lactose free: False\n",
      "  Predicted lactose free: False\n",
      "  Lactose free probabilities: False: 93.94%, True: 6.06%\n",
      "Recipe 5:\n",
      "  Expected lactose free: False\n",
      "  Predicted lactose free: False\n",
      "  Lactose free probabilities: False: 98.65%, True: 1.35%\n",
      "Recipe 6:\n",
      "  Expected lactose free: True\n",
      "  Predicted lactose free: True\n",
      "  Lactose free probabilities: False: 0.72%, True: 99.28%\n",
      "Recipe 7:\n",
      "  Expected lactose free: False\n",
      "  Predicted lactose free: False\n",
      "  Lactose free probabilities: False: 93.62%, True: 6.38%\n",
      "Recipe 8:\n",
      "  Expected lactose free: False\n",
      "  Predicted lactose free: False\n",
      "  Lactose free probabilities: False: 90.99%, True: 9.01%\n",
      "Recipe 9:\n",
      "  Expected lactose free: False\n",
      "  Predicted lactose free: False\n",
      "  Lactose free probabilities: False: 98.94%, True: 1.06%\n",
      "Recipe 10:\n",
      "  Expected lactose free: True\n",
      "  Predicted lactose free: True\n",
      "  Lactose free probabilities: False: 1.39%, True: 98.61%\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creazione del modello per la classificazione del tag 'GLUTEN_FREE' e testing",
   "id": "79c3e6ac48068a43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T11:00:49.222350Z",
     "start_time": "2025-06-21T10:59:37.455677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Default parameters\n",
    "MAX_LEN = 500\n",
    "NUM_WORDS = 20000\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "def preprocess(df, text_columns, label_column, n=10000):\n",
    "    df = df[text_columns + [label_column]].dropna().head(n).copy()\n",
    "    df[text_columns[0]] = df[text_columns[0]].apply(eval)\n",
    "    df[text_columns[1]] = df[text_columns[1]].apply(eval)\n",
    "    df[\"full_text\"] = df[text_columns[0]].apply(lambda x: \" \".join(x)) + \" \" + df[text_columns[1]].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"encoded_label\"] = label_encoder.fit_transform(df[label_column])\n",
    "\n",
    "    return df, label_encoder\n",
    "\n",
    "def train_text_classifier(df, label_encoder, model_name_prefix):\n",
    "    X = df[\"full_text\"]\n",
    "    y = df[\"encoded_label\"]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "    X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=NUM_WORDS, output_dim=128, input_length=MAX_LEN),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(len(label_encoder.classes_), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(\n",
    "        X_train_pad, y_train,\n",
    "        validation_data=(X_val_pad, y_val),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "    model.save(f\"classifiers/gluten_free/{model_name_prefix}.h5\")\n",
    "    with open(f\"classifiers/gluten_free/{model_name_prefix}_label_mapping.pkl\", \"wb\") as f:\n",
    "        pickle.dump(dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))), f)\n",
    "\n",
    "    with open(f\"classifiers/gluten_free/{model_name_prefix}_tokenizer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "    print(f\"Modello '{model_name_prefix}' salvato con successo.\")\n",
    "    return model\n",
    "\n",
    "# === EIGHTH MODEL: GLUTEN_FREE ===\n",
    "df_gluten = df[[\"INGREDIENTS\", \"DIRECTIONS\", \"GLUTEN_FREE\"]].copy()\n",
    "df_gluten, gluten_free_encoder = preprocess(df_gluten, [\"INGREDIENTS\", \"DIRECTIONS\"], \"GLUTEN_FREE\")\n",
    "gluten_free_classifier_model = train_text_classifier(df_gluten, gluten_free_encoder, model_name_prefix=\"gluten_free_classifier\")"
   ],
   "id": "1439c245b01ef91c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aless\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 37ms/step - accuracy: 0.8127 - loss: 0.4831 - val_accuracy: 0.8985 - val_loss: 0.2785\n",
      "Epoch 2/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 35ms/step - accuracy: 0.8991 - loss: 0.2366 - val_accuracy: 0.9175 - val_loss: 0.1931\n",
      "Epoch 3/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 36ms/step - accuracy: 0.9181 - loss: 0.1926 - val_accuracy: 0.9230 - val_loss: 0.1788\n",
      "Epoch 4/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 33ms/step - accuracy: 0.9287 - loss: 0.1677 - val_accuracy: 0.9220 - val_loss: 0.1739\n",
      "Epoch 5/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 33ms/step - accuracy: 0.9378 - loss: 0.1438 - val_accuracy: 0.9215 - val_loss: 0.1677\n",
      "Epoch 6/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 32ms/step - accuracy: 0.9486 - loss: 0.1244 - val_accuracy: 0.9240 - val_loss: 0.1716\n",
      "Epoch 7/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 32ms/step - accuracy: 0.9538 - loss: 0.1190 - val_accuracy: 0.9215 - val_loss: 0.1707\n",
      "Epoch 8/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 31ms/step - accuracy: 0.9597 - loss: 0.1006 - val_accuracy: 0.9165 - val_loss: 0.1772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello 'gluten_free_classifier' salvato con successo.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T11:06:52.252295Z",
     "start_time": "2025-06-21T11:06:52.032225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "MAX_LEN = 500  # same as training\n",
    "\n",
    "# --- Load model and tokenizer ---\n",
    "glut_model = load_model(\"classifiers/gluten_free/gluten_free_classifier.h5\")\n",
    "\n",
    "with open(\"classifiers/gluten_free/gluten_free_classifier_tokenizer.pkl\", \"rb\") as f:\n",
    "    glut_tokenizer = pickle.load(f)\n",
    "\n",
    "with open(\"classifiers/gluten_free/gluten_free_classifier_label_mapping.pkl\", \"rb\") as f:\n",
    "    glut_label_map = pickle.load(f)\n",
    "\n",
    "# Reverse dictionaries to decode predictions\n",
    "inv_glut_label_map = {v: k for k, v in glut_label_map.items()}\n",
    "\n",
    "# List of 10 recipes with expected gluten free\n",
    "recipes = [\n",
    "    {\n",
    "        \"ingredients\": \"200g spaghetti, 2 cloves garlic, chili pepper, extra virgin olive oil, salt\",\n",
    "        \"instructions\": \"Cook the spaghetti. Meanwhile, sautÃ© garlic and chili in the oil. Mix with the pasta.\",\n",
    "        \"gluten_free\": False\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Lettuce, 1 can of tuna, tomatoes, black olives, onion, olive oil, salt\",\n",
    "        \"instructions\": \"Wash and cut the vegetables. Add the drained tuna and olives. Dress with oil and salt.\",\n",
    "        \"gluten_free\": True\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"300g Carnaroli rice, 1 saffron sachet, vegetable broth, butter, onion, Parmesan cheese\",\n",
    "        \"instructions\": \"SautÃ© the onion, add rice and deglaze with wine. Gradually pour in the broth. Add saffron and stir in butter and cheese.\",\n",
    "        \"gluten_free\": True\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"3 eggs, salt, pepper, butter\",\n",
    "        \"instructions\": \"Beat the eggs with salt and pepper. Pour into a pan with melted butter and stir until set.\",\n",
    "        \"gluten_free\": True\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Lasagna sheets, meat sauce, bÃ©chamel, Parmesan cheese\",\n",
    "        \"instructions\": \"Layer pasta, sauce, bÃ©chamel and cheese. Bake at 180Â°C for 40 minutes.\",\n",
    "        \"gluten_free\": False\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Apples, bananas, oranges, strawberries, lemon juice, sugar\",\n",
    "        \"instructions\": \"Cut the fruit into pieces, mix with lemon juice and sugar, refrigerate.\",\n",
    "        \"gluten_free\": True\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Chicken breast, onion, curry powder, cream or coconut milk, oil, salt\",\n",
    "        \"instructions\": \"Brown the onion, add diced chicken, curry, and cream. Cook until the chicken is tender.\",\n",
    "        \"gluten_free\": True\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Ladyfingers, mascarpone, eggs, sugar, coffee, cocoa powder\",\n",
    "        \"instructions\": \"Make a cream with yolks, sugar and mascarpone. Layer ladyfingers soaked in coffee and the cream. Dust with cocoa and chill.\",\n",
    "        \"gluten_free\": False  # ladyfingers contengono farina di frumento\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"2 eggs, 250ml milk, 125g flour, sugar, butter\",\n",
    "        \"instructions\": \"Mix flour, eggs and milk into a batter. Cook in pan with butter. Fill as desired.\",\n",
    "        \"gluten_free\": False  # contiene farina\n",
    "    },\n",
    "    {\n",
    "        \"ingredients\": \"Mixed dried legumes, onion, carrot, celery, vegetable broth, oil, salt\",\n",
    "        \"instructions\": \"Soak legumes for 12 hours. SautÃ© onion, carrot, celery. Add legumes and broth and cook for at least an hour.\",\n",
    "        \"gluten_free\": True\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prepare input texts for models (ingredients + instructions)\n",
    "texts = [r[\"ingredients\"] + \" \" + r[\"instructions\"] for r in recipes]\n",
    "\n",
    "# Tokenize and padding\n",
    "glut_sequences = glut_tokenizer.texts_to_sequences(texts)\n",
    "glut_padded = pad_sequences(glut_sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Predicting gluten free with probabilities\n",
    "glut_preds = glut_model.predict(glut_padded)\n",
    "glut_classes = glut_preds.argmax(axis=1)\n",
    "glut_labels = [inv_glut_label_map[c] for c in glut_classes]\n",
    "\n",
    "# Print results with probabilities\n",
    "for i, recipe in enumerate(recipes):\n",
    "    print(f\"Recipe {i+1}:\")\n",
    "    print(f\"  Expected gluten free: {recipe['gluten_free']}\")\n",
    "    print(f\"  Predicted gluten free: {glut_labels[i]}\")\n",
    "    glut_probs_str = \", \".join([f\"{inv_glut_label_map[j]}: {glut_preds[i][j]*100:.2f}%\" for j in range(len(glut_preds[i]))])\n",
    "    print(f\"  Gluten free probabilities: {glut_probs_str}\")"
   ],
   "id": "6e0e298ff477a355",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32mââââââââââââââââââââ\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 79ms/step\n",
      "Recipe 1:\n",
      "  Expected gluten free: False\n",
      "  Predicted gluten free: False\n",
      "  Gluten free probabilities: False: 52.81%, True: 47.19%\n",
      "Recipe 2:\n",
      "  Expected gluten free: True\n",
      "  Predicted gluten free: True\n",
      "  Gluten free probabilities: False: 0.16%, True: 99.84%\n",
      "Recipe 3:\n",
      "  Expected gluten free: True\n",
      "  Predicted gluten free: True\n",
      "  Gluten free probabilities: False: 1.60%, True: 98.40%\n",
      "Recipe 4:\n",
      "  Expected gluten free: True\n",
      "  Predicted gluten free: True\n",
      "  Gluten free probabilities: False: 0.40%, True: 99.60%\n",
      "Recipe 5:\n",
      "  Expected gluten free: False\n",
      "  Predicted gluten free: False\n",
      "  Gluten free probabilities: False: 79.89%, True: 20.11%\n",
      "Recipe 6:\n",
      "  Expected gluten free: True\n",
      "  Predicted gluten free: True\n",
      "  Gluten free probabilities: False: 0.08%, True: 99.92%\n",
      "Recipe 7:\n",
      "  Expected gluten free: True\n",
      "  Predicted gluten free: True\n",
      "  Gluten free probabilities: False: 0.03%, True: 99.97%\n",
      "Recipe 8:\n",
      "  Expected gluten free: False\n",
      "  Predicted gluten free: True\n",
      "  Gluten free probabilities: False: 0.02%, True: 99.98%\n",
      "Recipe 9:\n",
      "  Expected gluten free: False\n",
      "  Predicted gluten free: True\n",
      "  Gluten free probabilities: False: 0.52%, True: 99.48%\n",
      "Recipe 10:\n",
      "  Expected gluten free: True\n",
      "  Predicted gluten free: True\n",
      "  Gluten free probabilities: False: 4.03%, True: 95.97%\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
